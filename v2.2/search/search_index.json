{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Zamba \u00b6 Zamba means \"forest\" in Lingala, a Bantu language spoken throughout the Democratic Republic of the Congo and the Republic of the Congo. zamba is a tool built in Python that uses machine learning and computer vision to automatically detect and classify animals in camera trap videos. You can use zamba to: Identify which species appear in each video Filter out blank videos Create your own custom models that identify your species in your habitats And more! \ud83d\ude48 \ud83d\ude49 \ud83d\ude4a The official models in zamba can identify blank videos (where no animal is present) along with 32 species common to Africa and 11 species common to Europe. Users can also finetune models using their own labeled videos to then make predictions for new species and/or new ecologies. zamba can be used both as a command-line tool and as a Python package. It is also available as a user-friendly website application, Zamba Cloud . We encourage people to share their custom models trained with Zamba. If you train a model and want to make it available, please add it to the Model Zoo Wiki for others to be able to use! Installing zamba \u00b6 First, make sure you have the prerequisites installed: Python 3.7 or 3.8 FFmpeg > 4.3 Then run: pip install https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz See the Installation page of the documentation for details. Getting started \u00b6 Once you have zamba installed, some good starting points are: The Quickstart page for basic examples of usage The user tutorial for either classifying videos or training a model depending on what you want to do with zamba Example usage \u00b6 Once zamba is installed, you can see the basic command options with: $ zamba --help Usage: zamba [OPTIONS] COMMAND [ARGS]... Zamba is a tool built in Python to automatically identify the species seen in camera trap videos from sites in Africa and Europe. Visit https://zamba.drivendata.org/docs for more in-depth documentation. Options: --version Show zamba version and exit. --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: densepose Run densepose algorithm on videos. predict Identify species in a video. train Train a model on your labeled data. zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. Classifying unlabeled videos \u00b6 $ zamba predict --data-dir path/to/videos By default, predictions will be saved to zamba_predictions.csv . Run zamba predict --help to list all possible options to pass to predict . See the Quickstart page or the user tutorial on classifying videos for more details. Training a model \u00b6 $ zamba train --data-dir path/to/videos --labels path_to_labels.csv --save_dir my_trained_model The newly trained model will be saved to the specified save directory. The folder will contain a model checkpoint as well as training configuration, model hyperparameters, and validation and test metrics. Run zamba train --help to list all possible options to pass to train . You can use your trained model on new videos by editing the train_configuration.yaml that is generated by zamba . Add a predict_config section to the yaml that points to the checkpoint file that is generated: ... # generated train_config and video_loader_config ... predict_config : checkpoint : PATH_TO_YOUR_CHECKPOINT_FILE Now you can pass this configuration to the command line. See the Quickstart page or the user tutorial on training a model for more details. You can then share your model with others by adding it to the Model Zoo Wiki . Contributing \u00b6 We would love your contributions of code fixes, new models, additional training data, docs revisions, and anything else you can bring to the project! See the docs page on contributing to zamba for details.","title":"Home"},{"location":"#zamba","text":"Zamba means \"forest\" in Lingala, a Bantu language spoken throughout the Democratic Republic of the Congo and the Republic of the Congo. zamba is a tool built in Python that uses machine learning and computer vision to automatically detect and classify animals in camera trap videos. You can use zamba to: Identify which species appear in each video Filter out blank videos Create your own custom models that identify your species in your habitats And more! \ud83d\ude48 \ud83d\ude49 \ud83d\ude4a The official models in zamba can identify blank videos (where no animal is present) along with 32 species common to Africa and 11 species common to Europe. Users can also finetune models using their own labeled videos to then make predictions for new species and/or new ecologies. zamba can be used both as a command-line tool and as a Python package. It is also available as a user-friendly website application, Zamba Cloud . We encourage people to share their custom models trained with Zamba. If you train a model and want to make it available, please add it to the Model Zoo Wiki for others to be able to use!","title":"Zamba"},{"location":"#installing-zamba","text":"First, make sure you have the prerequisites installed: Python 3.7 or 3.8 FFmpeg > 4.3 Then run: pip install https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz See the Installation page of the documentation for details.","title":"Installing zamba"},{"location":"#getting-started","text":"Once you have zamba installed, some good starting points are: The Quickstart page for basic examples of usage The user tutorial for either classifying videos or training a model depending on what you want to do with zamba","title":"Getting started"},{"location":"#example-usage","text":"Once zamba is installed, you can see the basic command options with: $ zamba --help Usage: zamba [OPTIONS] COMMAND [ARGS]... Zamba is a tool built in Python to automatically identify the species seen in camera trap videos from sites in Africa and Europe. Visit https://zamba.drivendata.org/docs for more in-depth documentation. Options: --version Show zamba version and exit. --install-completion Install completion for the current shell. --show-completion Show completion for the current shell, to copy it or customize the installation. --help Show this message and exit. Commands: densepose Run densepose algorithm on videos. predict Identify species in a video. train Train a model on your labeled data. zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training.","title":"Example usage"},{"location":"#contributing","text":"We would love your contributions of code fixes, new models, additional training data, docs revisions, and anything else you can bring to the project! See the docs page on contributing to zamba for details.","title":"Contributing"},{"location":"configurations/","text":"All configuration options \u00b6 To make it easy to associate a model configuration with and a set of results, zamba accepts a yaml file to define all of the relevant parameters for training or prediction. You can then store the configuration you used with the results in order to easily reproduce it in the future. In general, we've tried to pick defaults that are reasonable, but it is worth it to familiarize yourself with the options available. The primary configurations you may want to set are: VideoLoaderConfig : Defines all possible parameters for how videos are loaded PredictConfig : Defines all possible parameters for model inference TrainConfig : Defines all possible parameters for model training Here's a helpful diagram which shows how everything is related. Video loading arguments \u00b6 The VideoLoaderConfig class defines all of the optional parameters that can be specified for how videos are loaded before either inference or training. This includes selecting which frames to use from each video. All video loading arguments can be specified either in a YAML file or when instantiating the VideoLoaderConfig class in Python. Some can also be specified directly in the command line. Each model comes with a default video loading configuration. If no user-specified video loading configuration is passed - either through a YAML file or the Python VideoLoaderConfig class - all video loading arguments will be set based on the defaults for the given model. YAML file video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 # ... other parameters Python from zamba.data.video import VideoLoaderConfig from zamba.models.config import PredictConfig from zamba.models.model_manager import predict_model predict_config = PredictConfig ( data_dir = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( model_input_height = 240 , model_input_width = 426 , total_frames = 16 # ... other parameters ) predict_model ( predict_config = predict_config , video_loader_config = video_loader_config ) Let's look at the class documentation in Python. >> from zamba.data.video import VideoLoaderConfig >> help ( VideoLoaderConfig ) class VideoLoaderConfig ( pydantic . main . BaseModel ) | VideoLoaderConfig ( * , crop_bottom_pixels : int = None , i_frames : bool = False , scene_threshold : float = None , megadetector_lite_config : zamba . models . megadetector_lite_yolox . MegadetectorLiteYoloXConfig = None , frame_selection_height : int = None , frame_selection_width : int = None , total_frames : int = None , ensure_total_frames : bool = True , fps : float = None , early_bias : bool = False , frame_indices : List [ int ] = None , evenly_sample_total_frames : bool = False , pix_fmt : str = 'rgb24' , model_input_height : int = None , model_input_width : int = None , cache_dir : pathlib . Path = None , cleanup_cache : bool = False ) -> None ... crop_bottom_pixels (int, optional) \u00b6 Number of pixels to crop from the bottom of the video (prior to resizing to frame_selection_height ). This can sometimes be useful if your videos have a persistent timestamp/camera brand logo at the bottom. Defaults to None i_frames (bool, optional) \u00b6 Only load the I-Frames . I-frames are highly dependent on the encoding of the video, so it is not recommended to use them unless you have verified that the i-frames of your videos are useful. Defaults to False scene_threshold (float, optional) \u00b6 Only load frames that correspond to scene changes , which are detected when scene_threshold percent of pixels are different. This can be useful for selecting frames efficiently if in general you have large animals and stable backgrounds. Defaults to None megadetector_lite_config (MegadetectorLiteYoloXConfig, optional) \u00b6 The megadetector_lite_config is used to specify any parameters that should be passed to the MegadetectorLite model for frame selection. For all possible options, see the MegadetectorLiteYoloXConfig class . If megadetector_lite_config is None (the default), the MegadetectorLite model will not be used to select frames. frame_selection_height (int, optional), frame_selection_width (int, optional) \u00b6 Resize the video to this height and width in pixels, prior to frame selection. If None, the full size video will be used for frame selection. Using full size videos (setting to None ) is recommended for MegadetectorLite, especially if your species of interest are smaller. Defaults to None total_frames (int, optional) \u00b6 Number of frames that should ultimately be returned. Defaults to None ensure_total_frames (bool) \u00b6 Some frame selection methods may yield varying numbers of frames depending on timestamps of the video frames. If True , ensure the requested number of frames is returned by either clipping or duplicating the final frame. If no frames are selected, returns an array of the desired shape with all zeros. Otherwise, return the array unchanged. Defaults to True fps (float, optional) \u00b6 Resample the video evenly from the entire duration to a specific number of frames per second. Use values less than 1 for rates lower than a single frame per second (e.g., fps=0.5 will result in 1 frame every 2 seconds). Defaults to None early_bias (bool, optional) \u00b6 Resamples to 24 fps and selects 16 frames biased toward the beginning of the video. This strategy was used by the Pri-matrix Factorization machine learning competition winner. Defaults to False frame_indices (list(int), optional) \u00b6 Select specific frame numbers. Note: frame selection is done after any resampling. Defaults to None evenly_sample_total_frames (bool, optional) \u00b6 Reach the total number of frames specified by evenly sampling from the duration of the video. Defaults to False pix_fmt (str, optional) \u00b6 FFmpeg pixel format, defaults to rgb24 for RGB channels; can be changed to bgr24 for BGR. model_input_height (int, optional), model_input_width (int, optional) \u00b6 After frame selection, resize the video to this height and width in pixels. This controls the height and width of the video frames returned by load_video_frames . Defaults to None cache_dir (Path, optional) \u00b6 Cache directory where preprocessed videos will be saved upon first load. Alternatively, can be set with VIDEO_CACHE_DIR environment variable. Provided there is enough space on your machine, it is highly encouraged to cache videos for training as this will speed up all subsequent epochs after the first. If you are predicting on the same videos with the same video loader configuration, this will save time on future runs. Defaults to None , which means videos will not be cached. cleanup_cache (bool, optional) \u00b6 Whether to delete the cache directory after training or predicting ends. Defaults to False Prediction arguments \u00b6 All possible model inference parameters are defined by the PredictConfig class . Let's see the class documentation in Python: >> from zamba.models.config import PredictConfig >> help ( PredictConfig ) class PredictConfig ( ZambaBaseModel ) | PredictConfig ( * , data_dir : DirectoryPath = Path . cwd (), filepaths : FilePath = None , checkpoint : FilePath = None , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , gpus : int = 0 , num_workers : int = 3 , batch_size : int = 2 , save : bool = True , save_dir : Optional [ Path ] = None , overwrite : bool = False , dry_run : bool = False , proba_threshold : float = None , output_class_names : bool = False , weight_download_region : zamba . models . utils . RegionEnum = 'us' , skip_load_validation : bool = False , model_cache_dir : pathlib . Path = None ) -> None ... Either data_dir or filepaths must be specified to instantiate PredictConfig . If neither is specified, the current working directory will be used as the default data_dir . data_dir (DirectoryPath, optional) \u00b6 Path to the directory containing videos for inference. Defaults to the current working directory. filepaths (FilePath, optional) \u00b6 Path to a csv containing a filepath column with paths to the videos that should be classified. checkpoint (Path or str, optional) \u00b6 Path to a model checkpoint to load and use for inference. If you train your own custom models, this is how you can pass those models to zamba when you want to predict on new videos. The default is None , which will load the pretrained checkpoint if the model specified by model_name . model_name (time_distributed|slowfast|european, optional) \u00b6 Name of the model to use for inference. The model options that ship with zamba are blank_nonblank , time_distributed , slowfast , and european . See the Available Models page for details. Defaults to time_distributed gpus (int, optional) \u00b6 The number of GPUs to use during inference. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available on the machine. num_workers (int, optional) \u00b6 The number of CPUs to use during training. The maximum value for num_workers is the number of CPUs available on the machine. If you are using MegadetectorLite for frame selection, it is not recommended to use the total number of CPUs available. Defaults to 3 batch_size (int, optional) \u00b6 The batch size to use for inference. Defaults to 2 save (bool) \u00b6 Whether to save out predictions. If False , predictions are not saved. Defaults to True . save_dir (Path, optional) \u00b6 An optional directory in which to save the model predictions and configuration yaml. If no save_dir is specified and save is True, outputs will be written to the current working directory. Defaults to None overwrite (bool) \u00b6 If True, will overwrite zamba_predictions.csv and predict_configuration.yaml in save_dir if they exist. Defaults to False. dry_run (bool, optional) \u00b6 Specifying True is useful for ensuring a model implementation or configuration works properly by running only a single batch of inference. Defaults to False proba_threshold (float between 0 and 1, optional) \u00b6 For advanced uses, you may want the algorithm to be more or less sensitive to if a species is present. This parameter is a float, e.g., 0.6 corresponding to the probability threshold beyond which an animal is considered to be present in the video being analyzed. By default no threshold is passed, proba_threshold=None . This will return a probability from 0-1 for each species that could occur in each video. If a threshold is passed, then the final prediction value returned for each class is probability >= proba_threshold , so that all class values become 0 ( False , the species does not appear) or 1 ( True , the species does appear). output_class_names (bool, optional) \u00b6 Setting this option to True yields the most concise output zamba is capable of. The highest species probability in a video is taken to be the only species in that video, and the output returned is simply the video name and the name of the species with the highest class probability, or blank if the most likely classification is no animal. Defaults to False weight_download_region [us|eu|asia] \u00b6 Because zamba needs to download pretrained weights for the neural network architecture, we make these weights available in different regions. us is the default, but if you are not in the US you should use either eu for the European Union or asia for Asia Pacific to make sure that these download as quickly as possible for you. skip_load_validation (bool, optional) \u00b6 By default, before kicking off inference zamba will iterate through all of the videos in the data and verify that each can be loaded. Setting skip_load_verification to True skips this step. Validation can be very time intensive depending on the number of videos. It is recommended to run validation once, but not on future inference runs if the videos have not changed. Defaults to False model_cache_dir (Path, optional) \u00b6 Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, will use your default cache directory (e.g. ~/.cache ). Defaults to None Training arguments \u00b6 All possible model training parameters are defined by the TrainConfig class . Let's see the class documentation in Python: >> from zamba.models.config import TrainConfig >> help ( TrainConfig ) class TrainConfig ( ZambaBaseModel ) | TrainConfig ( * , labels : Union [ FilePath , pandas . DataFrame ], data_dir : DirectoryPath = # your current working directory , checkpoint : FilePath = None , scheduler_config : Union [ str , zamba . models . config . SchedulerConfig , NoneType ] = 'default' , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , dry_run : Union [ bool , int ] = False , batch_size : int = 2 , auto_lr_find : bool = False , backbone_finetune_config : zamba . models . config . BackboneFinetuneConfig = BackboneFinetuneConfig ( unfreeze_backbone_at_epoch = 5 , backbone_initial_ratio_lr = 0.01 , multiplier = 1 , pre_train_bn = False , train_bn = False , verbose = True ), gpus : int = 0 , num_workers : int = 3 , max_epochs : int = None , early_stopping_config : zamba . models . config . EarlyStoppingConfig = EarlyStoppingConfig ( monitor = 'val_macro_f1' , patience = 5 , verbose = True , mode = 'max' ), weight_download_region : zamba . models . utils . RegionEnum = 'us' , split_proportions : Dict [ str , int ] = { 'train' : 3 , 'val' : 1 , 'holdout' : 1 }, save_dir : pathlib . Path = # your current working directory , overwrite : bool = False , skip_load_validation : bool = False , from_scratch : bool = False , use_default_model_labels : bool = True , model_cache_dir : pathlib . Path = None ) -> None ... labels (FilePath or pd.DataFrame, required) \u00b6 Either the path to a CSV file with labels for training, or a dataframe of the training labels. There must be columns for filename and label . labels must be specified to instantiate TrainConfig . data_dir (DirectoryPath, optional) \u00b6 Path to the directory containing training videos. Defaults to the current working directory. checkpoint (Path or str, optional) \u00b6 Path to a model checkpoint to load and resume training from. The default is None , which automatically loads the pretrained checkpoint for the model specified by model_name . Since the default model_name is time_distributed the default checkpoint is zamba_time_distributed.ckpt scheduler_config (zamba.models.config.SchedulerConfig, optional) \u00b6 A PyTorch learning rate schedule to adjust the learning rate based on the number of epochs. Scheduler can either be default (the default), None , or a torch.optim.lr_scheduler . model_name (time_distributed|slowfast|european, optional) \u00b6 Name of the model to use for inference. The model options that ship with zamba are blank_nonblank , time_distributed , slowfast , and european . See the Available Models page for details. Defaults to time_distributed dry_run (bool, optional) \u00b6 Specifying True is useful for trying out model implementations more quickly by running only a single batch of train and validation. Defaults to False batch_size (int, optional) \u00b6 The batch size to use for training. Defaults to 2 auto_lr_find (bool, optional) \u00b6 Whether to run a learning rate finder algorithm when calling pytorch_lightning.trainer.tune() to try to find an optimal initial learning rate. The learning rate finder is not guaranteed to find a good learning rate; depending on the dataset, it can select a learning rate that leads to poor model training. Use with caution. See the PyTorch Lightning docs for more details. Defaults to False backbone_finetune_config (zamba.models.config.BackboneFinetuneConfig, optional) \u00b6 Set parameters to finetune a backbone model to align with the current learning rate. Derived from Pytorch Lightning's built-in BackboneFinetuning . The default values are specified in the BackboneFinetuneConfig class : BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5, backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False, train_bn=False, verbose=True) gpus (int, optional) \u00b6 The number of GPUs to use during training. By default, all of the available GPUs found on the machine will be used. An error will be raised if the number of GPUs specified is more than the number that are available on the machine. num_workers (int, optional) \u00b6 The number of CPUs to use during training. The maximum value for num_workers is the number of CPUs available in the system. If you are using the Megadetector, it is not recommended to use the total number of CPUs available. Defaults to 3 max_epochs (int, optional) \u00b6 The maximum number of epochs to run during training. Defaults to None early_stopping_config (zamba.models.config.EarlyStoppingConfig, optional) \u00b6 Parameters to pass to Pytorch lightning's EarlyStopping to monitor a metric during model training and stop training when the metric stops improving. The default values are specified in the EarlyStoppingConfig class : EarlyStoppingConfig(monitor='val_macro_f1', patience=5, verbose=True, mode='max') weight_download_region [us|eu|asia] \u00b6 Because zamba needs to download pretrained weights for the neural network architecture, we make these weights available in different regions. us is the default, but if you are not in the US you should use either eu for the European Union or asia for Asia Pacific to make sure that these download as quickly as possible for you. split_proportions (dict(str, int), optional) \u00b6 The proportion of data to use during training, validation, and as a holdout set. Defaults to {\"train\": 3, \"val\": 1, \"holdout\": 1} save_dir (Path, optional) \u00b6 Directory in which to save model checkpoint and configuration file. If not specified, will save to a version_n folder in your current working directory. overwrite (bool, optional) \u00b6 If True , will save outputs in save_dir and overwrite the directory if it exists. If False, will create an auto-incremented version_n folder within save_dir with model outputs. Defaults to False skip_load_validation (bool, optional) \u00b6 By default, before kicking off training zamba will iterate through all of the videos in the training data and verify that each can be loaded. Setting skip_load_verification to True skips this step. Validation can be very time intensive depending on the number of videos. It is recommended to run validation once, but not on future training runs if the videos have not changed. Defaults to False from_scratch (bool, optional) \u00b6 Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Only used if labels is not None. Defaults to False use_default_model_labels (bool, optional) \u00b6 Whether the species outputted by the model should be the default model classes (e.g. all 32 species classes for the time_distributed model). If you want the model classes to only be the species in your labels file (e.g. just gorillas and elephants), set to False . If either use_default_model_labels is False or the labels contain species that are not in the model, the model head will be replaced for finetuning. Defaults to True model_cache_dir (Path, optional) \u00b6 Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, will use your default cache directory, which is often an automatic temp directory at ~/.cache/zamba . Defaults to None","title":"All configuration options"},{"location":"configurations/#all-configuration-options","text":"To make it easy to associate a model configuration with and a set of results, zamba accepts a yaml file to define all of the relevant parameters for training or prediction. You can then store the configuration you used with the results in order to easily reproduce it in the future. In general, we've tried to pick defaults that are reasonable, but it is worth it to familiarize yourself with the options available. The primary configurations you may want to set are: VideoLoaderConfig : Defines all possible parameters for how videos are loaded PredictConfig : Defines all possible parameters for model inference TrainConfig : Defines all possible parameters for model training Here's a helpful diagram which shows how everything is related.","title":"All configuration options"},{"location":"configurations/#video-loading-arguments","text":"The VideoLoaderConfig class defines all of the optional parameters that can be specified for how videos are loaded before either inference or training. This includes selecting which frames to use from each video. All video loading arguments can be specified either in a YAML file or when instantiating the VideoLoaderConfig class in Python. Some can also be specified directly in the command line. Each model comes with a default video loading configuration. If no user-specified video loading configuration is passed - either through a YAML file or the Python VideoLoaderConfig class - all video loading arguments will be set based on the defaults for the given model. YAML file video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 # ... other parameters Python from zamba.data.video import VideoLoaderConfig from zamba.models.config import PredictConfig from zamba.models.model_manager import predict_model predict_config = PredictConfig ( data_dir = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( model_input_height = 240 , model_input_width = 426 , total_frames = 16 # ... other parameters ) predict_model ( predict_config = predict_config , video_loader_config = video_loader_config ) Let's look at the class documentation in Python. >> from zamba.data.video import VideoLoaderConfig >> help ( VideoLoaderConfig ) class VideoLoaderConfig ( pydantic . main . BaseModel ) | VideoLoaderConfig ( * , crop_bottom_pixels : int = None , i_frames : bool = False , scene_threshold : float = None , megadetector_lite_config : zamba . models . megadetector_lite_yolox . MegadetectorLiteYoloXConfig = None , frame_selection_height : int = None , frame_selection_width : int = None , total_frames : int = None , ensure_total_frames : bool = True , fps : float = None , early_bias : bool = False , frame_indices : List [ int ] = None , evenly_sample_total_frames : bool = False , pix_fmt : str = 'rgb24' , model_input_height : int = None , model_input_width : int = None , cache_dir : pathlib . Path = None , cleanup_cache : bool = False ) -> None ...","title":"Video loading arguments"},{"location":"configurations/#prediction-arguments","text":"All possible model inference parameters are defined by the PredictConfig class . Let's see the class documentation in Python: >> from zamba.models.config import PredictConfig >> help ( PredictConfig ) class PredictConfig ( ZambaBaseModel ) | PredictConfig ( * , data_dir : DirectoryPath = Path . cwd (), filepaths : FilePath = None , checkpoint : FilePath = None , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , gpus : int = 0 , num_workers : int = 3 , batch_size : int = 2 , save : bool = True , save_dir : Optional [ Path ] = None , overwrite : bool = False , dry_run : bool = False , proba_threshold : float = None , output_class_names : bool = False , weight_download_region : zamba . models . utils . RegionEnum = 'us' , skip_load_validation : bool = False , model_cache_dir : pathlib . Path = None ) -> None ... Either data_dir or filepaths must be specified to instantiate PredictConfig . If neither is specified, the current working directory will be used as the default data_dir .","title":"Prediction arguments"},{"location":"configurations/#training-arguments","text":"All possible model training parameters are defined by the TrainConfig class . Let's see the class documentation in Python: >> from zamba.models.config import TrainConfig >> help ( TrainConfig ) class TrainConfig ( ZambaBaseModel ) | TrainConfig ( * , labels : Union [ FilePath , pandas . DataFrame ], data_dir : DirectoryPath = # your current working directory , checkpoint : FilePath = None , scheduler_config : Union [ str , zamba . models . config . SchedulerConfig , NoneType ] = 'default' , model_name : zamba . models . config . ModelEnum = < ModelEnum . time_distributed : 'time_distributed' > , dry_run : Union [ bool , int ] = False , batch_size : int = 2 , auto_lr_find : bool = False , backbone_finetune_config : zamba . models . config . BackboneFinetuneConfig = BackboneFinetuneConfig ( unfreeze_backbone_at_epoch = 5 , backbone_initial_ratio_lr = 0.01 , multiplier = 1 , pre_train_bn = False , train_bn = False , verbose = True ), gpus : int = 0 , num_workers : int = 3 , max_epochs : int = None , early_stopping_config : zamba . models . config . EarlyStoppingConfig = EarlyStoppingConfig ( monitor = 'val_macro_f1' , patience = 5 , verbose = True , mode = 'max' ), weight_download_region : zamba . models . utils . RegionEnum = 'us' , split_proportions : Dict [ str , int ] = { 'train' : 3 , 'val' : 1 , 'holdout' : 1 }, save_dir : pathlib . Path = # your current working directory , overwrite : bool = False , skip_load_validation : bool = False , from_scratch : bool = False , use_default_model_labels : bool = True , model_cache_dir : pathlib . Path = None ) -> None ...","title":"Training arguments"},{"location":"debugging/","text":"Debugging \u00b6 Before kicking off a full run of inference or model training, we recommend testing your code with a \"dry run\". If you are generating predictions, this will run one batch of inference to quickly detect any bugs. If you are trainig a model, this will run one training and validation batch for one epoch. If the dry run completes successfully, predict and train away! CLI $ zamba predict --data-dir example_vids/ --dry-run $ zamba train --data-dir example_vids/ --labels example_labels.csv --dry-run Python In Python, add dry_run=True to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , dry_run = True ) GPU memory errors \u00b6 The dry run will also catch any GPU memory errors. If you hit a GPU memory error, there are a couple fixes. Reducing the batch size \u00b6 CLI zamba train --data-dir example_vids/ --labels example_labels.csv --batch-size 1 Python In Python, add batch_size to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , batch_size = 1 ) Decreasing video size \u00b6 Resize video frames to be smaller before they are passed to the model. The default for all models is 240x426 pixels. model_input_height and model_input_width cannot be passed directly to the command line, so if you are using the CLI these must be specified in a YAML file . If you are using MegadetectorLite to select frames (which is the default for the official models we ship with), you can also decrease the size of the frame used at this stage by setting frame_selection_height and frame_selection_width . YAML file video_loader_config : frame_selection_height : 400 # if using megadetectorlite frame_selection_width : 600 # if using megadetectorlite model_input_height : 100 model_input_width : 100 total_frames : 16 # total_frames is always required Python video_loader_config = VideoLoaderConfig ( frame_selection_height = 400 , frame_selection_width = 600 , # if using megadetectorlite model_input_height = 100 , model_input_width = 100 , total_frames = 16 , ) # total_frames is always required Reducing num_workers \u00b6 Reduce the number of workers (subprocesses) used for data loading. By default num_workers will be set to 3. The minimum value is 0, which means that the data will be loaded in the main process, and the maximum is one less than the number of CPUs in the system. We recommend trying 1 if 3 is too many. CLI $ zamba predict --data-dir example_vids/ --num-workers 1 $ zamba train --data-dir example_vids/ --labels example_labels.csv --num-workers 1 Python In Python, add num_workers to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , num_workers = 1 ) Logging \u00b6 To check that videos are getting loaded and cached as expected, set your environment variabe LOG_LEVEL to DEBUG . The default log level is INFO . For example: $ LOG_LEVEL = DEBUG zamba predict --data-dir example_vids/","title":"Debugging"},{"location":"debugging/#debugging","text":"Before kicking off a full run of inference or model training, we recommend testing your code with a \"dry run\". If you are generating predictions, this will run one batch of inference to quickly detect any bugs. If you are trainig a model, this will run one training and validation batch for one epoch. If the dry run completes successfully, predict and train away! CLI $ zamba predict --data-dir example_vids/ --dry-run $ zamba train --data-dir example_vids/ --labels example_labels.csv --dry-run Python In Python, add dry_run=True to PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , dry_run = True )","title":"Debugging"},{"location":"debugging/#gpu-memory-errors","text":"The dry run will also catch any GPU memory errors. If you hit a GPU memory error, there are a couple fixes.","title":"GPU memory errors"},{"location":"debugging/#logging","text":"To check that videos are getting loaded and cached as expected, set your environment variabe LOG_LEVEL to DEBUG . The default log level is INFO . For example: $ LOG_LEVEL = DEBUG zamba predict --data-dir example_vids/","title":"Logging"},{"location":"extra-options/","text":"Guide to common optional parameters \u00b6 There are a LOT of ways to customize model training or inference. Here, we take that elephant-sized list of options and condense it to a manageable monkey-sized list of common considerations. To read about all possible customizations, see All Configuration Options . Many of the options below cannot be passed directly to the command line. Instead, some must be passed as part of a YAML configuration file. For example: $ zamba train --config path_to_your_config_file.yaml For using a YAML file with the Python package and other details, see the YAML Configuration File page. Downloading model weights \u00b6 zamba needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Model weights are stored on servers in three locations, and downloading weights from the server closest to you will run the fastest. By default, weights will be downloaded from the US. To specify a different region: CLI zamba predict --data-dir example_vids/ --weight_download_region asia Python In Python this can be specified in PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , weight_download_region = 'asia' , ) The options for weight_download_region are us , eu , and asia . Once a model's weights are downloaded, zamba will use the local version and will not need to perform this download again. Video size \u00b6 When zamba loads videos prior to either inference or training, it resizes all of the video frames before feeding them into a model. Higher resolution videos will lead to superior accuracy in prediction, but will use more memory and take longer to train and/or predict. The default video loading configuration for all pretrained models resizes images to 240x426 pixels. Say that you have a large number of videos, and you are more concerned with detecting blank v. non-blank videos than with identifying different species. In this case, you may not need a very high resolution and iterating through all of your videos with a high resolution would take a very long time. For example, to resize all images to 150x150 pixels instead of the default 240x426: YAML file video_loader_config : model_input_height : 150 model_input_width : 150 total_frames : 16 # total_frames must always be specified Python In Python, video resizing can be specified when VideoLoaderConfig is instantiated: from zamba.data.video import VideoLoaderConfig from zamba.models.config import PredictConfig from zamba.models.model_manager import predict_model predict_config = PredictConfig ( data_dir = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( model_input_height = 150 , model_input_width = 150 , total_frames = 16 ) # total_frames must always be specified predict_model ( predict_config = predict_config , video_loader_config = video_loader_config ) Frame selection \u00b6 Each video is simply a series of frames, or images. Most of the videos on which zamba was trained had 30 frames per second. That means even just a 15-second video would contain 450 frames. All models only use a subset of the frames in a video, because using every frame would be far too computationally intensive, and many frames are not different enough from each other to look at independently. There are a number of different ways to select frames. For a full list of options, see the section on Video loading arguments . A few common approaches are explained below. Early bias \u00b6 Some camera traps begin recording a video when movement is detected. If this is the case, you may be more likely to see an animal towards when the video starts. Setting early_bias to True selects 16 frames towards the beginning of a video. YAML File video_loader_config : early_bias : True # ... other parameters Python In Python, early_bias is specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( early_bias = True , ... ) This method was used by the winning solution of the Pri-matrix Factorization machine learning competition, which was the basis for zamba v1. This is a simple heuristic approach that is computationally cheap, and works decently for camera traps that are motion-triggered and short in total duration. Evenly distributed frames \u00b6 A simple option is to sample frames that are evenly distributed throughout a video. For example, to select 32 evenly distributed frames: YAML file video_loader_config : total_frames : 32 evenly_sample_total_frames : True ensure_total_frames : True # ... other parameters Python In Python, these arguments can be specified when VideoLoaderConfig is instantiated: video_loader_config = VideoLoaderConfig ( total_frames = 32 , evenly_sample_total_frames = True , ensure_total_frames = True , ... ) MegadetectorLite \u00b6 You can use a pretrained object detection model called MegadetectorLite to select only the frames that are mostly likely to contain an animal. This is the default strategy for all pretrained models. The parameter megadetector_lite_config is used to specify any arguments that should be passed to the MegadetectorLite model. If megadetector_lite_config is None, the MegadetectorLite model will not be used. For example, to take the 16 frames with the highest probability of detection: YAML file video_loader_config : megadetector_lite_config : n_frames : 16 fill_mode : \"score_sorted\" # ... other parameters Python In Python, these can be specified in the megadetector_lite_config argument passed to VideoLoaderConfig : video_loader_config = VideoLoaderConfig ( model_input_height = 240 , model_input_width = 426 , crop_bottom_pixels = 50 , ensure_total_frames = True , megadetector_lite_config = { \"confidence\" : 0.25 , \"fill_mode\" : \"score_sorted\" , \"n_frames\" : 16 , }, total_frames = 16 , ) train_config = TrainConfig ( data_dir = \"example_vids/\" , labels = \"example_labels.csv\" ,) train_model ( video_loader_config = video_loader_config , train_config = train_config ) If you are using the MegadetectorLite for frame selection, there are two ways that you can specify frame resizing: frame_selection_width and frame_selection_height resize images before they are input to the frame selection method (in this case, before being fed into MegadetectorLite). If both are None , the full size images will be used during frame selection . Using full size images for selection is recommended for better detection of smaller species, but will slow down training and inference. model_input_height and model_input_width resize images after frame selection. These specify the image size that is passed to the actual model for classification. You can specify both of the above at once, just one, or neither. The example code feeds full-size images to MegadetectorLite, and then resizes images before running them through the neural network. To see all of the options that can be passed to the MegadetectorLite, see the MegadetectorLiteYoloXConfig class . Speed up training \u00b6 Training will run faster if you increase num_workers and/or increase batch_size . num_workers is the number of subprocesses to use for data loading. The minimum is 0, meaning the data will be loaded in the main process, and the maximum is one less than the number of CPUs in your system. By default num_workers is set to 3 and batch_size is set to 2. Increasing either of these will use more GPU memory, and could raise an error if the memory required is more than your machine has available. You may need to try a few configuration of num_workers , batch_size and the image sizes above to settle on a configuration that works on your particular hardware. Both can be specified in either predict_config or train_config . For example, to increase num_workers to 5 and batch_size to 4 for inference: YAML file predict_config : data_dir : example_vids/ num_workers : 5 batch_size : 4 # ... other parameters Python predict_config = PredictConfig ( data_dir = \"example_vids/\" , num_workers = 5 , batch_size = 4 , # ... other parameters ) And that's just the tip of the iceberg! See All Configuration Options page for more possibilities.","title":"Guide to common optional parameters"},{"location":"extra-options/#guide-to-common-optional-parameters","text":"There are a LOT of ways to customize model training or inference. Here, we take that elephant-sized list of options and condense it to a manageable monkey-sized list of common considerations. To read about all possible customizations, see All Configuration Options . Many of the options below cannot be passed directly to the command line. Instead, some must be passed as part of a YAML configuration file. For example: $ zamba train --config path_to_your_config_file.yaml For using a YAML file with the Python package and other details, see the YAML Configuration File page.","title":"Guide to common optional parameters"},{"location":"extra-options/#downloading-model-weights","text":"zamba needs to download the \"weights\" files for the neural networks that it uses to make predictions. On first run it will download ~200-500 MB of files with these weights depending which model you choose. Model weights are stored on servers in three locations, and downloading weights from the server closest to you will run the fastest. By default, weights will be downloaded from the US. To specify a different region: CLI zamba predict --data-dir example_vids/ --weight_download_region asia Python In Python this can be specified in PredictConfig or TrainConfig : predict_config = PredictConfig ( data_dir = \"example_vids/\" , weight_download_region = 'asia' , ) The options for weight_download_region are us , eu , and asia . Once a model's weights are downloaded, zamba will use the local version and will not need to perform this download again.","title":"Downloading model weights"},{"location":"extra-options/#video-size","text":"When zamba loads videos prior to either inference or training, it resizes all of the video frames before feeding them into a model. Higher resolution videos will lead to superior accuracy in prediction, but will use more memory and take longer to train and/or predict. The default video loading configuration for all pretrained models resizes images to 240x426 pixels. Say that you have a large number of videos, and you are more concerned with detecting blank v. non-blank videos than with identifying different species. In this case, you may not need a very high resolution and iterating through all of your videos with a high resolution would take a very long time. For example, to resize all images to 150x150 pixels instead of the default 240x426: YAML file video_loader_config : model_input_height : 150 model_input_width : 150 total_frames : 16 # total_frames must always be specified Python In Python, video resizing can be specified when VideoLoaderConfig is instantiated: from zamba.data.video import VideoLoaderConfig from zamba.models.config import PredictConfig from zamba.models.model_manager import predict_model predict_config = PredictConfig ( data_dir = \"example_vids/\" ) video_loader_config = VideoLoaderConfig ( model_input_height = 150 , model_input_width = 150 , total_frames = 16 ) # total_frames must always be specified predict_model ( predict_config = predict_config , video_loader_config = video_loader_config )","title":"Video size"},{"location":"extra-options/#frame-selection","text":"Each video is simply a series of frames, or images. Most of the videos on which zamba was trained had 30 frames per second. That means even just a 15-second video would contain 450 frames. All models only use a subset of the frames in a video, because using every frame would be far too computationally intensive, and many frames are not different enough from each other to look at independently. There are a number of different ways to select frames. For a full list of options, see the section on Video loading arguments . A few common approaches are explained below.","title":"Frame selection"},{"location":"extra-options/#speed-up-training","text":"Training will run faster if you increase num_workers and/or increase batch_size . num_workers is the number of subprocesses to use for data loading. The minimum is 0, meaning the data will be loaded in the main process, and the maximum is one less than the number of CPUs in your system. By default num_workers is set to 3 and batch_size is set to 2. Increasing either of these will use more GPU memory, and could raise an error if the memory required is more than your machine has available. You may need to try a few configuration of num_workers , batch_size and the image sizes above to settle on a configuration that works on your particular hardware. Both can be specified in either predict_config or train_config . For example, to increase num_workers to 5 and batch_size to 4 for inference: YAML file predict_config : data_dir : example_vids/ num_workers : 5 batch_size : 4 # ... other parameters Python predict_config = PredictConfig ( data_dir = \"example_vids/\" , num_workers = 5 , batch_size = 4 , # ... other parameters ) And that's just the tip of the iceberg! See All Configuration Options page for more possibilities.","title":"Speed up training"},{"location":"install/","text":"Installing zamba \u00b6 Zamba has been developed and tested on macOS and Ubuntu Linux for both CPU and GPU configurations. To install zamba \u00b6 1. Install prerequisites \u00b6 Prerequisites: Python 3.7 or 3.8 FFmpeg Python 3.7 or 3.8 \u00b6 We recommend Python installation using Anaconda for all platforms. For more information about how to install Anaconda, here are some useful YouTube videos of installation: Anaconda download link Windows install video macOS installation video FFmpeg version 4 \u00b6 FFmpeg is an open source library for loading videos of different codecs. Using FFmpeg means that zamba can be flexible in terms of the video formats we support. FFmpeg can be installed on all different platforms, but requires some additional configuration depending on the platform. Here are some videos and instructions walking through FFmpeg installation: FFmpeg download link Install on Ubuntu or Linux . In the command line, enter sudo apt update and then sudo apt install ffmpeg . MacOS install video First, install Homebrew . Then run brew install ffmpeg@4 Follow the brew instructions to add FFmpeg to your path. To check that FFmpeg is installed, run ffmpeg : $ ffmpeg ffmpeg version 4.4 Copyright (c) 2000-2021 the FFmpeg developers built with Apple clang version 12.0.0 (clang-1200.0.32.29) ... To check your installed version, run ffmpeg -version . 2. Install zamba \u00b6 On macOS, run these commands in the terminal (\u2318+space, \"Terminal\"). On Windows, run them in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt). To install zamba: $ pip install https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz To check what version of zamba you have installed: $ pip show zamba To update zamba to the most recent version if needed: $ pip install -U https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz Operating systems that have been tested \u00b6 macOS \u00b6 zamba has been tested on macOS High Sierra. Linux \u00b6 zamba has been tested on Ubuntu versions 16 and 17. Note: zamba does not currently work on Windows because one of our dependencies fails to build. Using GPU(s) \u00b6 zamba is much faster on a machine with a graphics processing unit (GPU), but has also been developed and tested for machines without GPU(s). To use a GPU, you must be using an NVIDIA GPU , have installed and configured CUDA , and have installed and configured CuDNN per their specifications. If you are using conda , these dependencies can be installed through the cudatoolkit package . If using a GPU, you will also want to make sure that you install a compatible version of PyTorch with the version of CUDA you use. See the PyTorch installation docs for the easiest way to install the right version on your system.","title":"Installing zamba"},{"location":"install/#installing-zamba","text":"Zamba has been developed and tested on macOS and Ubuntu Linux for both CPU and GPU configurations.","title":"Installing zamba"},{"location":"install/#to-install-zamba","text":"","title":"To install zamba"},{"location":"install/#operating-systems-that-have-been-tested","text":"","title":"Operating systems that have been tested"},{"location":"install/#using-gpus","text":"zamba is much faster on a machine with a graphics processing unit (GPU), but has also been developed and tested for machines without GPU(s). To use a GPU, you must be using an NVIDIA GPU , have installed and configured CUDA , and have installed and configured CuDNN per their specifications. If you are using conda , these dependencies can be installed through the cudatoolkit package . If using a GPU, you will also want to make sure that you install a compatible version of PyTorch with the version of CUDA you use. See the PyTorch installation docs for the easiest way to install the right version on your system.","title":"Using GPU(s)"},{"location":"predict-tutorial/","text":"User tutorial: Classifying unlabeled videos \u00b6 This section walks through how to classify videos using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have unlabeled videos that you want to generate labels for The possible class species labels for your videos are included in the list of possible zamba labels . If your species are not included in this list, you can retrain a model using your own labeled data and then run inference. Basic usage: command line interface \u00b6 Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction in the command line: $ zamba predict --data-dir example_vids/ Required arguments \u00b6 To run zamba predict in the command line, you must specify --data-dir and/or --filepaths . --data-dir PATH : Path to the folder containing your videos. If you don't also provide filepaths , Zamba will recursively search this folder for videos. --filepaths PATH : Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for filepath . Filepaths can be absolute on your system or relative to the data directory that your provide in --data-dir . All other flags are optional. To choose the model you want to use for prediction, either --model or --checkpoint must be specified. Use --model to specify one of the pretrained models that ship with zamba . Use --checkpoint to run inference with a locally saved model. --model defaults to time_distributed . Basic usage: Python package \u00b6 Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction using the Python package: from zamba.models.model_manager import predict_model from zamba.models.config import PredictConfig predict_config = PredictConfig ( data_dir = \"example_vids/\" ) predict_model ( predict_config = predict_config ) The only two arguments that can be passed to predict_model are predict_config and (optionally) video_loader_config . The first step is to instantiate PredictConfig . Optionally, you can also specify video loading arguments by instantiating and passing in VideoLoaderConfig . Required arguments \u00b6 To run predict_model in Python, you must specify either data_dir or filepaths when PredictConfig is instantiated. data_dir (DirectoryPath) : Path to the folder containing your videos. If you don't also provide filepaths , Zamba will recursively search this folder for videos. filepaths (FilePath) : Path to a CSV file with a column for the filepath to each video you want to classify. The CSV must have a column for filepath . Filepaths can be absolute or relative to the data directory provided as data_dir . For detailed explanations of all possible configuration arguments, see All Optional Arguments . Default behavior \u00b6 By default, the time_distributed model will be used. zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) can be interpreted as the probability that animal j is present in video i. By default, predictions will be saved to a file called zamba_predictions.csv in your working directory. You can save predictions to a custom directory using the --save-dir argument. $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal eleph.MP4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 leopard.MP4,0.0,0.0,0.0,0.0,2e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 blank.MP4,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 chimp.MP4,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,4e-05,0.00162,0.0,0.0,0.0,0.0,0.0,2e-05,2e-05,0.0,1e-05,0.0,0.0038,4e-05,0.0 The full prediction and video loading configuration for the process will also be saved out, in the same folder as the predictions under predict_configuration.yaml . To run the exact same inference process a second time, you can pass this YAML file to zamba predict per the Using YAML Configuration Files page: $ zamba predict --config predict_configuration.yaml Step-by-step tutorial \u00b6 1. Specify the path to your videos \u00b6 Save all of your videos within one parent folder. Videos can be in nested subdirectories within the folder. Your videos should be in be saved in formats that are suppored by FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. By default, zamba will look for files with the following suffixes: .avi , .mp4 , .asf . To use other video suffixes that are supported by FFmpeg, set your VIDEO_SUFFIXES environment variable. Add the path to your video folder. For example, if your videos are in a folder called example_vids : CLI $ zamba predict --data-dir example_vids/ Python predict_config = PredictConfig ( data_dir = 'example_vids/' ) predict_model ( predict_config = predict_config ) 2. Choose a model for prediction \u00b6 If your camera videos contain species common to Central or West Africa, use either the time_distributed model or slowfast model model. slowfast is better for blank and small species detection. time_distributed performs better if you have many different species of interest, or are focused on duikers, chimpanzees, and/or gorillas. If your videos contain species common to Europe, use the european model . Add the model name to your command. The time_distributed model will be used if no model is specified. For example, if you want to use the slowfast model to classify the videos in example_vids : CLI $ zamba predict --data-dir example_vids/ --model slowfast Python predict_config = PredictConfig ( data_dir = 'example_vids/' , model_name = 'slowfast' ) predict_model ( predict_config = predict_config ) 3. Choose the output format \u00b6 There are three options for how to format predictions, listed from most information to least: Store all probabilities (default): Return predictions with a row for each filename and a column for each class label, with probabilities between 0 and 1. Cell (i,j) is the probability that animal j is present in video i . Presence/absence: Return predictions with a row for each filename and a column for each class label, with cells indicating either presence or absense based on a user-specified probability threshold. Cell (i, j) indicates whether animal j is present ( 1 ) or not present ( 0 ) in video i . The probability threshold cutoff is specified with --proba-threshold in the CLI. Most likely class: Return predictions with a row for each filename and one column for the most likely class in each video. The most likely class can also be blank. To get the most likely class, add --output-class-names to your command. In Python, it can be specified by adding output_class_names=True when PredictConfig is instantiated. This is not recommended if you'd like to detect more than one species in each video. Say we want to generate predictions for the videos in example_vids indicating which animals are present in each video based on a probability threshold of 50%: CLI $ zamba predict --data-dir example_vids/ --proba-threshold 0 .5 $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal eleph.MP4,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 leopard.MP4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0 blank.MP4,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 chimp.MP4,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 Python predict_config = PredictConfig ( data_dir = \"example_vids/\" , proba_threshold = 0.5 ) predict_model ( predict_config = predict_config ) predictions = pd . read_csv ( \"zamba_predictions.csv\" ) predictions filepath aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal blank.MP4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 chimp.MP4 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 eleph.MP4 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 leopard.MP4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 4. Specify any additional parameters \u00b6 And there's so much more! You can also do things like specify your region for faster model download ( --weight-download-region ), use a saved model checkpoint ( --checkpoint ), or specify a different folder where your predictions should be saved ( --save-dir ). To read about a few common considerations, see the Guide to Common Optional Parameters page. 5. Test your configuration with a dry run \u00b6 Before kicking off a full run of inference, we recommend testing your code with a \"dry run\". This will run one batch of inference to quickly detect any bugs. See the Debugging page for details.","title":"Classifying unlabeled videos"},{"location":"predict-tutorial/#user-tutorial-classifying-unlabeled-videos","text":"This section walks through how to classify videos using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have unlabeled videos that you want to generate labels for The possible class species labels for your videos are included in the list of possible zamba labels . If your species are not included in this list, you can retrain a model using your own labeled data and then run inference.","title":"User tutorial: Classifying unlabeled videos"},{"location":"predict-tutorial/#basic-usage-command-line-interface","text":"Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction in the command line: $ zamba predict --data-dir example_vids/","title":"Basic usage: command line interface"},{"location":"predict-tutorial/#basic-usage-python-package","text":"Say that we want to classify the videos in a folder called example_vids as simply as possible using all of the default settings. Minimum example for prediction using the Python package: from zamba.models.model_manager import predict_model from zamba.models.config import PredictConfig predict_config = PredictConfig ( data_dir = \"example_vids/\" ) predict_model ( predict_config = predict_config ) The only two arguments that can be passed to predict_model are predict_config and (optionally) video_loader_config . The first step is to instantiate PredictConfig . Optionally, you can also specify video loading arguments by instantiating and passing in VideoLoaderConfig .","title":"Basic usage: Python package"},{"location":"predict-tutorial/#default-behavior","text":"By default, the time_distributed model will be used. zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) can be interpreted as the probability that animal j is present in video i. By default, predictions will be saved to a file called zamba_predictions.csv in your working directory. You can save predictions to a custom directory using the --save-dir argument. $ cat zamba_predictions.csv filepath,aardvark,antelope_duiker,badger,bat,bird,blank,cattle,cheetah,chimpanzee_bonobo,civet_genet,elephant,equid,forest_buffalo,fox,giraffe,gorilla,hare_rabbit,hippopotamus,hog,human,hyena,large_flightless_bird,leopard,lion,mongoose,monkey_prosimian,pangolin,porcupine,reptile,rodent,small_cat,wild_dog_jackal eleph.MP4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 leopard.MP4,0.0,0.0,0.0,0.0,2e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 blank.MP4,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 chimp.MP4,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1e-05,4e-05,0.00162,0.0,0.0,0.0,0.0,0.0,2e-05,2e-05,0.0,1e-05,0.0,0.0038,4e-05,0.0 The full prediction and video loading configuration for the process will also be saved out, in the same folder as the predictions under predict_configuration.yaml . To run the exact same inference process a second time, you can pass this YAML file to zamba predict per the Using YAML Configuration Files page: $ zamba predict --config predict_configuration.yaml","title":"Default behavior"},{"location":"predict-tutorial/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"quickstart/","text":"Quickstart \u00b6 This section assumes you have successfully installed zamba and are ready to train a model or identify species in your videos! zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. To perform inference, you simply need to run zamba predict followed by a set of arguments that let zamba know where your videos are located, which model you want to use, and where to save your output. To train a model, you can similarly run zamba train and specify your labels. The following sections provide details about these separate modules. There are two ways to interact with the zamba package: Use zamba as a command line interface tool. This page provides an overview of how to use the CLI. Import zamba in Python and use it as a Python package. This guide uses the CLI, but you can see the prediction tutorial or the training tutorial , which have both the CLI and Python approaches documented. Installation is the same for both the command line interface tool and the Python package. All of the commands on this page should be run at the command line. On macOS, this can be done in the terminal (\u2318+space, \"Terminal\"). On Windows, this can be done in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt). How do I organize my videos for zamba ? \u00b6 You can specify the path to a directory of videos or specify a list of filepaths in a .csv file. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. For example, say we have a directory of videos called example_vids that we want to generate predictions for using zamba . Let's list the videos: $ ls example_vids/ blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 Here are some screenshots from those videos: blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 In this example, the videos have meaningful names so that we can easily compare the predictions made by zamba . In practice, your videos will probably be named something much less useful! Generating predictions \u00b6 To generate and save predictions for your videos using the default settings, run: $ zamba predict --data-dir example_vids/ zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) is the probability that animal j is present in video i . Comprehensive predictions are helpful when a single video contains multiple species. Predictions will be saved to zamba_predictions.csv in the current working directory by default. You can save out predictions to a different folder using the --save-dir argument. Adding the argument --output-class-names will simplify the predictions to return only the most likely animal in each video: $ zamba predict --data-dir example_vids/ --output-class-names $ cat zamba_predictions.csv blank.mp4,blank chimp.mp4,chimpanzee_bonobo eleph.mp4,elephant leopard.mp4,leopard There are pretrained models that ship with zamba : blank_nonblank , time_distributed , slowfast , and european . Which model you should use depends on your priorities and geography (see the Available Models page for more details). By default zamba will use the time_distributed model. Add the --model argument to specify one of other options: $ zamba predict --data-dir example_vids/ --model slowfast Training a model \u00b6 You can continue training one of the models that ships with zamba by either: Finetuning with additional labeled videos where the species are included in the list of zamba class labels Finetuning with labeled videos that include new species In either case, the commands for training are the same. Say that we have labels for the videos in the example_vids folder saved in example_labels.csv . To train a model, run: $ zamba train --data-dir example_vids/ --labels example_labels.csv The labels file must have columns for both filepath and label. The filepath column should contain either absolute paths or paths relative to the data-dir . Optionally, there can also be columns for split ( train , val , or holdout ) and site . Let's print the example labels: $ cat example_labels.csv filepath,label blank.MP4,blank chimp.MP4,chimpanzee_bonobo eleph.MP4,elephant leopard.MP4,leopard By default, the trained model and additional training output will be saved to a version_n folder in the current working directory. For example, $ zamba train --data-dir example_vids/ --labels example_labels.csv $ ls version_0/ hparams.yaml time_distributed.ckpt train_configuration.yamml val_metrics.json ... Downloading model weights \u00b6 zamba needs to download the \"weights\" files for the models it uses to make predictions. On first run, it will download ~200-500 MB of files with these weights depending which model you choose. Once a model's weights are downloaded, zamba will use the local version and will not need to perform this download again. If you are not in the United States, we recommend running the above command with the additional flag either --weight_download_region eu or --weight_download_region asia depending on your location. The closer you are to the server, the faster the downloads will be. Getting help \u00b6 Once zamba is installed, you can see more details of each function with --help . For example, you can run zamba predict --help : Usage: zamba predict [OPTIONS] Identify species in a video. This is a command line interface for prediction on camera trap footage. Given a path to camera trap footage, the predict function use a deep learning model to predict the presence or absense of a variety of species of common interest to wildlife researchers working with camera trap data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --filepaths PATH Path to csv containing `filepath` column with videos. --model [time_distributed|slowfast|european] Model to use for inference. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for inference. If provided, model is not required. --gpus INTEGER Number of GPUs to use for inference. If not specifiied, will use all GPUs found on machine. --batch-size INTEGER Batch size to use for training. --save / --no-save Whether to save out predictions. If you want to specify the output directory, use save_dir instead. --save-dir PATH An optional directory in which to save the model predictions and configuration yaml. Defaults to the current working directory if save is True. --dry-run / --no-dry-run Runs one batch of inference to check for bugs. --config PATH Specify options using yaml configuration file instead of through command line options. --proba-threshold FLOAT Probability threshold for classification between 0 and 1. If specified binary predictions are returned with 1 being greater than the threshold, 0 being less than or equal to. If not specified, probabilities between 0 and 1 are returned. --output-class-names / --no-output-class-names If True, we just return a video and the name of the most likely class. If False, we return a probability or indicator (depending on --proba_threshold) for every possible class. --num-workers INTEGER Number of subprocesses to use for data loading. --weight-download-region [us|eu|asia] Server region for downloading weights. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to inference. Only use if you're very confident all your videos can be loaded. -o, --overwrite Overwrite outputs in the save directory if they exist. -y, --yes Skip confirmation of configuration and proceed right to prediction. --help Show this message and exit. Or if you are training a model, you can run zamba train --help : $ zamba train --help Usage: zamba train [OPTIONS] Train a model on your labeled data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --labels PATH Path to csv containing video labels. --model [time_distributed|slowfast|european] Model to train. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for training. If provided, model is not required. --config PATH Specify options using yaml configuration file instead of through command line options. --batch-size INTEGER Batch size to use for training. --gpus INTEGER Number of GPUs to use for training. If not specifiied, will use all GPUs found on machine. --dry-run / --no-dry-run Runs one batch of train and validation to check for bugs. --save-dir PATH An optional directory in which to save the model checkpoint and configuration file. If not specified, will save to a `version_n` folder in your working directory. --num-workers INTEGER Number of subprocesses to use for data loading. --weight-download-region [us|eu|asia] Server region for downloading weights. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to training. Only use if you're very confident all your videos can be loaded. -y, --yes Skip confirmation of configuration and proceed right to training. --help Show this message and exit.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This section assumes you have successfully installed zamba and are ready to train a model or identify species in your videos! zamba can be used \"out of the box\" to generate predictions or train a model using your own videos. To perform inference, you simply need to run zamba predict followed by a set of arguments that let zamba know where your videos are located, which model you want to use, and where to save your output. To train a model, you can similarly run zamba train and specify your labels. The following sections provide details about these separate modules. There are two ways to interact with the zamba package: Use zamba as a command line interface tool. This page provides an overview of how to use the CLI. Import zamba in Python and use it as a Python package. This guide uses the CLI, but you can see the prediction tutorial or the training tutorial , which have both the CLI and Python approaches documented. Installation is the same for both the command line interface tool and the Python package. All of the commands on this page should be run at the command line. On macOS, this can be done in the terminal (\u2318+space, \"Terminal\"). On Windows, this can be done in a command prompt, or if you installed Anaconda an anaconda prompt (Start > Anaconda3 > Anaconda Prompt).","title":"Quickstart"},{"location":"quickstart/#how-do-i-organize-my-videos-for-zamba","text":"You can specify the path to a directory of videos or specify a list of filepaths in a .csv file. zamba supports the same video formats as FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. For example, say we have a directory of videos called example_vids that we want to generate predictions for using zamba . Let's list the videos: $ ls example_vids/ blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 Here are some screenshots from those videos: blank.mp4 chimp.mp4 eleph.mp4 leopard.mp4 In this example, the videos have meaningful names so that we can easily compare the predictions made by zamba . In practice, your videos will probably be named something much less useful!","title":"How do I organize my videos for zamba?"},{"location":"quickstart/#generating-predictions","text":"To generate and save predictions for your videos using the default settings, run: $ zamba predict --data-dir example_vids/ zamba will output a .csv file with rows labeled by each video filename and columns for each class (ie. species). The default prediction will store all class probabilities, so that cell (i,j) is the probability that animal j is present in video i . Comprehensive predictions are helpful when a single video contains multiple species. Predictions will be saved to zamba_predictions.csv in the current working directory by default. You can save out predictions to a different folder using the --save-dir argument. Adding the argument --output-class-names will simplify the predictions to return only the most likely animal in each video: $ zamba predict --data-dir example_vids/ --output-class-names $ cat zamba_predictions.csv blank.mp4,blank chimp.mp4,chimpanzee_bonobo eleph.mp4,elephant leopard.mp4,leopard There are pretrained models that ship with zamba : blank_nonblank , time_distributed , slowfast , and european . Which model you should use depends on your priorities and geography (see the Available Models page for more details). By default zamba will use the time_distributed model. Add the --model argument to specify one of other options: $ zamba predict --data-dir example_vids/ --model slowfast","title":"Generating predictions"},{"location":"quickstart/#training-a-model","text":"You can continue training one of the models that ships with zamba by either: Finetuning with additional labeled videos where the species are included in the list of zamba class labels Finetuning with labeled videos that include new species In either case, the commands for training are the same. Say that we have labels for the videos in the example_vids folder saved in example_labels.csv . To train a model, run: $ zamba train --data-dir example_vids/ --labels example_labels.csv The labels file must have columns for both filepath and label. The filepath column should contain either absolute paths or paths relative to the data-dir . Optionally, there can also be columns for split ( train , val , or holdout ) and site . Let's print the example labels: $ cat example_labels.csv filepath,label blank.MP4,blank chimp.MP4,chimpanzee_bonobo eleph.MP4,elephant leopard.MP4,leopard By default, the trained model and additional training output will be saved to a version_n folder in the current working directory. For example, $ zamba train --data-dir example_vids/ --labels example_labels.csv $ ls version_0/ hparams.yaml time_distributed.ckpt train_configuration.yamml val_metrics.json ...","title":"Training a model"},{"location":"quickstart/#downloading-model-weights","text":"zamba needs to download the \"weights\" files for the models it uses to make predictions. On first run, it will download ~200-500 MB of files with these weights depending which model you choose. Once a model's weights are downloaded, zamba will use the local version and will not need to perform this download again. If you are not in the United States, we recommend running the above command with the additional flag either --weight_download_region eu or --weight_download_region asia depending on your location. The closer you are to the server, the faster the downloads will be.","title":"Downloading model weights"},{"location":"quickstart/#getting-help","text":"Once zamba is installed, you can see more details of each function with --help . For example, you can run zamba predict --help : Usage: zamba predict [OPTIONS] Identify species in a video. This is a command line interface for prediction on camera trap footage. Given a path to camera trap footage, the predict function use a deep learning model to predict the presence or absense of a variety of species of common interest to wildlife researchers working with camera trap data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --filepaths PATH Path to csv containing `filepath` column with videos. --model [time_distributed|slowfast|european] Model to use for inference. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for inference. If provided, model is not required. --gpus INTEGER Number of GPUs to use for inference. If not specifiied, will use all GPUs found on machine. --batch-size INTEGER Batch size to use for training. --save / --no-save Whether to save out predictions. If you want to specify the output directory, use save_dir instead. --save-dir PATH An optional directory in which to save the model predictions and configuration yaml. Defaults to the current working directory if save is True. --dry-run / --no-dry-run Runs one batch of inference to check for bugs. --config PATH Specify options using yaml configuration file instead of through command line options. --proba-threshold FLOAT Probability threshold for classification between 0 and 1. If specified binary predictions are returned with 1 being greater than the threshold, 0 being less than or equal to. If not specified, probabilities between 0 and 1 are returned. --output-class-names / --no-output-class-names If True, we just return a video and the name of the most likely class. If False, we return a probability or indicator (depending on --proba_threshold) for every possible class. --num-workers INTEGER Number of subprocesses to use for data loading. --weight-download-region [us|eu|asia] Server region for downloading weights. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to inference. Only use if you're very confident all your videos can be loaded. -o, --overwrite Overwrite outputs in the save directory if they exist. -y, --yes Skip confirmation of configuration and proceed right to prediction. --help Show this message and exit. Or if you are training a model, you can run zamba train --help : $ zamba train --help Usage: zamba train [OPTIONS] Train a model on your labeled data. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to folder containing videos. --labels PATH Path to csv containing video labels. --model [time_distributed|slowfast|european] Model to train. Model will be superseded by checkpoint if provided. [default: time_distributed] --checkpoint PATH Model checkpoint path to use for training. If provided, model is not required. --config PATH Specify options using yaml configuration file instead of through command line options. --batch-size INTEGER Batch size to use for training. --gpus INTEGER Number of GPUs to use for training. If not specifiied, will use all GPUs found on machine. --dry-run / --no-dry-run Runs one batch of train and validation to check for bugs. --save-dir PATH An optional directory in which to save the model checkpoint and configuration file. If not specified, will save to a `version_n` folder in your working directory. --num-workers INTEGER Number of subprocesses to use for data loading. --weight-download-region [us|eu|asia] Server region for downloading weights. --skip-load-validation / --no-skip-load-validation Skip check that verifies all videos can be loaded prior to training. Only use if you're very confident all your videos can be loaded. -y, --yes Skip confirmation of configuration and proceed right to training. --help Show this message and exit.","title":"Getting help"},{"location":"train-tutorial/","text":"User tutorial: Training a model on labeled videos \u00b6 This section walks through how to train a model using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have labeled videos that you want to use to train or finetune a model zamba can run two types of model training: Finetuning a model with labels that are a subset of the possible zamba labels Finetuning a model to predict an entirely new set of labels The process is the same for both cases. Basic usage: command line interface \u00b6 By default, the time_distributed species classification model is used. Say that we want to finetune that model based on the videos in example_vids and the labels in example_labels.csv . $ cat example_labels.csv filepath,label blank.MP4,blank chimp.MP4,chimpanzee_bonobo eleph.MP4,elephant leopard.MP4,leopard Training at the command line would look like: $ zamba train --data-dir example_vids/ --labels example_labels.csv Required arguments \u00b6 To run zamba train in the command line, you must specify labels . --labels PATH : Path to a CSV containing the video labels to use as ground truth during training. There must be columns for both filepath and label . Optionally, there can also be columns for split (which can have one of the three values for each row: train , val , or holdout ) or site (which can contain any string identifying the location of the camera, used to allocate videos to splits if not already specified). If the video filepaths in the labels csv are not absolute, be sure to provide the data-dir to which the filepaths are relative. --data-dir PATH : Path to the folder containing your labeled videos. Basic usage: Python package \u00b6 To do the same thing as above using the library code, this would look like: from zamba.models.model_manager import train_model from zamba.models.config import TrainConfig train_config = TrainConfig ( data_dir = \"example_vids/\" , labels = \"example_labels.csv\" ) train_model ( train_config = train_config ) The only two arguments that can be passed to train_model are train_config and (optionally) video_loader_config . The first step is to instantiate TrainConfig . Optionally, you can also specify video loading arguments by instantiating and passing in VideoLoaderConfig . You'll want to go over the documentation to familiarize yourself with the options in both of these configurations since what you choose can have a large impact on the results of your model. We've tried to include in the documentation sane defaults and recommendations for how to set these parameters. For detailed explanations of all possible configuration arguments, see All Configuration Options . Model output classes \u00b6 The classes your trained model will predict are determined by which model you choose and whether the species in your labels are a subset of that model's default labels . This table outlines the default behavior for a set of common scenarios. Classes in labels csv Model What we infer Classes trained model predicts cat, blank blank_nonblank binary model where one is \"blank\" blank zebra, grizzly, blank time_distributed multiclass but not a subset of the zamba labels zebra, grizzly, blank elephant, antelope_duiker, blank time_distributed multiclass and a subset of the zamba labels all African forest zamba species Step-by-step tutorial \u00b6 1. Specify the path to your videos \u00b6 Save all of your videos in a folder. They can be in nested directories within the folder. Your videos should all be saved in formats that are suppored by FFmpeg, which are listed here . Any videos that fail a set of FFmpeg checks will be skipped during inference or training. By default, zamba will look for files with the following suffixes: .avi , .mp4 , .asf . To use other video suffixes that are supported by FFmpeg, set your VIDEO_SUFFIXES environment variable. Add the path to your video folder with --data-dir . For example, if your videos are in a folder called example_vids , add --data-dir example_vids/ to your command. CLI $ zamba train --data-dir example_vids Python from zamba.models.config import TrainConfig from zamba.models.model_manager import train_model train_config = TrainConfig ( data_dir = 'example_vids/' ) train_model ( train_config = train_config ) Note that the above will not run yet because labels are not specified. The more training data you have, the better the resulting model will be. We recommend having a minimum of 100 videos per species . Having an imbalanced dataset - for example, where most of the videos are blank - is okay as long as there are enough examples of each individual species. 2. Specify your labels \u00b6 Your labels should be saved in a .csv file with columns for filepath and label. For example: $ cat example_labels.csv filepath,label eleph.MP4,elephant leopard.MP4,leopard blank.MP4,blank chimp.MP4,chimpanzee_bonobo Add the path to your labels with --labels . For example, if your videos are in a folder called example_vids and your labels are saved in example_labels.csv : CLI $ zamba train --data-dir example_vids/ --labels example_labels.csv Python In Python, the labels are passed in when TrainConfig is instantiated. The Python package allows you to pass in labels as either a file path or a pandas dataframe: labels_dataframe = pd . read_csv ( 'example_labels.csv' , index_col = 'filepath' ) train_config = TrainConfig ( data_dir = 'example_vids/' , labels = labels_dataframe ) train_model ( train_config = train_config ) Labels zamba has seen before \u00b6 Your labels may be included in the list of zamba class labels that the provided models are trained to predict. If so, the relevant model that ships with zamba will essentially be used as a checkpoint, and model training will resume from that checkpoint. By default, the model you train will continue to output all of the Zamba class labels, not just the ones in your dataset. For different behavior, see use_default_model_labels . Completely new labels \u00b6 You can also train a model to predict completely new labels - the world is your oyster! (We'd love to see a model trained to predict oysters.) If this is the case, the model architecture will replace the final neural network layer with a new head that predicts your labels instead of those that ship with zamba . You can then make your model available to others by adding it to the Model Zoo on our wiki . 3. Choose a model for training \u00b6 Any of the models that ship with zamba can be trained. If you're training on entirely new species or new ecologies, we recommend starting with the time_distributed model as this model is less computationally intensive than the slowfast model . However, if you're tuning a model to a subset of species (e.g. a european_beaver or blank model), use the model that was trained on data that is most similar to your new data. Add the model name to your command with --model . The time_distributed model will be used if no model is specified. For example, if you want to continue training the european model based on the videos in example_euro_vids and the labels in example_euro_labels.csv : CLI $ zamba train --data-dir example_euro_vids/ --labels example_euro_labels.csv --model european Python train_config = TrainConfig ( data_dir = \"example_euro_vids/\" , labels = \"example_euro_labels.csv\" , model_name = \"european\" , ) train_model ( train_config = train_config ) 4. Specify any additional parameters \u00b6 And there's so much more! You can also do things like specify your region for faster model download ( --weight-download-region ), start training from a saved model checkpoint ( --checkpoint ), or specify a different path where your model should be saved ( --save-dir ). To read about a few common considerations, see the Guide to Common Optional Parameters page. 5. Test your configuration with a dry run \u00b6 Before kicking off the full model training, we recommend testing your code with a \"dry run\". This will run one training and validation batch for one epoch to quickly detect any bugs. See the Debugging page for details. Files that get written out during training \u00b6 You can specify where the outputs should be saved with --save-dir . If no save directory is specified, zamba will write out incremental version_n folders to your current working directory. For example, a model finetuned from the provided time_distributed model (the default) will be saved in version_0 . version_0 contains: train_configuration.yaml : The full model configuration used to generate the given model, including video_loader_config and train_config . To continue training using the same configuration, or to train another model using the same configuration, you can pass in train_configurations.yaml (see Specifying Model Configurations with a YAML File ) along with the labels filepath. hparams.yaml : Model hyperparameters. These are included in the checkpoint file as well. time_distributed.ckpt : Model checkpoint. You can continue training from this checkpoint by passing it to zamba train with the --checkpoint flag: $ zamba train --checkpoint version_0/time_distributed.ckpt --data-dir example_vids/ --labels example_labels.csv events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 : TensorBoard logs. You can view these with tensorboard: $ tensorboard --logdir version_0/ val_metrics.json : The model's performance on the validation subset test_metrics.json : The model's performance on the test (holdout) subset splits.csv : Which files were used for training, validation, and as a holdout set. If split is specified in the labels file passed to training, splits.csv will not be saved out.","title":"Training a model on labeled videos"},{"location":"train-tutorial/#user-tutorial-training-a-model-on-labeled-videos","text":"This section walks through how to train a model using zamba . If you are new to zamba and just want to classify some videos as soon as possible, see the Quickstart guide. This tutorial goes over the steps for using zamba if: You already have zamba installed (for details see the Installation page) You have labeled videos that you want to use to train or finetune a model zamba can run two types of model training: Finetuning a model with labels that are a subset of the possible zamba labels Finetuning a model to predict an entirely new set of labels The process is the same for both cases.","title":"User tutorial: Training a model on labeled videos"},{"location":"train-tutorial/#basic-usage-command-line-interface","text":"By default, the time_distributed species classification model is used. Say that we want to finetune that model based on the videos in example_vids and the labels in example_labels.csv . $ cat example_labels.csv filepath,label blank.MP4,blank chimp.MP4,chimpanzee_bonobo eleph.MP4,elephant leopard.MP4,leopard Training at the command line would look like: $ zamba train --data-dir example_vids/ --labels example_labels.csv","title":"Basic usage: command line interface"},{"location":"train-tutorial/#basic-usage-python-package","text":"To do the same thing as above using the library code, this would look like: from zamba.models.model_manager import train_model from zamba.models.config import TrainConfig train_config = TrainConfig ( data_dir = \"example_vids/\" , labels = \"example_labels.csv\" ) train_model ( train_config = train_config ) The only two arguments that can be passed to train_model are train_config and (optionally) video_loader_config . The first step is to instantiate TrainConfig . Optionally, you can also specify video loading arguments by instantiating and passing in VideoLoaderConfig . You'll want to go over the documentation to familiarize yourself with the options in both of these configurations since what you choose can have a large impact on the results of your model. We've tried to include in the documentation sane defaults and recommendations for how to set these parameters. For detailed explanations of all possible configuration arguments, see All Configuration Options .","title":"Basic usage: Python package"},{"location":"train-tutorial/#model-output-classes","text":"The classes your trained model will predict are determined by which model you choose and whether the species in your labels are a subset of that model's default labels . This table outlines the default behavior for a set of common scenarios. Classes in labels csv Model What we infer Classes trained model predicts cat, blank blank_nonblank binary model where one is \"blank\" blank zebra, grizzly, blank time_distributed multiclass but not a subset of the zamba labels zebra, grizzly, blank elephant, antelope_duiker, blank time_distributed multiclass and a subset of the zamba labels all African forest zamba species","title":"Model output classes"},{"location":"train-tutorial/#step-by-step-tutorial","text":"","title":"Step-by-step tutorial"},{"location":"train-tutorial/#files-that-get-written-out-during-training","text":"You can specify where the outputs should be saved with --save-dir . If no save directory is specified, zamba will write out incremental version_n folders to your current working directory. For example, a model finetuned from the provided time_distributed model (the default) will be saved in version_0 . version_0 contains: train_configuration.yaml : The full model configuration used to generate the given model, including video_loader_config and train_config . To continue training using the same configuration, or to train another model using the same configuration, you can pass in train_configurations.yaml (see Specifying Model Configurations with a YAML File ) along with the labels filepath. hparams.yaml : Model hyperparameters. These are included in the checkpoint file as well. time_distributed.ckpt : Model checkpoint. You can continue training from this checkpoint by passing it to zamba train with the --checkpoint flag: $ zamba train --checkpoint version_0/time_distributed.ckpt --data-dir example_vids/ --labels example_labels.csv events.out.tfevents.1632250686.ip-172-31-15-179.14229.0 : TensorBoard logs. You can view these with tensorboard: $ tensorboard --logdir version_0/ val_metrics.json : The model's performance on the validation subset test_metrics.json : The model's performance on the test (holdout) subset splits.csv : Which files were used for training, validation, and as a holdout set. If split is specified in the labels file passed to training, splits.csv will not be saved out.","title":"Files that get written out during training"},{"location":"yaml-config/","text":"Using YAML configuration files \u00b6 In both the command line and the Python module, options for video loading, training, and prediction can be set by passing a YAML file instead of passing arguments directly. YAML files ( .yml or .yaml ) are commonly used to serialize data in an easily readable way. The basic structure of a YAML model configuration is: $ cat basic_config.yaml video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 # other video loading parameters train_config : model_name : time_distributed data_dir : example_vids/ labels : example_labels.csv # other training parameters, eg. batch_size predict_config : model_name : time_distributed data_dir : example_vids/ # other training parameters, eg. batch_size For example, the configuration below will predict labels for the videos in example_vids using the time_distributed model. When videos are loaded, each will be resized to 240x426 pixels and 16 frames will be selected: video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 predict_config : model_name : time_distributed data_dir : example_vids/ Required arguments \u00b6 Either predict_config or train_config is required, based on whether you will be running inference or training a model. See All Configuration Options for a full list of what can be specified under each class. To run inference, data_dir and/or filepaths must be specified. To train a model, labels must be specified. In video_loader_config , you must specify at least model_input_height , model_input_width , and total_frames . While this is the minimum required, we strongly recommend being intentional in your choice of frame selection method. total_frames by itself will just take the first n frames. For a full list of frame selection methods, see the section on Video loading arguments . For time_distributed or european , total_frames must be 16 For slowfast , total_frames must be 32 Command line interface \u00b6 A YAML configuration file can be passed to the command line interface with the --config argument. For example, say the example configuration above is saved as example_config.yaml . To run prediction: $ zamba predict --config example_config.yaml Only some of the possible parameters can be passed directly as arguments to the command line. Those not listed in zamba predict --help or zamba train --help must be passed in a YAML file (see the Quickstart guide for details). Python package \u00b6 The main API for zamba is the ModelManager class that can be accessed with: from zamba.models.manager import ModelManager The ModelManager class is used by zamba \u2019s command line interface to handle preprocessing the filenames, loading the videos, training the model, performing inference, and saving predictions. Therefore any functionality available to the command line interface is accessible via the ModelManager class. To instantiate the ModelManager based on a configuration file saved at test_config.yaml : >>> manager = ModelManager . from_yaml ( 'test_config.yaml' ) We can now run inference or model training without specifying any additional parameters, because they are already associated with our instance of the ModelManager class. To run inference or training: manager . predict () # inference manager . train () # training In our user tutorials, we refer to train_model and predict_model functions. The ModelManager class calls these same functions behind the scenes when .predict() or .train() is run. Default configurations \u00b6 In the command line, the default configuration for each model is passed in using a specified YAML file that ships with zamba . You can see the default configuration YAML files on Github in the config.yaml file within each model's folder. For example, the default configuration for the time_distributed model is: train_config : scheduler_config : scheduler : MultiStepLR scheduler_params : gamma : 0.5 milestones : - 3 verbose : true model_name : time_distributed backbone_finetune_config : backbone_initial_ratio_lr : 0.01 multiplier : 1 pre_train_bn : true train_bn : false unfreeze_backbone_at_epoch : 3 verbose : true early_stopping_config : patience : 5 video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 4 total_frames : 16 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 16 predict_config : model_name : time_distributed public_checkpoint : time_distributed_9e710aa8c92d25190a64b3b04b9122bdcb456982.ckpt Templates \u00b6 To make modifying the existing defaults easier, we've set up the official models as templates in the templates folder . Just fill in your data directory and labels, make any desired tweaks to the model config, and then kick off some training . Happy modeling!","title":"Using YAML configuration files"},{"location":"yaml-config/#using-yaml-configuration-files","text":"In both the command line and the Python module, options for video loading, training, and prediction can be set by passing a YAML file instead of passing arguments directly. YAML files ( .yml or .yaml ) are commonly used to serialize data in an easily readable way. The basic structure of a YAML model configuration is: $ cat basic_config.yaml video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 # other video loading parameters train_config : model_name : time_distributed data_dir : example_vids/ labels : example_labels.csv # other training parameters, eg. batch_size predict_config : model_name : time_distributed data_dir : example_vids/ # other training parameters, eg. batch_size For example, the configuration below will predict labels for the videos in example_vids using the time_distributed model. When videos are loaded, each will be resized to 240x426 pixels and 16 frames will be selected: video_loader_config : model_input_height : 240 model_input_width : 426 total_frames : 16 predict_config : model_name : time_distributed data_dir : example_vids/","title":"Using YAML configuration files"},{"location":"yaml-config/#required-arguments","text":"Either predict_config or train_config is required, based on whether you will be running inference or training a model. See All Configuration Options for a full list of what can be specified under each class. To run inference, data_dir and/or filepaths must be specified. To train a model, labels must be specified. In video_loader_config , you must specify at least model_input_height , model_input_width , and total_frames . While this is the minimum required, we strongly recommend being intentional in your choice of frame selection method. total_frames by itself will just take the first n frames. For a full list of frame selection methods, see the section on Video loading arguments . For time_distributed or european , total_frames must be 16 For slowfast , total_frames must be 32","title":"Required arguments"},{"location":"yaml-config/#command-line-interface","text":"A YAML configuration file can be passed to the command line interface with the --config argument. For example, say the example configuration above is saved as example_config.yaml . To run prediction: $ zamba predict --config example_config.yaml Only some of the possible parameters can be passed directly as arguments to the command line. Those not listed in zamba predict --help or zamba train --help must be passed in a YAML file (see the Quickstart guide for details).","title":"Command line interface"},{"location":"yaml-config/#python-package","text":"The main API for zamba is the ModelManager class that can be accessed with: from zamba.models.manager import ModelManager The ModelManager class is used by zamba \u2019s command line interface to handle preprocessing the filenames, loading the videos, training the model, performing inference, and saving predictions. Therefore any functionality available to the command line interface is accessible via the ModelManager class. To instantiate the ModelManager based on a configuration file saved at test_config.yaml : >>> manager = ModelManager . from_yaml ( 'test_config.yaml' ) We can now run inference or model training without specifying any additional parameters, because they are already associated with our instance of the ModelManager class. To run inference or training: manager . predict () # inference manager . train () # training In our user tutorials, we refer to train_model and predict_model functions. The ModelManager class calls these same functions behind the scenes when .predict() or .train() is run.","title":"Python package"},{"location":"yaml-config/#default-configurations","text":"In the command line, the default configuration for each model is passed in using a specified YAML file that ships with zamba . You can see the default configuration YAML files on Github in the config.yaml file within each model's folder. For example, the default configuration for the time_distributed model is: train_config : scheduler_config : scheduler : MultiStepLR scheduler_params : gamma : 0.5 milestones : - 3 verbose : true model_name : time_distributed backbone_finetune_config : backbone_initial_ratio_lr : 0.01 multiplier : 1 pre_train_bn : true train_bn : false unfreeze_backbone_at_epoch : 3 verbose : true early_stopping_config : patience : 5 video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 4 total_frames : 16 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 16 predict_config : model_name : time_distributed public_checkpoint : time_distributed_9e710aa8c92d25190a64b3b04b9122bdcb456982.ckpt","title":"Default configurations"},{"location":"yaml-config/#templates","text":"To make modifying the existing defaults easier, we've set up the official models as templates in the templates folder . Just fill in your data directory and labels, make any desired tweaks to the model config, and then kick off some training . Happy modeling!","title":"Templates"},{"location":"api-reference/data-metadata/","text":"zamba.data.metadata \u00b6 Functions \u00b6 create_site_specific_splits ( site : pd . Series , proportions : Dict [ str , int ], random_state : Optional [ Union [ int , np . random . mtrand . RandomState ]] = 989 ) \u00b6 Splits sites into distinct groups whose sizes roughly matching the given proportions. Null sites are randomly assigned to groups using the provided proportions. Parameters: Name Type Description Default site pd . Series A series of sites, one element per observation, required proportions dict A dict whose keys are the resulting groups and whose values are the rough proportion of data in each group. required seed int Seed for random split of null sites. required Example Split data into groups where each site is in one and only one group with roughly 50-25-25 train-val-holdout proportions. create_site_specific_splits(site, proportions={\"train\": 2, \"val\": 1, \"holdout\": 1}) Returns: Type Description pd.Series: A series containing the resulting split, one element per observation. Source code in zamba/data/metadata.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def create_site_specific_splits ( site : pd . Series , proportions : Dict [ str , int ], random_state : Optional [ Union [ int , np . random . mtrand . RandomState ]] = 989 , ): \"\"\"Splits sites into distinct groups whose sizes roughly matching the given proportions. Null sites are randomly assigned to groups using the provided proportions. Args: site (pd.Series): A series of sites, one element per observation, proportions (dict): A dict whose keys are the resulting groups and whose values are the rough proportion of data in each group. seed (int): Seed for random split of null sites. Example: Split data into groups where each site is in one and only one group with roughly 50-25-25 train-val-holdout proportions. >>> create_site_specific_splits(site, proportions={\"train\": 2, \"val\": 1, \"holdout\": 1}) Returns: pd.Series: A series containing the resulting split, one element per observation. \"\"\" assignments = {} sites = site . value_counts ( dropna = True ) . sort_values ( ascending = False ) . index n_subgroups = sum ( proportions . values ()) for i , subset in enumerate ( roundrobin ( * ([ subset ] * proportions [ subset ] for subset in proportions )) ): for group in sites [ i :: n_subgroups ]: assignments [ group ] = subset # Divide null sites among the groups null_sites = site . isnull () if null_sites . sum () > 0 : logger . debug ( f \" { null_sites . sum () : , } null sites randomly assigned to groups.\" ) null_groups = [] for group , group_proportion in proportions . items (): null_group = f \" { group } - { uuid4 () } \" null_groups . append ( null_group ) assignments [ null_group ] = group rng = ( np . random . RandomState ( random_state ) if isinstance ( random_state , int ) else random_state ) site = site . copy () site . loc [ null_sites ] = rng . choice ( null_groups , p = np . asarray ( list ( proportions . values ())) / sum ( proportions . values ()), size = null_sites . sum (), replace = True , ) return site . replace ( assignments ) one_hot_to_labels ( one_hot : pd . DataFrame , column_prefix : Optional [ str ] = 'species_' ) -> pd . DataFrame \u00b6 Source code in zamba/data/metadata.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def one_hot_to_labels ( one_hot : pd . DataFrame , column_prefix : Optional [ str ] = r \"species_\" ) -> pd . DataFrame : if column_prefix : one_hot = one_hot . filter ( regex = column_prefix ) # remove prefix one_hot . columns = [ c . split ( column_prefix , 1 )[ 1 ] for c in one_hot . columns ] one_hot . index = one_hot . index . rename ( \"filepath\" ) one_hot . columns = one_hot . columns . rename ( \"label\" ) labels = ( one_hot == 1 ) . stack () labels = labels [ labels ] return labels . reset_index () . drop ( 0 , axis = 1 ) roundrobin ( * iterables ) \u00b6 roundrobin('ABC', 'D', 'EF') --> A D E B F C Source code in zamba/data/metadata.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def roundrobin ( * iterables ): \"roundrobin('ABC', 'D', 'EF') --> A D E B F C\" # From https://docs.python.org/3/library/itertools.html#recipes # Recipe credited to George Sakkis num_active = len ( iterables ) nexts = itertools . cycle ( iter ( it ) . __next__ for it in iterables ) while num_active : try : for next in nexts : yield next () except StopIteration : # Remove the iterator we just exhausted from the cycle. num_active -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , num_active ))","title":"zamba.data.metadata"},{"location":"api-reference/data-metadata/#zambadatametadata","text":"","title":"zamba.data.metadata"},{"location":"api-reference/data-metadata/#zamba.data.metadata-functions","text":"","title":"Functions"},{"location":"api-reference/data-video/","text":"zamba.data.video \u00b6 Classes \u00b6 VideoLoaderConfig \u00b6 Bases: BaseModel Configuration for load_video_frames. Parameters: Name Type Description Default crop_bottom_pixels int Number of pixels to crop from the bottom of the video (prior to resizing to video_height ). required i_frames bool Only load the I-Frames. See https://en.wikipedia.org/wiki/Video_compression_picture_types#Intra-coded_(I) frames/slices (key_frames) required scene_threshold float Only load frames that correspond to scene changes. See http://www.ffmpeg.org/ffmpeg-filters.html#select_002c-aselect required megadetector_lite_config MegadetectorLiteYoloXConfig Configuration of MegadetectorLiteYoloX frame selection model. required frame_selection_height int Resize the video to this height in pixels, prior to frame selection. If None, the full size video will be used for frame selection. Using full size images (setting to None) is recommended for MegadetectorLite, especially if your species of interest are smaller. required frame_selection_width int Resize the video to this width in pixels, prior to frame selection. required total_frames int Number of frames that should ultimately be returned. required ensure_total_frames bool Selecting the number of frames by resampling may result in one more or fewer frames due to rounding. If True, ensure the requested number of frames is returned by either clipping or duplicating the final frame. Raises an error if no frames have been selected. Otherwise, return the array unchanged. required fps float Resample the video evenly from the entire duration to a specific number of frames per second. required early_bias bool Resamples to 24 fps and selects 16 frames biased toward the front (strategy used by competition winner). required frame_indices list ( int ) Select specific frame numbers. Note: frame selection is done after any resampling. required evenly_sample_total_frames bool Reach the total number of frames specified by evenly sampling from the duration of the video. Defaults to False. required pix_fmt str ffmpeg pixel format, defaults to 'rgb24' for RGB channels; can be changed to 'bgr24' for BGR. required model_input_height int After frame selection, resize the video to this height in pixels. required model_input_width int After frame selection, resize the video to this width in pixels. required cache_dir Path Cache directory where preprocessed videos will be saved upon first load. Alternatively, can be set with VIDEO_CACHE_DIR environment variable. Defaults to None, which means videos will not be cached. Provided there is enough space on your machine, it is highly encouraged to cache videos for training as this will speed up all subsequent epochs. If you are predicting on the same videos with the same video loader configuration, this will save time on future runs. required cleanup_cache bool Whether to delete the cache dir after training or predicting ends. Defaults to False. required Source code in zamba/data/video.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 class VideoLoaderConfig ( BaseModel ): \"\"\" Configuration for load_video_frames. Args: crop_bottom_pixels (int, optional): Number of pixels to crop from the bottom of the video (prior to resizing to `video_height`). i_frames (bool, optional): Only load the I-Frames. See https://en.wikipedia.org/wiki/Video_compression_picture_types#Intra-coded_(I)_frames/slices_(key_frames) scene_threshold (float, optional): Only load frames that correspond to scene changes. See http://www.ffmpeg.org/ffmpeg-filters.html#select_002c-aselect megadetector_lite_config (MegadetectorLiteYoloXConfig, optional): Configuration of MegadetectorLiteYoloX frame selection model. frame_selection_height (int, optional): Resize the video to this height in pixels, prior to frame selection. If None, the full size video will be used for frame selection. Using full size images (setting to None) is recommended for MegadetectorLite, especially if your species of interest are smaller. frame_selection_width (int, optional): Resize the video to this width in pixels, prior to frame selection. total_frames (int, optional): Number of frames that should ultimately be returned. ensure_total_frames (bool): Selecting the number of frames by resampling may result in one more or fewer frames due to rounding. If True, ensure the requested number of frames is returned by either clipping or duplicating the final frame. Raises an error if no frames have been selected. Otherwise, return the array unchanged. fps (float, optional): Resample the video evenly from the entire duration to a specific number of frames per second. early_bias (bool, optional): Resamples to 24 fps and selects 16 frames biased toward the front (strategy used by competition winner). frame_indices (list(int), optional): Select specific frame numbers. Note: frame selection is done after any resampling. evenly_sample_total_frames (bool, optional): Reach the total number of frames specified by evenly sampling from the duration of the video. Defaults to False. pix_fmt (str, optional): ffmpeg pixel format, defaults to 'rgb24' for RGB channels; can be changed to 'bgr24' for BGR. model_input_height (int, optional): After frame selection, resize the video to this height in pixels. model_input_width (int, optional): After frame selection, resize the video to this width in pixels. cache_dir (Path, optional): Cache directory where preprocessed videos will be saved upon first load. Alternatively, can be set with VIDEO_CACHE_DIR environment variable. Defaults to None, which means videos will not be cached. Provided there is enough space on your machine, it is highly encouraged to cache videos for training as this will speed up all subsequent epochs. If you are predicting on the same videos with the same video loader configuration, this will save time on future runs. cleanup_cache (bool): Whether to delete the cache dir after training or predicting ends. Defaults to False. \"\"\" crop_bottom_pixels : Optional [ int ] = None i_frames : Optional [ bool ] = False scene_threshold : Optional [ float ] = None megadetector_lite_config : Optional [ MegadetectorLiteYoloXConfig ] = None frame_selection_height : Optional [ int ] = None frame_selection_width : Optional [ int ] = None total_frames : Optional [ int ] = None ensure_total_frames : Optional [ bool ] = True fps : Optional [ float ] = None early_bias : Optional [ bool ] = False frame_indices : Optional [ List [ int ]] = None evenly_sample_total_frames : Optional [ bool ] = False pix_fmt : Optional [ str ] = \"rgb24\" model_input_height : Optional [ int ] = None model_input_width : Optional [ int ] = None cache_dir : Optional [ Path ] = None cleanup_cache : bool = False class Config : extra = \"forbid\" @validator ( \"cache_dir\" , always = True ) def validate_video_cache_dir ( cls , cache_dir ): \"\"\"Set up cache directory for preprocessed videos. Config argument takes precedence over environment variable. \"\"\" if cache_dir is None : cache_dir = os . getenv ( \"VIDEO_CACHE_DIR\" , None ) if cache_dir in [ \"\" , \"0\" ]: cache_dir = None if cache_dir is not None : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) return cache_dir @root_validator ( skip_on_failure = True ) def check_height_and_width ( cls , values ): if ( values [ \"frame_selection_height\" ] is None ) ^ ( values [ \"frame_selection_width\" ] is None ): raise ValueError ( f \"Must provide both frame_selection_height and frame_selection_width or neither. Values provided are { values } .\" ) if ( values [ \"model_input_height\" ] is None ) ^ ( values [ \"model_input_width\" ] is None ): raise ValueError ( f \"Must provide both model_input_height and model_input_width or neither. Values provided are { values } .\" ) return values @root_validator ( skip_on_failure = True ) def check_fps_compatibility ( cls , values ): if values [ \"fps\" ] and ( values [ \"evenly_sample_total_frames\" ] or values [ \"i_frames\" ] or values [ \"scene_threshold\" ] ): raise ValueError ( f \"fps cannot be used with evenly_sample_total_frames, i_frames, or scene_threshold. Values provided are { values } .\" ) return values @root_validator ( skip_on_failure = True ) def check_i_frame_compatibility ( cls , values ): if values [ \"scene_threshold\" ] and values [ \"i_frames\" ]: raise ValueError ( f \"i_frames cannot be used with scene_threshold. Values provided are { values } .\" ) return values @root_validator ( skip_on_failure = True ) def check_early_bias_compatibility ( cls , values ): if values [ \"early_bias\" ] and ( values [ \"i_frames\" ] or values [ \"scene_threshold\" ] or values [ \"total_frames\" ] or values [ \"evenly_sample_total_frames\" ] or values [ \"fps\" ] ): raise ValueError ( f \"early_bias cannot be used with i_frames, scene_threshold, total_frames, evenly_sample_total_frames, or fps. Values provided are { values } .\" ) return values @root_validator ( skip_on_failure = True ) def check_frame_indices_compatibility ( cls , values ): if values [ \"frame_indices\" ] and ( values [ \"total_frames\" ] or values [ \"scene_threshold\" ] or values [ \"i_frames\" ] or values [ \"early_bias\" ] or values [ \"evenly_sample_total_frames\" ] ): raise ValueError ( f \"frame_indices cannot be used with total_frames, scene_threshold, i_frames, early_bias, or evenly_sample_total_frames. Values provided are { values } .\" ) return values @root_validator ( skip_on_failure = True ) def check_megadetector_lite_compatibility ( cls , values ): if values [ \"megadetector_lite_config\" ] and ( values [ \"early_bias\" ] or values [ \"evenly_sample_total_frames\" ] ): raise ValueError ( f \"megadetector_lite_config cannot be used with early_bias or evenly_sample_total_frames. Values provided are { values } .\" ) return values @root_validator ( skip_on_failure = True ) def check_evenly_sample_total_frames_compatibility ( cls , values ): if values [ \"evenly_sample_total_frames\" ] is True and values [ \"total_frames\" ] is None : raise ValueError ( f \"total_frames must be specified if evenly_sample_total_frames is used. Values provided are { values } .\" ) if values [ \"evenly_sample_total_frames\" ] and ( values [ \"scene_threshold\" ] or values [ \"i_frames\" ] or values [ \"fps\" ] or values [ \"early_bias\" ] ): raise ValueError ( f \"evenly_sample_total_frames cannot be used with scene_threshold, i_frames, fps, or early_bias. Values provided are { values } .\" ) return values @root_validator ( skip_on_failure = True ) def validate_total_frames ( cls , values ): if values [ \"megadetector_lite_config\" ] is not None : # set n frames for megadetector_lite_config if only specified by total_frames if values [ \"megadetector_lite_config\" ] . n_frames is None : values [ \"megadetector_lite_config\" ] . n_frames = values [ \"total_frames\" ] # set total frames if only specified in megadetector_lite_config if values [ \"total_frames\" ] is None : values [ \"total_frames\" ] = values [ \"megadetector_lite_config\" ] . n_frames return values Attributes \u00b6 cache_dir : Optional [ Path ] = None class-attribute \u00b6 cleanup_cache : bool = False class-attribute \u00b6 crop_bottom_pixels : Optional [ int ] = None class-attribute \u00b6 early_bias : Optional [ bool ] = False class-attribute \u00b6 ensure_total_frames : Optional [ bool ] = True class-attribute \u00b6 evenly_sample_total_frames : Optional [ bool ] = False class-attribute \u00b6 fps : Optional [ float ] = None class-attribute \u00b6 frame_indices : Optional [ List [ int ]] = None class-attribute \u00b6 frame_selection_height : Optional [ int ] = None class-attribute \u00b6 frame_selection_width : Optional [ int ] = None class-attribute \u00b6 i_frames : Optional [ bool ] = False class-attribute \u00b6 megadetector_lite_config : Optional [ MegadetectorLiteYoloXConfig ] = None class-attribute \u00b6 model_input_height : Optional [ int ] = None class-attribute \u00b6 model_input_width : Optional [ int ] = None class-attribute \u00b6 pix_fmt : Optional [ str ] = 'rgb24' class-attribute \u00b6 scene_threshold : Optional [ float ] = None class-attribute \u00b6 total_frames : Optional [ int ] = None class-attribute \u00b6 Classes \u00b6 Config \u00b6 Source code in zamba/data/video.py 212 213 class Config : extra = \"forbid\" Attributes \u00b6 extra = 'forbid' class-attribute \u00b6 Functions \u00b6 check_early_bias_compatibility ( values ) \u00b6 Source code in zamba/data/video.py 262 263 264 265 266 267 268 269 270 271 272 273 274 @root_validator ( skip_on_failure = True ) def check_early_bias_compatibility ( cls , values ): if values [ \"early_bias\" ] and ( values [ \"i_frames\" ] or values [ \"scene_threshold\" ] or values [ \"total_frames\" ] or values [ \"evenly_sample_total_frames\" ] or values [ \"fps\" ] ): raise ValueError ( f \"early_bias cannot be used with i_frames, scene_threshold, total_frames, evenly_sample_total_frames, or fps. Values provided are { values } .\" ) return values check_evenly_sample_total_frames_compatibility ( values ) \u00b6 Source code in zamba/data/video.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 @root_validator ( skip_on_failure = True ) def check_evenly_sample_total_frames_compatibility ( cls , values ): if values [ \"evenly_sample_total_frames\" ] is True and values [ \"total_frames\" ] is None : raise ValueError ( f \"total_frames must be specified if evenly_sample_total_frames is used. Values provided are { values } .\" ) if values [ \"evenly_sample_total_frames\" ] and ( values [ \"scene_threshold\" ] or values [ \"i_frames\" ] or values [ \"fps\" ] or values [ \"early_bias\" ] ): raise ValueError ( f \"evenly_sample_total_frames cannot be used with scene_threshold, i_frames, fps, or early_bias. Values provided are { values } .\" ) return values check_fps_compatibility ( values ) \u00b6 Source code in zamba/data/video.py 244 245 246 247 248 249 250 251 252 @root_validator ( skip_on_failure = True ) def check_fps_compatibility ( cls , values ): if values [ \"fps\" ] and ( values [ \"evenly_sample_total_frames\" ] or values [ \"i_frames\" ] or values [ \"scene_threshold\" ] ): raise ValueError ( f \"fps cannot be used with evenly_sample_total_frames, i_frames, or scene_threshold. Values provided are { values } .\" ) return values check_frame_indices_compatibility ( values ) \u00b6 Source code in zamba/data/video.py 276 277 278 279 280 281 282 283 284 285 286 287 288 @root_validator ( skip_on_failure = True ) def check_frame_indices_compatibility ( cls , values ): if values [ \"frame_indices\" ] and ( values [ \"total_frames\" ] or values [ \"scene_threshold\" ] or values [ \"i_frames\" ] or values [ \"early_bias\" ] or values [ \"evenly_sample_total_frames\" ] ): raise ValueError ( f \"frame_indices cannot be used with total_frames, scene_threshold, i_frames, early_bias, or evenly_sample_total_frames. Values provided are { values } .\" ) return values check_height_and_width ( values ) \u00b6 Source code in zamba/data/video.py 232 233 234 235 236 237 238 239 240 241 242 @root_validator ( skip_on_failure = True ) def check_height_and_width ( cls , values ): if ( values [ \"frame_selection_height\" ] is None ) ^ ( values [ \"frame_selection_width\" ] is None ): raise ValueError ( f \"Must provide both frame_selection_height and frame_selection_width or neither. Values provided are { values } .\" ) if ( values [ \"model_input_height\" ] is None ) ^ ( values [ \"model_input_width\" ] is None ): raise ValueError ( f \"Must provide both model_input_height and model_input_width or neither. Values provided are { values } .\" ) return values check_i_frame_compatibility ( values ) \u00b6 Source code in zamba/data/video.py 254 255 256 257 258 259 260 @root_validator ( skip_on_failure = True ) def check_i_frame_compatibility ( cls , values ): if values [ \"scene_threshold\" ] and values [ \"i_frames\" ]: raise ValueError ( f \"i_frames cannot be used with scene_threshold. Values provided are { values } .\" ) return values check_megadetector_lite_compatibility ( values ) \u00b6 Source code in zamba/data/video.py 290 291 292 293 294 295 296 297 298 @root_validator ( skip_on_failure = True ) def check_megadetector_lite_compatibility ( cls , values ): if values [ \"megadetector_lite_config\" ] and ( values [ \"early_bias\" ] or values [ \"evenly_sample_total_frames\" ] ): raise ValueError ( f \"megadetector_lite_config cannot be used with early_bias or evenly_sample_total_frames. Values provided are { values } .\" ) return values validate_total_frames ( values ) \u00b6 Source code in zamba/data/video.py 317 318 319 320 321 322 323 324 325 326 327 328 @root_validator ( skip_on_failure = True ) def validate_total_frames ( cls , values ): if values [ \"megadetector_lite_config\" ] is not None : # set n frames for megadetector_lite_config if only specified by total_frames if values [ \"megadetector_lite_config\" ] . n_frames is None : values [ \"megadetector_lite_config\" ] . n_frames = values [ \"total_frames\" ] # set total frames if only specified in megadetector_lite_config if values [ \"total_frames\" ] is None : values [ \"total_frames\" ] = values [ \"megadetector_lite_config\" ] . n_frames return values validate_video_cache_dir ( cache_dir ) \u00b6 Set up cache directory for preprocessed videos. Config argument takes precedence over environment variable. Source code in zamba/data/video.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 @validator ( \"cache_dir\" , always = True ) def validate_video_cache_dir ( cls , cache_dir ): \"\"\"Set up cache directory for preprocessed videos. Config argument takes precedence over environment variable. \"\"\" if cache_dir is None : cache_dir = os . getenv ( \"VIDEO_CACHE_DIR\" , None ) if cache_dir in [ \"\" , \"0\" ]: cache_dir = None if cache_dir is not None : cache_dir = Path ( cache_dir ) cache_dir . mkdir ( parents = True , exist_ok = True ) return cache_dir VideoMetadata \u00b6 Bases: BaseModel Source code in zamba/data/video.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class VideoMetadata ( BaseModel ): height : int width : int n_frames : int duration_s : float fps : int @classmethod def from_video ( cls , path : os . PathLike ): stream = get_video_stream ( path ) return cls ( height = int ( stream [ \"height\" ]), width = int ( stream [ \"width\" ]), n_frames = int ( stream [ \"nb_frames\" ]), duration_s = float ( stream [ \"duration\" ]), fps = int ( Fraction ( stream [ \"r_frame_rate\" ])), # reported, not average ) Attributes \u00b6 duration_s : float class-attribute \u00b6 fps : int class-attribute \u00b6 height : int class-attribute \u00b6 n_frames : int class-attribute \u00b6 width : int class-attribute \u00b6 Functions \u00b6 from_video ( path : os . PathLike ) classmethod \u00b6 Source code in zamba/data/video.py 134 135 136 137 138 139 140 141 142 143 @classmethod def from_video ( cls , path : os . PathLike ): stream = get_video_stream ( path ) return cls ( height = int ( stream [ \"height\" ]), width = int ( stream [ \"width\" ]), n_frames = int ( stream [ \"nb_frames\" ]), duration_s = float ( stream [ \"duration\" ]), fps = int ( Fraction ( stream [ \"r_frame_rate\" ])), # reported, not average ) npy_cache \u00b6 Source code in zamba/data/video.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 class npy_cache : def __init__ ( self , cache_path : Optional [ Path ] = None , cleanup : bool = False ): self . cache_path = cache_path self . cleanup = cleanup def __call__ ( self , f ): def _wrapped ( * args , ** kwargs ): try : vid_path = kwargs [ \"filepath\" ] except Exception : vid_path = args [ 0 ] try : config = kwargs [ \"config\" ] except Exception : config = VideoLoaderConfig ( ** kwargs ) # NOTE: what should we do if this assert fails? assert config . cache_dir == self . cache_path # get the path for the cached data npy_path = get_cached_array_path ( vid_path , config ) # make parent directories since we're using absolute paths npy_path . parent . mkdir ( parents = True , exist_ok = True ) if npy_path . exists (): logger . debug ( f \"Loading from cache { npy_path } : size { npy_path . stat () . st_size } \" ) return np . load ( npy_path ) else : logger . debug ( f \"Loading video from disk: { vid_path } \" ) loaded_video = f ( * args , ** kwargs ) np . save ( npy_path , loaded_video ) logger . debug ( f \"Wrote to cache { npy_path } : size { npy_path . stat () . st_size } \" ) return loaded_video if self . cache_path is not None : return _wrapped else : return f def __del__ ( self ): if hasattr ( self , \"cache_path\" ) and self . cleanup and self . cache_path . exists (): if self . cache_path . parents [ 0 ] == tempfile . gettempdir (): logger . info ( f \"Deleting cache dir { self . cache_path } .\" ) rmtree ( self . cache_path ) else : logger . warning ( \"Bravely refusing to delete directory that is not a subdirectory of the \" \"system temp directory. If you really want to delete, do so manually using: \\n \" f \"rm -r { self . cache_path } \" ) Attributes \u00b6 cache_path = cache_path instance-attribute \u00b6 cleanup = cleanup instance-attribute \u00b6 Functions \u00b6 __init__ ( cache_path : Optional [ Path ] = None , cleanup : bool = False ) \u00b6 Source code in zamba/data/video.py 365 366 367 def __init__ ( self , cache_path : Optional [ Path ] = None , cleanup : bool = False ): self . cache_path = cache_path self . cleanup = cleanup Functions \u00b6 ensure_frame_number ( arr , total_frames : int ) \u00b6 Ensures the array contains the requested number of frames either by clipping frames from the end or dulpicating the last frame. Parameters: Name Type Description Default arr np . ndarray Array of video frames with shape (frames, height, width, channel). required total_frames int Desired number of frames in output array. required Source code in zamba/data/video.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def ensure_frame_number ( arr , total_frames : int ): \"\"\"Ensures the array contains the requested number of frames either by clipping frames from the end or dulpicating the last frame. Args: arr (np.ndarray): Array of video frames with shape (frames, height, width, channel). total_frames (int): Desired number of frames in output array. \"\"\" if ( total_frames is None ) or ( arr . shape [ 0 ] == total_frames ): return arr elif arr . shape [ 0 ] == 0 : logger . warning ( \"No frames selected. Returning an array in the desired shape with all zeros.\" ) return np . zeros (( total_frames , arr . shape [ 1 ], arr . shape [ 2 ], arr . shape [ 3 ]), dtype = \"int\" ) elif arr . shape [ 0 ] > total_frames : logger . info ( f \"Clipping { arr . shape [ 0 ] - total_frames } frames \" f \"(original: { arr . shape [ 0 ] } , requested: { total_frames } ).\" ) return arr [: total_frames ] elif arr . shape [ 0 ] < total_frames : logger . info ( f \"Duplicating last frame { total_frames - arr . shape [ 0 ] } times \" f \"(original: { arr . shape [ 0 ] } , requested: { total_frames } ).\" ) return np . concatenate ( [ arr , np . tile ( arr [ - 1 ], ( total_frames - arr . shape [ 0 ], 1 , 1 , 1 ))], axis = 0 ) ffprobe ( path : os . PathLike ) -> pd . Series \u00b6 Source code in zamba/data/video.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def ffprobe ( path : os . PathLike ) -> pd . Series : def flatten_json ( j , name = \"\" ): for k in j : if isinstance ( j [ k ], dict ): yield from flatten_json ( j [ k ], f \" { name } . { k } \" ) elif isinstance ( j [ k ], list ): for i in range ( len ( j [ k ])): yield from flatten_json ( j [ k ][ i ], f \" { name } . { k } [ { i } ]\" ) else : yield { f \" { name } . { k } \" . strip ( \".\" ): j [ k ]} output = subprocess . check_output ( [ \"ffprobe\" , \"-v\" , \"quiet\" , \"-show_entries\" , \"stream:format\" , \"-select_streams\" , \"v\" , \"-of\" , \"json\" , path , ] ) output = json . loads ( output ) result = reduce ( lambda a , b : { ** a , ** b }, flatten_json ( output )) return pd . Series ( result ) get_cached_array_path ( vid_path , config ) \u00b6 Get the path to where the cached array would be, if it exists. vid_path: string path to the video, or Path config: VideoLoaderConfig Source code in zamba/data/video.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 def get_cached_array_path ( vid_path , config ): \"\"\"Get the path to where the cached array would be, if it exists. vid_path: string path to the video, or Path config: VideoLoaderConfig returns: Path object to the cached data \"\"\" assert isinstance ( config , VideoLoaderConfig ) # don't include `cleanup_cache` or `cache_dir` in the hashed config # NOTE: sorting the keys avoids a cache miss if we see the same config in a different order; # might not be necessary with a VideoLoaderConfig config_dict = config . dict () keys = config_dict . keys () - { \"cleanup_cache\" , \"cache_dir\" } hashed_part = { k : config_dict [ k ] for k in sorted ( keys )} # hash config for inclusion in path hash_str = hashlib . sha1 ( str ( hashed_part ) . encode ( \"utf-8\" )) . hexdigest () logger . opt ( lazy = True ) . debug ( f \"Generated hash { hash_str } from { hashed_part } \" ) # strip leading \"/\" in absolute path vid_path = AnyPath ( str ( vid_path ) . lstrip ( \"/\" )) # if the video is in S3, drop the prefix and bucket name if isinstance ( vid_path , S3Path ): vid_path = AnyPath ( vid_path . key ) cache_dir = config . cache_dir npy_path = AnyPath ( cache_dir ) / hash_str / vid_path . with_suffix ( \".npy\" ) return npy_path get_frame_time_estimates ( path : os . PathLike ) \u00b6 Source code in zamba/data/video.py 122 123 124 def get_frame_time_estimates ( path : os . PathLike ): probe = ffmpeg . probe ( str ( path ), show_entries = \"frame=best_effort_timestamp_time\" ) return [ float ( x [ \"best_effort_timestamp_time\" ]) for x in probe [ \"frames\" ]] get_video_stream ( path : Union [ os . PathLike , S3Path ]) -> dict \u00b6 Source code in zamba/data/video.py 58 59 60 61 62 63 64 def get_video_stream ( path : Union [ os . PathLike , S3Path ]) -> dict : try : probe = ffmpeg . probe ( str ( path )) except ffmpeg . Error as exc : raise ZambaFfmpegException ( exc . stderr ) return next (( stream for stream in probe [ \"streams\" ] if stream [ \"codec_type\" ] == \"video\" ), None ) load_video_frames ( filepath : os . PathLike , config : Optional [ VideoLoaderConfig ] = None , ** kwargs ) \u00b6 Loads frames from videos using fast ffmpeg commands. Parameters: Name Type Description Default filepath os . PathLike Path to the video. required config VideoLoaderConfig Configuration for video loading. None **kwargs Optionally, arguments for VideoLoaderConfig can be passed in directly. {} Returns: Type Description np.ndarray: An array of video frames with dimensions (time x height x width x channels). Source code in zamba/data/video.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def load_video_frames ( filepath : os . PathLike , config : Optional [ VideoLoaderConfig ] = None , ** kwargs , ): \"\"\"Loads frames from videos using fast ffmpeg commands. Args: filepath (os.PathLike): Path to the video. config (VideoLoaderConfig, optional): Configuration for video loading. **kwargs: Optionally, arguments for VideoLoaderConfig can be passed in directly. Returns: np.ndarray: An array of video frames with dimensions (time x height x width x channels). \"\"\" if not Path ( filepath ) . exists (): raise FileNotFoundError ( f \"No file found at { filepath } \" ) if config is None : config = VideoLoaderConfig ( ** kwargs ) video_stream = get_video_stream ( filepath ) w = int ( video_stream [ \"width\" ]) h = int ( video_stream [ \"height\" ]) pipeline = ffmpeg . input ( str ( filepath )) pipeline_kwargs = {} if ( config . crop_bottom_pixels is not None ) and ( config . crop_bottom_pixels > 0 ): # scale to ensure all frames are the same height and we can crop pipeline = pipeline . filter ( \"scale\" , f \" { w } , { h } \" ) pipeline = pipeline . crop ( \"0\" , \"0\" , \"iw\" , f \"ih- { config . crop_bottom_pixels } \" ) h = h - config . crop_bottom_pixels if config . evenly_sample_total_frames : config . fps = config . total_frames / float ( video_stream [ \"duration\" ]) if config . early_bias : config . fps = 24 # competition frame selection assumes 24 frames per second config . total_frames = 16 # used for ensure_total_frames if config . fps : pipeline = pipeline . filter ( \"fps\" , fps = config . fps , round = \"up\" ) if config . i_frames : pipeline = pipeline . filter ( \"select\" , \"eq(pict_type,PICT_TYPE_I)\" ) if config . scene_threshold : pipeline = pipeline . filter ( \"select\" , f \"gt(scene, { config . scene_threshold } )\" ) if config . frame_selection_height and config . frame_selection_width : pipeline = pipeline . filter ( \"scale\" , f \" { config . frame_selection_width } , { config . frame_selection_height } \" ) w , h = config . frame_selection_width , config . frame_selection_height if config . early_bias : config . frame_indices = [ 2 , 8 , 12 , 18 , 24 , 36 , 48 , 60 , 72 , 84 , 96 , 108 , 120 , 132 , 144 , 156 ] if config . frame_indices : pipeline = pipeline . filter ( \"select\" , \"+\" . join ( f \"eq(n, { f } )\" for f in config . frame_indices )) pipeline_kwargs = { \"vsync\" : 0 } pipeline = pipeline . output ( \"pipe:\" , format = \"rawvideo\" , pix_fmt = config . pix_fmt , ** pipeline_kwargs ) try : out , err = pipeline . run ( capture_stdout = True , capture_stderr = True ) except ffmpeg . Error as exc : raise ZambaFfmpegException ( exc . stderr ) arr = np . frombuffer ( out , np . uint8 ) . reshape ([ - 1 , h , w , 3 ]) if config . megadetector_lite_config is not None : mdlite = MegadetectorLiteYoloX ( config = config . megadetector_lite_config ) detection_probs = mdlite . detect_video ( video_arr = arr ) arr = mdlite . filter_frames ( arr , detection_probs ) if ( config . model_input_height is not None ) and ( config . model_input_width is not None ): resized_frames = np . zeros ( ( arr . shape [ 0 ], config . model_input_height , config . model_input_width , 3 ), np . uint8 ) for ix , f in enumerate ( arr ): if ( f . shape [ 0 ] != config . model_input_height ) or ( f . shape [ 1 ] != config . model_input_width ): f = cv2 . resize ( f , ( config . model_input_width , config . model_input_height ), # https://stackoverflow.com/a/51042104/1692709 interpolation = ( cv2 . INTER_LINEAR if f . shape [ 1 ] < config . model_input_width else cv2 . INTER_AREA ), ) resized_frames [ ix , ... ] = f arr = np . array ( resized_frames ) if config . ensure_total_frames : arr = ensure_frame_number ( arr , total_frames = config . total_frames ) return arr num_frames ( stream_or_path : Union [ dict , os . PathLike , S3Path ]) -> Optional [ int ] \u00b6 Source code in zamba/data/video.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def num_frames ( stream_or_path : Union [ dict , os . PathLike , S3Path ]) -> Optional [ int ]: if not isinstance ( stream_or_path , dict ): stream = get_video_stream ( stream_or_path ) else : stream = stream_or_path if not stream : return if \"nb_frames\" in stream : return int ( stream [ \"nb_frames\" ]) if \"duration\" in stream : duration = float ( stream [ \"duration\" ]) if \"r_frame_rate\" in stream : frame_rate = float ( Fraction ( stream [ \"r_frame_rate\" ])) elif \"avg_frame_rate\" in stream : frame_rate = float ( stream [ \"avg_frame_rate\" ]) duration -= float ( stream . get ( \"start_time\" , 0 )) return floor ( duration * frame_rate )","title":"zamba.data.video"},{"location":"api-reference/data-video/#zambadatavideo","text":"","title":"zamba.data.video"},{"location":"api-reference/data-video/#zamba.data.video-classes","text":"","title":"Classes"},{"location":"api-reference/data-video/#zamba.data.video-functions","text":"","title":"Functions"},{"location":"api-reference/densepose_config/","text":"zamba.models.densepose.config \u00b6 Attributes \u00b6 Classes \u00b6 DensePoseConfig \u00b6 Bases: ZambaBaseModel Configuration for running dense pose on videos. Parameters: Name Type Description Default video_loader_config VideoLoaderConfig Configuration for loading videos required output_type str one of DensePoseOutputEnum (currently \"segmentation\" or \"chimp_anatomy\"). required render_output bool Whether to save a version of the video with the output overlaid on top. Defaults to False. required embeddings_in_json bool Whether to save the embeddings matrices in the json of the DensePose result. Setting to True can result in large json files. Defaults to False. required data_dir Path Where to find the files listed in filepaths (or where to look if filepaths is not provided). required filepaths Path Path to a CSV file with a list of filepaths to process. required save_dir Path Directory for where to save the output files; defaults to os.getcwd(). required cache_dir Path Path for downloading and saving model weights. Defaults to env var MODEL_CACHE_DIR or the OS app cache dir. required weight_download_region RegionEnum region where to download weights; should be one of RegionEnum (currently 'us', 'asia', and 'eu'). Defaults to 'us'. required Source code in zamba/models/densepose/config.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class DensePoseConfig ( ZambaBaseModel ): \"\"\"Configuration for running dense pose on videos. Args: video_loader_config (VideoLoaderConfig): Configuration for loading videos output_type (str): one of DensePoseOutputEnum (currently \"segmentation\" or \"chimp_anatomy\"). render_output (bool): Whether to save a version of the video with the output overlaid on top. Defaults to False. embeddings_in_json (bool): Whether to save the embeddings matrices in the json of the DensePose result. Setting to True can result in large json files. Defaults to False. data_dir (Path): Where to find the files listed in filepaths (or where to look if filepaths is not provided). filepaths (Path, optional): Path to a CSV file with a list of filepaths to process. save_dir (Path, optional): Directory for where to save the output files; defaults to os.getcwd(). cache_dir (Path, optional): Path for downloading and saving model weights. Defaults to env var `MODEL_CACHE_DIR` or the OS app cache dir. weight_download_region (RegionEnum, optional): region where to download weights; should be one of RegionEnum (currently 'us', 'asia', and 'eu'). Defaults to 'us'. \"\"\" video_loader_config : VideoLoaderConfig output_type : DensePoseOutputEnum render_output : bool = False embeddings_in_json : bool = False data_dir : Path filepaths : Optional [ Path ] = None save_dir : Optional [ Path ] = None cache_dir : Optional [ Path ] = None weight_download_region : RegionEnum = RegionEnum ( \"us\" ) _validate_cache_dir = validator ( \"cache_dir\" , allow_reuse = True , always = True )( validate_model_cache_dir ) def run_model ( self ): \"\"\"Use this configuration to execute DensePose via the DensePoseManager\"\"\" if not isinstance ( self . output_type , DensePoseOutputEnum ): self . output_type = DensePoseOutputEnum ( self . output_type ) if self . output_type == DensePoseOutputEnum . segmentation . value : model = MODELS [ \"animals\" ] elif self . output_type == DensePoseOutputEnum . chimp_anatomy . value : model = MODELS [ \"chimps\" ] else : raise Exception ( f \"invalid { self . output_type } \" ) output_dir = Path ( os . getcwd ()) if self . save_dir is None else self . save_dir dpm = DensePoseManager ( model , model_cache_dir = self . cache_dir , download_region = self . weight_download_region ) for fp in tqdm ( self . filepaths . filepath , desc = \"Videos\" ): fp = Path ( fp ) vid_arr , labels = dpm . predict_video ( fp , video_loader_config = self . video_loader_config ) # serialize the labels generated by densepose to json output_path = output_dir / f \" { fp . stem } _denspose_labels.json\" dpm . serialize_video_output ( labels , filename = output_path , write_embeddings = self . embeddings_in_json ) # re-render the video with the densepose labels visualized on top of the video if self . render_output : output_path = output_dir / f \" { fp . stem } _denspose_video { '' . join ( fp . suffixes ) } \" visualized_video = dpm . visualize_video ( vid_arr , labels , output_path = output_path , fps = self . video_loader_config . fps ) # write out the anatomy present in each frame to a csv for later analysis if self . output_type == DensePoseOutputEnum . chimp_anatomy . value : output_path = output_dir / f \" { fp . stem } _denspose_anatomy.csv\" dpm . anatomize_video ( visualized_video , labels , output_path = output_path , fps = self . video_loader_config . fps , ) @root_validator ( pre = False , skip_on_failure = True ) def get_filepaths ( cls , values ): \"\"\"If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column `filepath` contains files with valid suffixes. \"\"\" if values [ \"filepaths\" ] is None : logger . info ( f \"Getting files in { values [ 'data_dir' ] } .\" ) files = [] new_suffixes = [] # iterate over all files in data directory for f in values [ \"data_dir\" ] . rglob ( \"*\" ): if f . is_file (): # keep just files with supported suffixes if f . suffix . lower () in VIDEO_SUFFIXES : files . append ( f . resolve ()) else : new_suffixes . append ( f . suffix . lower ()) if len ( new_suffixes ) > 0 : logger . warning ( f \"Ignoring { len ( new_suffixes ) } file(s) with suffixes { set ( new_suffixes ) } . To include, specify all video suffixes with a VIDEO_SUFFIXES environment variable.\" ) if len ( files ) == 0 : raise ValueError ( f \"No video files found in { values [ 'data_dir' ] } .\" ) logger . info ( f \"Found { len ( files ) } videos in { values [ 'data_dir' ] } .\" ) values [ \"filepaths\" ] = pd . DataFrame ( files , columns = [ \"filepath\" ]) return values @root_validator ( skip_on_failure = True ) def validate_files ( cls , values ): # if globbing from data directory, already have valid dataframe if isinstance ( values [ \"filepaths\" ], pd . DataFrame ): files_df = values [ \"filepaths\" ] else : # make into dataframe even if only one column for clearer indexing files_df = pd . DataFrame ( pd . read_csv ( values [ \"filepaths\" ])) if \"filepath\" not in files_df . columns : raise ValueError ( f \" { values [ 'filepaths' ] } must contain a `filepath` column.\" ) # can only contain one row per filepath num_duplicates = len ( files_df ) - files_df . filepath . nunique () if num_duplicates > 0 : logger . warning ( f \"Found { num_duplicates } duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\" ) files_df = files_df [[ \"filepath\" ]] . drop_duplicates () values [ \"filepaths\" ] = check_files_exist_and_load ( df = files_df , data_dir = values [ \"data_dir\" ], skip_load_validation = True , ) return values Attributes \u00b6 cache_dir : Optional [ Path ] = None class-attribute \u00b6 data_dir : Path class-attribute \u00b6 embeddings_in_json : bool = False class-attribute \u00b6 filepaths : Optional [ Path ] = None class-attribute \u00b6 output_type : DensePoseOutputEnum class-attribute \u00b6 render_output : bool = False class-attribute \u00b6 save_dir : Optional [ Path ] = None class-attribute \u00b6 video_loader_config : VideoLoaderConfig class-attribute \u00b6 weight_download_region : RegionEnum = RegionEnum ( 'us' ) class-attribute \u00b6 Functions \u00b6 get_filepaths ( values ) \u00b6 If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column filepath contains files with valid suffixes. Source code in zamba/models/densepose/config.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @root_validator ( pre = False , skip_on_failure = True ) def get_filepaths ( cls , values ): \"\"\"If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column `filepath` contains files with valid suffixes. \"\"\" if values [ \"filepaths\" ] is None : logger . info ( f \"Getting files in { values [ 'data_dir' ] } .\" ) files = [] new_suffixes = [] # iterate over all files in data directory for f in values [ \"data_dir\" ] . rglob ( \"*\" ): if f . is_file (): # keep just files with supported suffixes if f . suffix . lower () in VIDEO_SUFFIXES : files . append ( f . resolve ()) else : new_suffixes . append ( f . suffix . lower ()) if len ( new_suffixes ) > 0 : logger . warning ( f \"Ignoring { len ( new_suffixes ) } file(s) with suffixes { set ( new_suffixes ) } . To include, specify all video suffixes with a VIDEO_SUFFIXES environment variable.\" ) if len ( files ) == 0 : raise ValueError ( f \"No video files found in { values [ 'data_dir' ] } .\" ) logger . info ( f \"Found { len ( files ) } videos in { values [ 'data_dir' ] } .\" ) values [ \"filepaths\" ] = pd . DataFrame ( files , columns = [ \"filepath\" ]) return values run_model () \u00b6 Use this configuration to execute DensePose via the DensePoseManager Source code in zamba/models/densepose/config.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def run_model ( self ): \"\"\"Use this configuration to execute DensePose via the DensePoseManager\"\"\" if not isinstance ( self . output_type , DensePoseOutputEnum ): self . output_type = DensePoseOutputEnum ( self . output_type ) if self . output_type == DensePoseOutputEnum . segmentation . value : model = MODELS [ \"animals\" ] elif self . output_type == DensePoseOutputEnum . chimp_anatomy . value : model = MODELS [ \"chimps\" ] else : raise Exception ( f \"invalid { self . output_type } \" ) output_dir = Path ( os . getcwd ()) if self . save_dir is None else self . save_dir dpm = DensePoseManager ( model , model_cache_dir = self . cache_dir , download_region = self . weight_download_region ) for fp in tqdm ( self . filepaths . filepath , desc = \"Videos\" ): fp = Path ( fp ) vid_arr , labels = dpm . predict_video ( fp , video_loader_config = self . video_loader_config ) # serialize the labels generated by densepose to json output_path = output_dir / f \" { fp . stem } _denspose_labels.json\" dpm . serialize_video_output ( labels , filename = output_path , write_embeddings = self . embeddings_in_json ) # re-render the video with the densepose labels visualized on top of the video if self . render_output : output_path = output_dir / f \" { fp . stem } _denspose_video { '' . join ( fp . suffixes ) } \" visualized_video = dpm . visualize_video ( vid_arr , labels , output_path = output_path , fps = self . video_loader_config . fps ) # write out the anatomy present in each frame to a csv for later analysis if self . output_type == DensePoseOutputEnum . chimp_anatomy . value : output_path = output_dir / f \" { fp . stem } _denspose_anatomy.csv\" dpm . anatomize_video ( visualized_video , labels , output_path = output_path , fps = self . video_loader_config . fps , ) validate_files ( values ) \u00b6 Source code in zamba/models/densepose/config.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 @root_validator ( skip_on_failure = True ) def validate_files ( cls , values ): # if globbing from data directory, already have valid dataframe if isinstance ( values [ \"filepaths\" ], pd . DataFrame ): files_df = values [ \"filepaths\" ] else : # make into dataframe even if only one column for clearer indexing files_df = pd . DataFrame ( pd . read_csv ( values [ \"filepaths\" ])) if \"filepath\" not in files_df . columns : raise ValueError ( f \" { values [ 'filepaths' ] } must contain a `filepath` column.\" ) # can only contain one row per filepath num_duplicates = len ( files_df ) - files_df . filepath . nunique () if num_duplicates > 0 : logger . warning ( f \"Found { num_duplicates } duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\" ) files_df = files_df [[ \"filepath\" ]] . drop_duplicates () values [ \"filepaths\" ] = check_files_exist_and_load ( df = files_df , data_dir = values [ \"data_dir\" ], skip_load_validation = True , ) return values DensePoseOutputEnum \u00b6 Bases: Enum Source code in zamba/models/densepose/config.py 22 23 24 class DensePoseOutputEnum ( Enum ): segmentation = \"segmentation\" chimp_anatomy = \"chimp_anatomy\" Attributes \u00b6 chimp_anatomy = 'chimp_anatomy' class-attribute \u00b6 segmentation = 'segmentation' class-attribute \u00b6 Functions \u00b6","title":"zamba.models.densepose.config"},{"location":"api-reference/densepose_config/#zambamodelsdenseposeconfig","text":"","title":"zamba.models.densepose.config"},{"location":"api-reference/densepose_config/#zamba.models.densepose.config-attributes","text":"","title":"Attributes"},{"location":"api-reference/densepose_config/#zamba.models.densepose.config-classes","text":"","title":"Classes"},{"location":"api-reference/densepose_config/#zamba.models.densepose.config-functions","text":"","title":"Functions"},{"location":"api-reference/densepose_manager/","text":"zamba.models.densepose.densepose_manager \u00b6 Attributes \u00b6 DENSEPOSE_AVAILABLE = True module-attribute \u00b6 MODELS = dict ( animals = dict ( config = str ( Path ( __file__ ) . parent / 'assets' / 'densepose_rcnn_R_50_FPN_soft_animals_I0_finetune_16k.yaml' ), densepose_weights_url = 'https://dl.fbaipublicfiles.com/densepose/cse/densepose_rcnn_R_50_FPN_soft_animals_I0_finetune_16k/270727112/model_final_421d28.pkl' , weights = 'zamba_densepose_model_final_421d28.pkl' , viz_class = DensePoseOutputsVertexVisualizer , viz_class_kwargs = dict ()), chimps = dict ( config = str ( Path ( __file__ ) . parent / 'assets' / 'densepose_rcnn_R_50_FPN_soft_chimps_finetune_4k.yaml' ), densepose_weights_url = 'https://dl.fbaipublicfiles.com/densepose/cse/densepose_rcnn_R_50_FPN_soft_chimps_finetune_4k/253146869/model_final_52f649.pkl' , weights = 'zamba_densepose_model_final_52f649.pkl' , viz_class = DensePoseOutputsTextureVisualizer , viz_class_kwargs = dict ( texture_atlases_dict = { 'chimp_5029' : get_texture_atlas ( str ( Path ( __file__ ) . parent / 'assets' / 'chimp_texture_colors_flipped.tif' ))}), anatomy_color_mapping = str ( Path ( __file__ ) . parent / 'assets' / 'chimp_5029_parts.csv' ))) module-attribute \u00b6 Classes \u00b6 DensePoseManager \u00b6 Source code in zamba/models/densepose/densepose_manager.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 class DensePoseManager : def __init__ ( self , model = MODELS [ \"chimps\" ], model_cache_dir : Path = Path ( \".zamba_cache\" ), download_region = RegionEnum ( \"us\" ), ): \"\"\"Create a DensePoseManager object. Parameters ---------- model : dict, optional (default MODELS['chimps']) A dictionary with the densepose model defintion like those defined in MODELS. \"\"\" if not DENSEPOSE_AVAILABLE : raise ImportError ( \"Densepose not installed. See: https://zamba.drivendata.org/docs/stable/models/densepose/#installation\" ) # setup configuration for densepose self . cfg = get_cfg () add_densepose_config ( self . cfg ) self . cfg . merge_from_file ( model [ \"config\" ]) if not ( model_cache_dir / model [ \"weights\" ]) . exists (): model_cache_dir . mkdir ( parents = True , exist_ok = True ) self . cfg . MODEL . WEIGHTS = download_weights ( model [ \"weights\" ], model_cache_dir , download_region ) # automatically use CPU if no cuda available if not torch . cuda . is_available (): self . cfg . MODEL . DEVICE = \"cpu\" self . cfg . freeze () logging . getLogger ( \"fvcore\" ) . setLevel ( \"CRITICAL\" ) # silence noisy detectron2 logging # set up predictor with the configuration self . predictor = DefaultPredictor ( self . cfg ) # we have a specific texture atlas for chimps with relevant regions # labeled that we can use instead of the default segmentation self . visualizer = model [ \"viz_class\" ]( self . cfg , device = self . cfg . MODEL . DEVICE , ** model . get ( \"viz_class_kwargs\" , {}), ) # set up utilities for use with visualizer self . vis_extractor = create_extractor ( self . visualizer ) self . vis_embedder = build_densepose_embedder ( self . cfg ) self . vis_class_to_mesh_name = get_class_to_mesh_name_mapping ( self . cfg ) self . vis_mesh_vertex_embeddings = { mesh_name : self . vis_embedder ( mesh_name ) . to ( self . cfg . MODEL . DEVICE ) for mesh_name in self . vis_class_to_mesh_name . values () if self . vis_embedder . has_embeddings ( mesh_name ) } if \"anatomy_color_mapping\" in model : self . anatomy_color_mapping = pd . read_csv ( model [ \"anatomy_color_mapping\" ], index_col = 0 ) else : self . anatomy_color_mapping = None def predict_image ( self , image ): \"\"\"Run inference to get the densepose results for an image. Parameters ---------- image : numpy array (unit8) of an image in BGR format or path to an image Returns ------- tuple Returns the image array as passed or loaded and the the densepose Instances as results. \"\"\" if isinstance ( image , ( str , Path )): image = read_image ( image , format = \"BGR\" ) return image , self . predict ( image ) def predict_video ( self , video , video_loader_config = None , pbar = True ): \"\"\"Run inference to get the densepose results for a video. Parameters ---------- video : numpy array (uint8) of a a video in BGR layout with time dimension first or path to a video video_loader_config : VideoLoaderConfig, optional A video loader config for loading videos (uses all defaults except pix_fmt=\"bgr24\") pbar : bool, optional Whether to display a progress bar, by default True Returns ------- tuple Tuple of (video_array, list of densepose results per frame) \"\"\" if isinstance ( video , ( str , Path )): video = load_video_frames ( video , config = video_loader_config ) pbar = tqdm if pbar else lambda x , ** kwargs : x return video , [ self . predict_image ( img )[ 1 ] for img in pbar ( video , desc = \"Frames\" ) ] # just the predictions def predict ( self , image_arr ): \"\"\"Main call to DensePose for inference. Runs inference on an image array. Parameters ---------- image_arr : numpy array BGR image array Returns ------- Instances Detection instances with boxes, scores, and densepose estimates. \"\"\" with torch . no_grad (): instances = self . predictor ( image_arr )[ \"instances\" ] return instances def serialize_video_output ( self , instances , filename = None , write_embeddings = False ): serialized = { \"frames\" : [ self . serialize_image_output ( frame_instances , filename = None , write_embeddings = write_embeddings ) for frame_instances in instances ] } if filename is not None : with Path ( filename ) . open ( \"w\" ) as f : json . dump ( serialized , f , indent = 2 ) return serialized def serialize_image_output ( self , instances , filename = None , write_embeddings = False ): \"\"\"Convert the densepose output into Python-native objects that can be written and read with json. Parameters ---------- instances : Instance The output from the densepose model filename : (str, Path), optional If not None, the filename to write the output to, by default None \"\"\" if isinstance ( instances , list ): img_height , img_width = instances [ 0 ] . image_size else : img_height , img_width = instances . image_size boxes = instances . get ( \"pred_boxes\" ) . tensor scores = instances . get ( \"scores\" ) . tolist () labels = instances . get ( \"pred_classes\" ) . tolist () try : pose_result = instances . get ( \"pred_densepose\" ) except KeyError : pose_result = None # include embeddings + segmentation if they exist and they are requested write_embeddings = write_embeddings and ( pose_result is not None ) serialized = { \"instances\" : [ { \"img_height\" : img_height , \"img_width\" : img_width , \"box\" : boxes [ i ] . cpu () . tolist (), \"score\" : scores [ i ], \"label\" : { \"value\" : labels [ i ], \"mesh_name\" : self . vis_class_to_mesh_name [ labels [ i ]], }, \"embedding\" : pose_result . embedding [[ i ], ... ] . cpu () . tolist () if write_embeddings else None , \"segmentation\" : pose_result . coarse_segm [[ i ], ... ] . cpu () . tolist () if write_embeddings else None , } for i in range ( len ( instances )) ] } if filename is not None : with Path ( filename ) . open ( \"w\" ) as f : json . dump ( serialized , f , indent = 2 ) return serialized def deserialize_output ( self , instances_dict = None , filename = None ): if filename is not None : with Path ( filename ) . open ( \"r\" ) as f : instances_dict = json . load ( f ) # handle image case is_image = False if \"frames\" not in instances_dict : instances_dict = { \"frames\" : [ instances_dict ]} is_image = True frames = [] for frame in instances_dict [ \"frames\" ]: heights , widths , boxes , scores , labels , embeddings , segmentations = zip ( * [ ( i [ \"img_height\" ], i [ \"img_width\" ], i [ \"box\" ], i [ \"score\" ], i [ \"label\" ][ \"value\" ], i [ \"embedding\" ] if i [ \"embedding\" ] is not None else [ np . nan ], i [ \"segmentation\" ] if i [ \"segmentation\" ] is not None else [ np . nan ], ) for i in frame [ \"instances\" ] ] ) frames . append ( Instances ( ( heights [ 0 ], widths [ 0 ]), pred_boxes = boxes , scores = scores , pred_classes = labels , pred_densepose = DensePoseEmbeddingPredictorOutput ( embedding = torch . tensor ( embeddings ), coarse_segm = torch . tensor ( segmentations ), ), ) ) # if image or single frame, just return the instance if is_image : return frames [ 0 ] else : return frames def visualize_image ( self , image_arr , outputs , output_path = None ): \"\"\"Visualize the pose information. Parameters ---------- image_arr : numpy array (unit8) BGR The numpy array representing the image. outputs : The outputs from running DensePoseManager.predict* output_path : str or Path, optional If not None, write visualization to this path; by default None Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" bw_image = cv2 . cvtColor ( image_arr , cv2 . COLOR_BGR2GRAY ) bw_image = np . tile ( bw_image [:, :, np . newaxis ], [ 1 , 1 , 3 ]) data = self . vis_extractor ( outputs ) image_vis = self . visualizer . visualize ( bw_image , data ) if output_path is not None : cv2 . imwrite ( str ( output_path ), image_vis ) return image_vis def anatomize_image ( self , visualized_img_arr , outputs , output_path = None ): \"\"\"Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in an image. Parameters ---------- visualized_img_arr : numpy array (unit8) BGR The numpy array the image after the texture has been visualized (by calling DensePoseManager.visualize_image). outputs : The outputs from running DensePoseManager.predict* Returns ------- pandas.DataFrame DataFrame with percent of pixels of the bounding box that correspond to each anatomical part \"\"\" if self . anatomy_color_mapping is None : raise ValueError ( \"No anatomy_color_mapping provided to track anatomy; did you mean to use a different MODEL?\" ) # no detections, return empty df for joining later (e.g., in anatomize_video) if not outputs : return pd . DataFrame ([]) _ , _ , N , bboxes_xywh , pred_classes = self . visualizer . extract_and_check_outputs_and_boxes ( self . vis_extractor ( outputs ) ) all_detections = [] for n in range ( N ): x , y , w , h = bboxes_xywh [ n ] . int () . cpu () . numpy () detection_area = visualized_img_arr [ y : y + h , x : x + w ] detection_stats = { name : ( detection_area == np . array ([[[ color . B , color . G , color . R ]]])) . all ( axis =- 1 ) . sum () / ( h * w ) # calc percent of bounding box with this color for name , color in self . anatomy_color_mapping . iterrows () } detection_stats [ \"x\" ] = x detection_stats [ \"y\" ] = y detection_stats [ \"h\" ] = h detection_stats [ \"w\" ] = w all_detections . append ( detection_stats ) results = pd . DataFrame ( all_detections ) if output_path is not None : results . to_csv ( output_path , index = False ) return results def visualize_video ( self , video_arr , outputs , output_path = None , frame_size = None , fps = 30 , pbar = True ): \"\"\"Visualize the pose information on a video Parameters ---------- video_arr : numpy array (unit8) BGR, time first The numpy array representing the video. outputs : The outputs from running DensePoseManager.predict* output_path : str or Path, optional If not None, write visualization to this path (should be .mp4); by default None frame_size : (innt, float), optional If frame_size is float, scale up or down by that float value; if frame_size is an integer, set width to that size and scale height appropriately. fps : int frames per second for output video if writing; defaults to 30 pbar : bool display a progress bar Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" pbar = tqdm if pbar else lambda x , ** kwargs : x out_frames = np . array ( [ self . visualize_image ( image_arr , output , ) for image_arr , output in pbar ( zip ( video_arr , outputs ), total = video_arr . shape [ 0 ], desc = \"Visualize frames\" ) ] ) if output_path is not None : # get new size for output video if scaling if frame_size is None : frame_size = video_arr . shape [ 2 ] # default to same size # if float, scale as a multiple if isinstance ( frame_size , float ): frame_width = round ( video_arr . shape [ 2 ] * frame_size ) frame_height = round ( video_arr . shape [ 1 ] * frame_size ) # if int, use as width of the video and scale height proportionally elif isinstance ( frame_size , int ): frame_width = frame_size scale = frame_width / video_arr . shape [ 2 ] frame_height = round ( video_arr . shape [ 1 ] * scale ) # setup output for writing output_path = output_path . with_suffix ( \".mp4\" ) out = cv2 . VideoWriter ( str ( output_path ), cv2 . VideoWriter_fourcc ( * \"mp4v\" ), max ( 1 , int ( fps )), ( frame_width , frame_height ), ) for f in pbar ( out_frames , desc = \"Write frames\" ): if ( f . shape [ 0 ] != frame_height ) or ( f . shape [ 1 ] != frame_width ): f = cv2 . resize ( f , ( frame_width , frame_height ), # https://stackoverflow.com/a/51042104/1692709 interpolation = ( cv2 . INTER_LINEAR if f . shape [ 1 ] < frame_width else cv2 . INTER_AREA ), ) out . write ( f ) out . release () return out_frames def anatomize_video ( self , visualized_video_arr , outputs , output_path = None , fps = 30 ): \"\"\"Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in a video. Parameters ---------- visualized_video_arr : numpy array (unit8) BGR The numpy array the video after the texture has been visualized (by calling DensePoseManager.visualize_video). outputs : The outputs from running DensePoseManager.predict* Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" all_detections = [] for ix in range ( visualized_video_arr . shape [ 0 ]): detection_df = self . anatomize_image ( visualized_video_arr [ ix , ... ], outputs [ ix ]) detection_df [ \"frame\" ] = ix detection_df [ \"seconds\" ] = ix / fps all_detections . append ( detection_df ) results = pd . concat ( all_detections ) if output_path is not None : results . to_csv ( output_path , index = False ) return results Attributes \u00b6 anatomy_color_mapping = pd . read_csv ( model [ 'anatomy_color_mapping' ], index_col = 0 ) instance-attribute \u00b6 cfg = get_cfg () instance-attribute \u00b6 predictor = DefaultPredictor ( self . cfg ) instance-attribute \u00b6 vis_class_to_mesh_name = get_class_to_mesh_name_mapping ( self . cfg ) instance-attribute \u00b6 vis_embedder = build_densepose_embedder ( self . cfg ) instance-attribute \u00b6 vis_extractor = create_extractor ( self . visualizer ) instance-attribute \u00b6 vis_mesh_vertex_embeddings = { mesh_name : self . vis_embedder ( mesh_name ) . to ( self . cfg . MODEL . DEVICE ) for mesh_name in self . vis_class_to_mesh_name . values () if self . vis_embedder . has_embeddings ( mesh_name )} instance-attribute \u00b6 visualizer = model [ 'viz_class' ]( self . cfg , device = self . cfg . MODEL . DEVICE , None = model . get ( 'viz_class_kwargs' , {})) instance-attribute \u00b6 Functions \u00b6 __init__ ( model = MODELS [ 'chimps' ], model_cache_dir : Path = Path ( '.zamba_cache' ), download_region = RegionEnum ( 'us' )) \u00b6 Create a DensePoseManager object. Parameters \u00b6 dict, optional (default MODELS['chimps']) A dictionary with the densepose model defintion like those defined in MODELS. Source code in zamba/models/densepose/densepose_manager.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __init__ ( self , model = MODELS [ \"chimps\" ], model_cache_dir : Path = Path ( \".zamba_cache\" ), download_region = RegionEnum ( \"us\" ), ): \"\"\"Create a DensePoseManager object. Parameters ---------- model : dict, optional (default MODELS['chimps']) A dictionary with the densepose model defintion like those defined in MODELS. \"\"\" if not DENSEPOSE_AVAILABLE : raise ImportError ( \"Densepose not installed. See: https://zamba.drivendata.org/docs/stable/models/densepose/#installation\" ) # setup configuration for densepose self . cfg = get_cfg () add_densepose_config ( self . cfg ) self . cfg . merge_from_file ( model [ \"config\" ]) if not ( model_cache_dir / model [ \"weights\" ]) . exists (): model_cache_dir . mkdir ( parents = True , exist_ok = True ) self . cfg . MODEL . WEIGHTS = download_weights ( model [ \"weights\" ], model_cache_dir , download_region ) # automatically use CPU if no cuda available if not torch . cuda . is_available (): self . cfg . MODEL . DEVICE = \"cpu\" self . cfg . freeze () logging . getLogger ( \"fvcore\" ) . setLevel ( \"CRITICAL\" ) # silence noisy detectron2 logging # set up predictor with the configuration self . predictor = DefaultPredictor ( self . cfg ) # we have a specific texture atlas for chimps with relevant regions # labeled that we can use instead of the default segmentation self . visualizer = model [ \"viz_class\" ]( self . cfg , device = self . cfg . MODEL . DEVICE , ** model . get ( \"viz_class_kwargs\" , {}), ) # set up utilities for use with visualizer self . vis_extractor = create_extractor ( self . visualizer ) self . vis_embedder = build_densepose_embedder ( self . cfg ) self . vis_class_to_mesh_name = get_class_to_mesh_name_mapping ( self . cfg ) self . vis_mesh_vertex_embeddings = { mesh_name : self . vis_embedder ( mesh_name ) . to ( self . cfg . MODEL . DEVICE ) for mesh_name in self . vis_class_to_mesh_name . values () if self . vis_embedder . has_embeddings ( mesh_name ) } if \"anatomy_color_mapping\" in model : self . anatomy_color_mapping = pd . read_csv ( model [ \"anatomy_color_mapping\" ], index_col = 0 ) else : self . anatomy_color_mapping = None anatomize_image ( visualized_img_arr , outputs , output_path = None ) \u00b6 Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in an image. Parameters \u00b6 numpy array (unit8) BGR The numpy array the image after the texture has been visualized (by calling DensePoseManager.visualize_image). outputs The outputs from running DensePoseManager.predict* Returns \u00b6 pandas.DataFrame DataFrame with percent of pixels of the bounding box that correspond to each anatomical part Source code in zamba/models/densepose/densepose_manager.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 def anatomize_image ( self , visualized_img_arr , outputs , output_path = None ): \"\"\"Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in an image. Parameters ---------- visualized_img_arr : numpy array (unit8) BGR The numpy array the image after the texture has been visualized (by calling DensePoseManager.visualize_image). outputs : The outputs from running DensePoseManager.predict* Returns ------- pandas.DataFrame DataFrame with percent of pixels of the bounding box that correspond to each anatomical part \"\"\" if self . anatomy_color_mapping is None : raise ValueError ( \"No anatomy_color_mapping provided to track anatomy; did you mean to use a different MODEL?\" ) # no detections, return empty df for joining later (e.g., in anatomize_video) if not outputs : return pd . DataFrame ([]) _ , _ , N , bboxes_xywh , pred_classes = self . visualizer . extract_and_check_outputs_and_boxes ( self . vis_extractor ( outputs ) ) all_detections = [] for n in range ( N ): x , y , w , h = bboxes_xywh [ n ] . int () . cpu () . numpy () detection_area = visualized_img_arr [ y : y + h , x : x + w ] detection_stats = { name : ( detection_area == np . array ([[[ color . B , color . G , color . R ]]])) . all ( axis =- 1 ) . sum () / ( h * w ) # calc percent of bounding box with this color for name , color in self . anatomy_color_mapping . iterrows () } detection_stats [ \"x\" ] = x detection_stats [ \"y\" ] = y detection_stats [ \"h\" ] = h detection_stats [ \"w\" ] = w all_detections . append ( detection_stats ) results = pd . DataFrame ( all_detections ) if output_path is not None : results . to_csv ( output_path , index = False ) return results anatomize_video ( visualized_video_arr , outputs , output_path = None , fps = 30 ) \u00b6 Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in a video. Parameters \u00b6 numpy array (unit8) BGR The numpy array the video after the texture has been visualized (by calling DensePoseManager.visualize_video). outputs The outputs from running DensePoseManager.predict* Returns \u00b6 numpy array (unit8) BGR DensePose outputs visualized on top of the image. Source code in zamba/models/densepose/densepose_manager.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def anatomize_video ( self , visualized_video_arr , outputs , output_path = None , fps = 30 ): \"\"\"Convert the pose information into the percent of pixels in the detection bounding box that correspond to each part of the anatomy in a video. Parameters ---------- visualized_video_arr : numpy array (unit8) BGR The numpy array the video after the texture has been visualized (by calling DensePoseManager.visualize_video). outputs : The outputs from running DensePoseManager.predict* Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" all_detections = [] for ix in range ( visualized_video_arr . shape [ 0 ]): detection_df = self . anatomize_image ( visualized_video_arr [ ix , ... ], outputs [ ix ]) detection_df [ \"frame\" ] = ix detection_df [ \"seconds\" ] = ix / fps all_detections . append ( detection_df ) results = pd . concat ( all_detections ) if output_path is not None : results . to_csv ( output_path , index = False ) return results deserialize_output ( instances_dict = None , filename = None ) \u00b6 Source code in zamba/models/densepose/densepose_manager.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 def deserialize_output ( self , instances_dict = None , filename = None ): if filename is not None : with Path ( filename ) . open ( \"r\" ) as f : instances_dict = json . load ( f ) # handle image case is_image = False if \"frames\" not in instances_dict : instances_dict = { \"frames\" : [ instances_dict ]} is_image = True frames = [] for frame in instances_dict [ \"frames\" ]: heights , widths , boxes , scores , labels , embeddings , segmentations = zip ( * [ ( i [ \"img_height\" ], i [ \"img_width\" ], i [ \"box\" ], i [ \"score\" ], i [ \"label\" ][ \"value\" ], i [ \"embedding\" ] if i [ \"embedding\" ] is not None else [ np . nan ], i [ \"segmentation\" ] if i [ \"segmentation\" ] is not None else [ np . nan ], ) for i in frame [ \"instances\" ] ] ) frames . append ( Instances ( ( heights [ 0 ], widths [ 0 ]), pred_boxes = boxes , scores = scores , pred_classes = labels , pred_densepose = DensePoseEmbeddingPredictorOutput ( embedding = torch . tensor ( embeddings ), coarse_segm = torch . tensor ( segmentations ), ), ) ) # if image or single frame, just return the instance if is_image : return frames [ 0 ] else : return frames predict ( image_arr ) \u00b6 Main call to DensePose for inference. Runs inference on an image array. Parameters \u00b6 numpy array BGR image array Returns \u00b6 Instances Detection instances with boxes, scores, and densepose estimates. Source code in zamba/models/densepose/densepose_manager.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def predict ( self , image_arr ): \"\"\"Main call to DensePose for inference. Runs inference on an image array. Parameters ---------- image_arr : numpy array BGR image array Returns ------- Instances Detection instances with boxes, scores, and densepose estimates. \"\"\" with torch . no_grad (): instances = self . predictor ( image_arr )[ \"instances\" ] return instances predict_image ( image ) \u00b6 Run inference to get the densepose results for an image. Parameters \u00b6 image numpy array (unit8) of an image in BGR format or path to an image Returns \u00b6 tuple Returns the image array as passed or loaded and the the densepose Instances as results. Source code in zamba/models/densepose/densepose_manager.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def predict_image ( self , image ): \"\"\"Run inference to get the densepose results for an image. Parameters ---------- image : numpy array (unit8) of an image in BGR format or path to an image Returns ------- tuple Returns the image array as passed or loaded and the the densepose Instances as results. \"\"\" if isinstance ( image , ( str , Path )): image = read_image ( image , format = \"BGR\" ) return image , self . predict ( image ) predict_video ( video , video_loader_config = None , pbar = True ) \u00b6 Run inference to get the densepose results for a video. Parameters \u00b6 video numpy array (uint8) of a a video in BGR layout with time dimension first or path to a video VideoLoaderConfig, optional A video loader config for loading videos (uses all defaults except pix_fmt=\"bgr24\") bool, optional Whether to display a progress bar, by default True Returns \u00b6 tuple Tuple of (video_array, list of densepose results per frame) Source code in zamba/models/densepose/densepose_manager.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def predict_video ( self , video , video_loader_config = None , pbar = True ): \"\"\"Run inference to get the densepose results for a video. Parameters ---------- video : numpy array (uint8) of a a video in BGR layout with time dimension first or path to a video video_loader_config : VideoLoaderConfig, optional A video loader config for loading videos (uses all defaults except pix_fmt=\"bgr24\") pbar : bool, optional Whether to display a progress bar, by default True Returns ------- tuple Tuple of (video_array, list of densepose results per frame) \"\"\" if isinstance ( video , ( str , Path )): video = load_video_frames ( video , config = video_loader_config ) pbar = tqdm if pbar else lambda x , ** kwargs : x return video , [ self . predict_image ( img )[ 1 ] for img in pbar ( video , desc = \"Frames\" ) ] # just the predictions serialize_image_output ( instances , filename = None , write_embeddings = False ) \u00b6 Convert the densepose output into Python-native objects that can be written and read with json. Parameters \u00b6 Instance The output from the densepose model (str, Path), optional If not None, the filename to write the output to, by default None Source code in zamba/models/densepose/densepose_manager.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def serialize_image_output ( self , instances , filename = None , write_embeddings = False ): \"\"\"Convert the densepose output into Python-native objects that can be written and read with json. Parameters ---------- instances : Instance The output from the densepose model filename : (str, Path), optional If not None, the filename to write the output to, by default None \"\"\" if isinstance ( instances , list ): img_height , img_width = instances [ 0 ] . image_size else : img_height , img_width = instances . image_size boxes = instances . get ( \"pred_boxes\" ) . tensor scores = instances . get ( \"scores\" ) . tolist () labels = instances . get ( \"pred_classes\" ) . tolist () try : pose_result = instances . get ( \"pred_densepose\" ) except KeyError : pose_result = None # include embeddings + segmentation if they exist and they are requested write_embeddings = write_embeddings and ( pose_result is not None ) serialized = { \"instances\" : [ { \"img_height\" : img_height , \"img_width\" : img_width , \"box\" : boxes [ i ] . cpu () . tolist (), \"score\" : scores [ i ], \"label\" : { \"value\" : labels [ i ], \"mesh_name\" : self . vis_class_to_mesh_name [ labels [ i ]], }, \"embedding\" : pose_result . embedding [[ i ], ... ] . cpu () . tolist () if write_embeddings else None , \"segmentation\" : pose_result . coarse_segm [[ i ], ... ] . cpu () . tolist () if write_embeddings else None , } for i in range ( len ( instances )) ] } if filename is not None : with Path ( filename ) . open ( \"w\" ) as f : json . dump ( serialized , f , indent = 2 ) return serialized serialize_video_output ( instances , filename = None , write_embeddings = False ) \u00b6 Source code in zamba/models/densepose/densepose_manager.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def serialize_video_output ( self , instances , filename = None , write_embeddings = False ): serialized = { \"frames\" : [ self . serialize_image_output ( frame_instances , filename = None , write_embeddings = write_embeddings ) for frame_instances in instances ] } if filename is not None : with Path ( filename ) . open ( \"w\" ) as f : json . dump ( serialized , f , indent = 2 ) return serialized visualize_image ( image_arr , outputs , output_path = None ) \u00b6 Visualize the pose information. Parameters \u00b6 numpy array (unit8) BGR The numpy array representing the image. outputs The outputs from running DensePoseManager.predict* str or Path, optional If not None, write visualization to this path; by default None Returns \u00b6 numpy array (unit8) BGR DensePose outputs visualized on top of the image. Source code in zamba/models/densepose/densepose_manager.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 def visualize_image ( self , image_arr , outputs , output_path = None ): \"\"\"Visualize the pose information. Parameters ---------- image_arr : numpy array (unit8) BGR The numpy array representing the image. outputs : The outputs from running DensePoseManager.predict* output_path : str or Path, optional If not None, write visualization to this path; by default None Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" bw_image = cv2 . cvtColor ( image_arr , cv2 . COLOR_BGR2GRAY ) bw_image = np . tile ( bw_image [:, :, np . newaxis ], [ 1 , 1 , 3 ]) data = self . vis_extractor ( outputs ) image_vis = self . visualizer . visualize ( bw_image , data ) if output_path is not None : cv2 . imwrite ( str ( output_path ), image_vis ) return image_vis visualize_video ( video_arr , outputs , output_path = None , frame_size = None , fps = 30 , pbar = True ) \u00b6 Visualize the pose information on a video Parameters \u00b6 numpy array (unit8) BGR, time first The numpy array representing the video. outputs The outputs from running DensePoseManager.predict* str or Path, optional If not None, write visualization to this path (should be .mp4); by default None (innt, float), optional If frame_size is float, scale up or down by that float value; if frame_size is an integer, set width to that size and scale height appropriately. int frames per second for output video if writing; defaults to 30 bool display a progress bar Returns \u00b6 numpy array (unit8) BGR DensePose outputs visualized on top of the image. Source code in zamba/models/densepose/densepose_manager.py 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 def visualize_video ( self , video_arr , outputs , output_path = None , frame_size = None , fps = 30 , pbar = True ): \"\"\"Visualize the pose information on a video Parameters ---------- video_arr : numpy array (unit8) BGR, time first The numpy array representing the video. outputs : The outputs from running DensePoseManager.predict* output_path : str or Path, optional If not None, write visualization to this path (should be .mp4); by default None frame_size : (innt, float), optional If frame_size is float, scale up or down by that float value; if frame_size is an integer, set width to that size and scale height appropriately. fps : int frames per second for output video if writing; defaults to 30 pbar : bool display a progress bar Returns ------- numpy array (unit8) BGR DensePose outputs visualized on top of the image. \"\"\" pbar = tqdm if pbar else lambda x , ** kwargs : x out_frames = np . array ( [ self . visualize_image ( image_arr , output , ) for image_arr , output in pbar ( zip ( video_arr , outputs ), total = video_arr . shape [ 0 ], desc = \"Visualize frames\" ) ] ) if output_path is not None : # get new size for output video if scaling if frame_size is None : frame_size = video_arr . shape [ 2 ] # default to same size # if float, scale as a multiple if isinstance ( frame_size , float ): frame_width = round ( video_arr . shape [ 2 ] * frame_size ) frame_height = round ( video_arr . shape [ 1 ] * frame_size ) # if int, use as width of the video and scale height proportionally elif isinstance ( frame_size , int ): frame_width = frame_size scale = frame_width / video_arr . shape [ 2 ] frame_height = round ( video_arr . shape [ 1 ] * scale ) # setup output for writing output_path = output_path . with_suffix ( \".mp4\" ) out = cv2 . VideoWriter ( str ( output_path ), cv2 . VideoWriter_fourcc ( * \"mp4v\" ), max ( 1 , int ( fps )), ( frame_width , frame_height ), ) for f in pbar ( out_frames , desc = \"Write frames\" ): if ( f . shape [ 0 ] != frame_height ) or ( f . shape [ 1 ] != frame_width ): f = cv2 . resize ( f , ( frame_width , frame_height ), # https://stackoverflow.com/a/51042104/1692709 interpolation = ( cv2 . INTER_LINEAR if f . shape [ 1 ] < frame_width else cv2 . INTER_AREA ), ) out . write ( f ) out . release () return out_frames Functions \u00b6","title":"zamba.models.densepose.densepose_manager"},{"location":"api-reference/densepose_manager/#zambamodelsdenseposedensepose_manager","text":"","title":"zamba.models.densepose.densepose_manager"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager-attributes","text":"","title":"Attributes"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager-classes","text":"","title":"Classes"},{"location":"api-reference/densepose_manager/#zamba.models.densepose.densepose_manager-functions","text":"","title":"Functions"},{"location":"api-reference/exceptions/","text":"zamba.exceptions \u00b6 Classes \u00b6 ZambaFfmpegException \u00b6 Bases: Exception Source code in zamba/exceptions.py 4 5 6 7 class ZambaFfmpegException ( Exception ): def __init__ ( self , stderr : Union [ bytes , str ]): message = stderr . decode ( \"utf8\" , errors = \"replace\" ) if isinstance ( stderr , bytes ) else stderr super () . __init__ ( f \"Video loading failer with error: \\n { message } \" ) Functions \u00b6 __init__ ( stderr : Union [ bytes , str ]) \u00b6 Source code in zamba/exceptions.py 5 6 7 def __init__ ( self , stderr : Union [ bytes , str ]): message = stderr . decode ( \"utf8\" , errors = \"replace\" ) if isinstance ( stderr , bytes ) else stderr super () . __init__ ( f \"Video loading failer with error: \\n { message } \" )","title":"zamba.exceptions"},{"location":"api-reference/exceptions/#zambaexceptions","text":"","title":"zamba.exceptions"},{"location":"api-reference/exceptions/#zamba.exceptions-classes","text":"","title":"Classes"},{"location":"api-reference/metrics/","text":"zamba.metrics \u00b6 Functions \u00b6 compute_species_specific_metrics ( y_true : np . ndarray , y_pred : np . ndarray , labels : Optional [ List [ str ]] = None ) -> Generator [ Tuple [ str , int , float ], None , None ] \u00b6 Computes species-specific accuracy, F1, precision, and recall. Parameters: Name Type Description Default y_true np . ndarray An array with shape (samples, species) where each value indicates the presence of a species in a sample. required y_pred np . ndarray An array with shape (samples, species) where each value indicates the predicted presence of a species in a sample. required Yields: Type Description Generator [ Tuple [ str , int , float ], None, None] str, int, float: The metric name, species label index, and metric value. Source code in zamba/metrics.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def compute_species_specific_metrics ( y_true : np . ndarray , y_pred : np . ndarray , labels : Optional [ List [ str ]] = None , ) -> Generator [ Tuple [ str , int , float ], None , None ]: \"\"\"Computes species-specific accuracy, F1, precision, and recall. Args: y_true (np.ndarray): An array with shape (samples, species) where each value indicates the presence of a species in a sample. y_pred (np.ndarray): An array with shape (samples, species) where each value indicates the predicted presence of a species in a sample. Yields: str, int, float: The metric name, species label index, and metric value. \"\"\" if labels is None : labels = range ( y_true . shape [ 1 ]) elif len ( labels ) != y_true . shape [ 1 ]: raise ValueError ( f \"The number of labels ( { len ( labels ) } ) must match the number of columns in y_true ( { y_true . shape [ 1 ] } ).\" ) for index , label in enumerate ( labels ): yield \"accuracy\" , label , accuracy_score ( y_true [:, index ], y_pred [:, index ]) yield \"f1\" , label , f1_score ( y_true [:, index ], y_pred [:, index ], zero_division = 0 ) yield \"precision\" , label , precision_score ( y_true [:, index ], y_pred [:, index ], zero_division = 0 ) yield \"recall\" , label , recall_score ( y_true [:, index ], y_pred [:, index ], zero_division = 0 )","title":"zamba.metrics"},{"location":"api-reference/metrics/#zambametrics","text":"","title":"zamba.metrics"},{"location":"api-reference/metrics/#zamba.metrics-functions","text":"","title":"Functions"},{"location":"api-reference/models-config/","text":"zamba.models.config \u00b6 Attributes \u00b6 GPUS_AVAILABLE = torch . cuda . device_count () module-attribute \u00b6 MODEL_MAPPING = { 'TimeDistributedEfficientNet' : { 'transform' : zamba_image_model_transforms (), 'n_frames' : 16 }, 'SlowFast' : { 'transform' : slowfast_transforms (), 'n_frames' : 32 }} module-attribute \u00b6 WEIGHT_LOOKUP = { 'time_distributed' : 's3://drivendata-client-zamba/data/results/zamba_classification_retraining/td_full_set/version_1/' , 'european' : 's3://drivendata-client-zamba/data/results/zamba_v2_classification/european_td_dev_base/version_0/' , 'slowfast' : 's3://drivendata-client-zamba/data/results/zamba_v2_classification/experiments/slowfast_small_set_full_size_mdlite/version_2/' , 'blank_nonblank' : 's3://drivendata-client-zamba/data/results/zamba_classification_retraining/td_full_set_bnb/version_0/' } module-attribute \u00b6 Classes \u00b6 BackboneFinetuneConfig \u00b6 Bases: ZambaBaseModel Configuration containing parameters to be used for backbone finetuning. Parameters: Name Type Description Default unfreeze_backbone_at_epoch int Epoch at which the backbone will be unfrozen. Defaults to 5. required backbone_initial_ratio_lr float Used to scale down the backbone learning rate compared to rest of model. Defaults to 0.01. required multiplier int or float Multiply the learning rate by a constant value at the end of each epoch. Defaults to 1. required pre_train_bn bool Train batch normalization layers prior to finetuning. False is recommended for slowfast models and True is recommended for time distributed models. Defaults to False. required train_bn bool Make batch normalization trainable. Defaults to False. required verbose bool Display current learning rate for model and backbone. Defaults to True. required Source code in zamba/models/config.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 class BackboneFinetuneConfig ( ZambaBaseModel ): \"\"\"Configuration containing parameters to be used for backbone finetuning. Args: unfreeze_backbone_at_epoch (int, optional): Epoch at which the backbone will be unfrozen. Defaults to 5. backbone_initial_ratio_lr (float, optional): Used to scale down the backbone learning rate compared to rest of model. Defaults to 0.01. multiplier (int or float, optional): Multiply the learning rate by a constant value at the end of each epoch. Defaults to 1. pre_train_bn (bool, optional): Train batch normalization layers prior to finetuning. False is recommended for slowfast models and True is recommended for time distributed models. Defaults to False. train_bn (bool, optional): Make batch normalization trainable. Defaults to False. verbose (bool, optional): Display current learning rate for model and backbone. Defaults to True. \"\"\" unfreeze_backbone_at_epoch : Optional [ int ] = 5 backbone_initial_ratio_lr : Optional [ float ] = 0.01 multiplier : Optional [ Union [ int , float ]] = 1 pre_train_bn : Optional [ bool ] = False # freeze batch norm layers prior to finetuning train_bn : Optional [ bool ] = False # don't train bn layers in unfrozen finetuning layers verbose : Optional [ bool ] = True Attributes \u00b6 backbone_initial_ratio_lr : Optional [ float ] = 0.01 class-attribute \u00b6 multiplier : Optional [ Union [ int , float ]] = 1 class-attribute \u00b6 pre_train_bn : Optional [ bool ] = False class-attribute \u00b6 train_bn : Optional [ bool ] = False class-attribute \u00b6 unfreeze_backbone_at_epoch : Optional [ int ] = 5 class-attribute \u00b6 verbose : Optional [ bool ] = True class-attribute \u00b6 EarlyStoppingConfig \u00b6 Bases: ZambaBaseModel Configuration containing parameters to be used for early stopping. Parameters: Name Type Description Default monitor str Metric to be monitored. Options are \"val_macro_f1\" or \"val_loss\". Defaults to \"val_macro_f1\". required patience int Number of epochs with no improvement after which training will be stopped. Defaults to 5. required verbose bool Verbosity mode. Defaults to True. required mode str Options are \"min\" or \"max\". In \"min\" mode, training will stop when the quantity monitored has stopped decreasing and in \"max\" mode it will stop when the quantity monitored has stopped increasing. If None, mode will be inferred from monitor. Defaults to None. required Source code in zamba/models/config.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 class EarlyStoppingConfig ( ZambaBaseModel ): \"\"\"Configuration containing parameters to be used for early stopping. Args: monitor (str): Metric to be monitored. Options are \"val_macro_f1\" or \"val_loss\". Defaults to \"val_macro_f1\". patience (int): Number of epochs with no improvement after which training will be stopped. Defaults to 5. verbose (bool): Verbosity mode. Defaults to True. mode (str, optional): Options are \"min\" or \"max\". In \"min\" mode, training will stop when the quantity monitored has stopped decreasing and in \"max\" mode it will stop when the quantity monitored has stopped increasing. If None, mode will be inferred from monitor. Defaults to None. \"\"\" monitor : MonitorEnum = \"val_macro_f1\" patience : int = 5 verbose : bool = True mode : Optional [ str ] = None @root_validator def validate_mode ( cls , values ): mode = { \"val_macro_f1\" : \"max\" , \"val_loss\" : \"min\" }[ values . get ( \"monitor\" )] user_mode = values . get ( \"mode\" ) if user_mode is None : values [ \"mode\" ] = mode elif user_mode != mode : raise ValueError ( f \"Provided mode { user_mode } is incorrect for { values . get ( 'monitor' ) } monitor.\" ) return values Attributes \u00b6 mode : Optional [ str ] = None class-attribute \u00b6 monitor : MonitorEnum = 'val_macro_f1' class-attribute \u00b6 patience : int = 5 class-attribute \u00b6 verbose : bool = True class-attribute \u00b6 Functions \u00b6 validate_mode ( values ) \u00b6 Source code in zamba/models/config.py 272 273 274 275 276 277 278 279 280 281 282 @root_validator def validate_mode ( cls , values ): mode = { \"val_macro_f1\" : \"max\" , \"val_loss\" : \"min\" }[ values . get ( \"monitor\" )] user_mode = values . get ( \"mode\" ) if user_mode is None : values [ \"mode\" ] = mode elif user_mode != mode : raise ValueError ( f \"Provided mode { user_mode } is incorrect for { values . get ( 'monitor' ) } monitor.\" ) return values ModelConfig \u00b6 Bases: ZambaBaseModel Contains all configs necessary to use a model for training or inference. Must contain a train_config or a predict_config at a minimum. Parameters: Name Type Description Default video_loader_config VideoLoaderConfig An instantiated VideoLoaderConfig. If None, will use default video loader config for model specified in TrainConfig or PredictConfig. required train_config TrainConfig An instantiated TrainConfig. Defaults to None. required predict_config PredictConfig An instantiated PredictConfig. Defaults to None. required Source code in zamba/models/config.py 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 class ModelConfig ( ZambaBaseModel ): \"\"\"Contains all configs necessary to use a model for training or inference. Must contain a train_config or a predict_config at a minimum. Args: video_loader_config (VideoLoaderConfig, optional): An instantiated VideoLoaderConfig. If None, will use default video loader config for model specified in TrainConfig or PredictConfig. train_config (TrainConfig, optional): An instantiated TrainConfig. Defaults to None. predict_config (PredictConfig, optional): An instantiated PredictConfig. Defaults to None. \"\"\" video_loader_config : Optional [ VideoLoaderConfig ] = None train_config : Optional [ TrainConfig ] = None predict_config : Optional [ PredictConfig ] = None class Config : json_loads = yaml . safe_load @root_validator ( skip_on_failure = True ) def one_config_must_exist ( cls , values ): if values [ \"train_config\" ] is None and values [ \"predict_config\" ] is None : raise ValueError ( \"Must provide either `train_config` or `predict_config`.\" ) else : return values @root_validator ( skip_on_failure = True ) def get_default_video_loader_config ( cls , values ): if values [ \"video_loader_config\" ] is None : model_name = ( values [ \"train_config\" ] . model_name if values [ \"train_config\" ] is not None else values [ \"predict_config\" ] . model_name ) logger . info ( f \"No video loader config specified. Using default for { model_name } .\" ) config_file = MODELS_DIRECTORY / f \" { model_name } /config.yaml\" with config_file . open () as f : config_dict = yaml . safe_load ( f ) values [ \"video_loader_config\" ] = VideoLoaderConfig ( ** config_dict [ \"video_loader_config\" ]) return values Attributes \u00b6 predict_config : Optional [ PredictConfig ] = None class-attribute \u00b6 train_config : Optional [ TrainConfig ] = None class-attribute \u00b6 video_loader_config : Optional [ VideoLoaderConfig ] = None class-attribute \u00b6 Classes \u00b6 Config \u00b6 Source code in zamba/models/config.py 884 885 class Config : json_loads = yaml . safe_load Attributes \u00b6 json_loads = yaml . safe_load class-attribute \u00b6 Functions \u00b6 get_default_video_loader_config ( values ) \u00b6 Source code in zamba/models/config.py 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 @root_validator ( skip_on_failure = True ) def get_default_video_loader_config ( cls , values ): if values [ \"video_loader_config\" ] is None : model_name = ( values [ \"train_config\" ] . model_name if values [ \"train_config\" ] is not None else values [ \"predict_config\" ] . model_name ) logger . info ( f \"No video loader config specified. Using default for { model_name } .\" ) config_file = MODELS_DIRECTORY / f \" { model_name } /config.yaml\" with config_file . open () as f : config_dict = yaml . safe_load ( f ) values [ \"video_loader_config\" ] = VideoLoaderConfig ( ** config_dict [ \"video_loader_config\" ]) return values one_config_must_exist ( values ) \u00b6 Source code in zamba/models/config.py 887 888 889 890 891 892 @root_validator ( skip_on_failure = True ) def one_config_must_exist ( cls , values ): if values [ \"train_config\" ] is None and values [ \"predict_config\" ] is None : raise ValueError ( \"Must provide either `train_config` or `predict_config`.\" ) else : return values ModelEnum \u00b6 Bases: str , Enum Shorthand names of models supported by zamba. Source code in zamba/models/config.py 53 54 55 56 57 58 59 class ModelEnum ( str , Enum ): \"\"\"Shorthand names of models supported by zamba.\"\"\" time_distributed = \"time_distributed\" slowfast = \"slowfast\" european = \"european\" blank_nonblank = \"blank_nonblank\" Attributes \u00b6 blank_nonblank = 'blank_nonblank' class-attribute \u00b6 european = 'european' class-attribute \u00b6 slowfast = 'slowfast' class-attribute \u00b6 time_distributed = 'time_distributed' class-attribute \u00b6 MonitorEnum \u00b6 Bases: str , Enum Validation metric to monitor for early stopping. Training is stopped when no improvement is observed. Source code in zamba/models/config.py 62 63 64 65 66 67 class MonitorEnum ( str , Enum ): \"\"\"Validation metric to monitor for early stopping. Training is stopped when no improvement is observed.\"\"\" val_macro_f1 = \"val_macro_f1\" val_loss = \"val_loss\" Attributes \u00b6 val_loss = 'val_loss' class-attribute \u00b6 val_macro_f1 = 'val_macro_f1' class-attribute \u00b6 PredictConfig \u00b6 Bases: ZambaBaseModel Configuration for using a model for inference. Parameters: Name Type Description Default filepaths FilePath Path to a CSV containing videos for inference, with one row per video in the data_dir. There must be a column called 'filepath' (absolute or relative to the data_dir). If None, uses all files in data_dir. Defaults to None. required data_dir DirectoryPath Path to a directory containing videos for inference. Defaults to the working directory. required model_name str Name of the model to use for inference. Options are: time_distributed, slowfast, european, blank_nonblank. Defaults to time_distributed. required checkpoint FilePath Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to generate predictions. If None, defaults to a pretrained model. Defaults to None. required gpus int Number of GPUs to use for inference. Defaults to all of the available GPUs found on the machine. required num_workers int Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. The maximum value is the number of CPUs in the system. Defaults to 3. required batch_size int Batch size to use for inference. Defaults to 2. required save bool Whether to save out predictions. If False, predictions are not saved. Defaults to True. required save_dir Path An optional directory in which to save the model predictions and configuration yaml. If no save_dir is specified and save=True, outputs will be written to the current working directory. Defaults to None. required overwrite bool If True, overwrite outputs in save_dir if they exist. Defaults to False. required dry_run bool Perform inference on a single batch for testing. Predictions will not be saved. Defaults to False. required proba_threshold float Probability threshold for classification. If specified, binary predictions are returned with 1 being greater than the threshold and 0 being less than or equal to the threshold. If None, return probability scores for each species. Defaults to None. required output_class_names bool Output the species with the highest probability score as a single prediction for each video. If False, return probabilty scores for each species. Defaults to False. required weight_download_region str s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (European Union), or \"asia\" (Asia Pacific). Defaults to \"us\". required skip_load_validation bool By default, zamba runs a check to verify that all videos can be loaded and skips files that cannot be loaded. This can be time intensive, depending on how many videos there are. If you are very confident all your videos can be loaded, you can set this to True and skip this check. Defaults to False. required model_cache_dir Path Cache directory where downloaded model weights will be saved. If None and no environment variable is set, will use your default cache directory. Defaults to None. required Source code in zamba/models/config.py 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 class PredictConfig ( ZambaBaseModel ): \"\"\" Configuration for using a model for inference. Args: filepaths (FilePath): Path to a CSV containing videos for inference, with one row per video in the data_dir. There must be a column called 'filepath' (absolute or relative to the data_dir). If None, uses all files in data_dir. Defaults to None. data_dir (DirectoryPath): Path to a directory containing videos for inference. Defaults to the working directory. model_name (str, optional): Name of the model to use for inference. Options are: time_distributed, slowfast, european, blank_nonblank. Defaults to time_distributed. checkpoint (FilePath, optional): Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to generate predictions. If None, defaults to a pretrained model. Defaults to None. gpus (int): Number of GPUs to use for inference. Defaults to all of the available GPUs found on the machine. num_workers (int): Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. The maximum value is the number of CPUs in the system. Defaults to 3. batch_size (int): Batch size to use for inference. Defaults to 2. save (bool): Whether to save out predictions. If False, predictions are not saved. Defaults to True. save_dir (Path, optional): An optional directory in which to save the model predictions and configuration yaml. If no save_dir is specified and save=True, outputs will be written to the current working directory. Defaults to None. overwrite (bool): If True, overwrite outputs in save_dir if they exist. Defaults to False. dry_run (bool): Perform inference on a single batch for testing. Predictions will not be saved. Defaults to False. proba_threshold (float, optional): Probability threshold for classification. If specified, binary predictions are returned with 1 being greater than the threshold and 0 being less than or equal to the threshold. If None, return probability scores for each species. Defaults to None. output_class_names (bool): Output the species with the highest probability score as a single prediction for each video. If False, return probabilty scores for each species. Defaults to False. weight_download_region (str): s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (European Union), or \"asia\" (Asia Pacific). Defaults to \"us\". skip_load_validation (bool): By default, zamba runs a check to verify that all videos can be loaded and skips files that cannot be loaded. This can be time intensive, depending on how many videos there are. If you are very confident all your videos can be loaded, you can set this to True and skip this check. Defaults to False. model_cache_dir (Path, optional): Cache directory where downloaded model weights will be saved. If None and no environment variable is set, will use your default cache directory. Defaults to None. \"\"\" data_dir : DirectoryPath = \"\" filepaths : Optional [ FilePath ] = None checkpoint : Optional [ FilePath ] = None model_name : Optional [ ModelEnum ] = ModelEnum . time_distributed gpus : int = GPUS_AVAILABLE num_workers : int = 3 batch_size : int = 2 save : bool = True save_dir : Optional [ Path ] = None overwrite : bool = False dry_run : bool = False proba_threshold : Optional [ float ] = None output_class_names : bool = False weight_download_region : RegionEnum = \"us\" skip_load_validation : bool = False model_cache_dir : Optional [ Path ] = None _validate_gpus = validator ( \"gpus\" , allow_reuse = True , pre = True )( validate_gpus ) _validate_model_cache_dir = validator ( \"model_cache_dir\" , allow_reuse = True , always = True )( validate_model_cache_dir ) @root_validator ( skip_on_failure = True ) def validate_dry_run_and_save ( cls , values ): if values [ \"dry_run\" ] and ( ( values [ \"save\" ] is not False ) or ( values [ \"save_dir\" ] is not None ) ): logger . warning ( \"Cannot save when predicting with dry_run=True. Setting save=False and save_dir=None.\" ) values [ \"save\" ] = False values [ \"save_dir\" ] = None return values @root_validator ( skip_on_failure = True ) def validate_save_dir ( cls , values ): save_dir = values [ \"save_dir\" ] save = values [ \"save\" ] # if no save_dir but save is True, use current working directory if save_dir is None and save : save_dir = Path . cwd () if save_dir is not None : # check if files exist if ( ( save_dir / \"zamba_predictions.csv\" ) . exists () or ( save_dir / \"predict_configuration.yaml\" ) . exists () ) and not values [ \"overwrite\" ]: raise ValueError ( f \"zamba_predictions.csv and/or predict_configuration.yaml already exist in { save_dir } . If you would like to overwrite, set overwrite=True\" ) # make a directory if needed save_dir . mkdir ( parents = True , exist_ok = True ) # set save to True if save_dir is set if not save : save = True values [ \"save_dir\" ] = save_dir values [ \"save\" ] = save return values _validate_model_name_and_checkpoint = root_validator ( allow_reuse = True , skip_on_failure = True )( validate_model_name_and_checkpoint ) @root_validator ( skip_on_failure = True ) def validate_proba_threshold ( cls , values ): if values [ \"proba_threshold\" ] is not None : if ( values [ \"proba_threshold\" ] <= 0 ) or ( values [ \"proba_threshold\" ] >= 1 ): raise ValueError ( \"Setting proba_threshold outside of the range (0, 1) will cause all probabilities to be rounded to the same value.\" ) if values [ \"output_class_names\" ] is True : logger . warning ( \"`output_class_names` will be ignored because `proba_threshold` is specified.\" ) return values @root_validator ( pre = False , skip_on_failure = True ) def get_filepaths ( cls , values ): \"\"\"If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column `filepath` contains files with valid suffixes. \"\"\" if values [ \"filepaths\" ] is None : logger . info ( f \"Getting files in { values [ 'data_dir' ] } .\" ) files = [] new_suffixes = [] # iterate over all files in data directory for f in values [ \"data_dir\" ] . rglob ( \"*\" ): if f . is_file (): # keep just files with supported suffixes if f . suffix . lower () in VIDEO_SUFFIXES : files . append ( f . resolve ()) else : new_suffixes . append ( f . suffix . lower ()) if len ( new_suffixes ) > 0 : logger . warning ( f \"Ignoring { len ( new_suffixes ) } file(s) with suffixes { set ( new_suffixes ) } . To include, specify all video suffixes with a VIDEO_SUFFIXES environment variable.\" ) if len ( files ) == 0 : raise ValueError ( f \"No video files found in { values [ 'data_dir' ] } .\" ) logger . info ( f \"Found { len ( files ) } videos in { values [ 'data_dir' ] } .\" ) values [ \"filepaths\" ] = pd . DataFrame ( files , columns = [ \"filepath\" ]) return values @root_validator ( skip_on_failure = True ) def validate_files ( cls , values ): # if globbing from data directory, already have valid dataframe if isinstance ( values [ \"filepaths\" ], pd . DataFrame ): files_df = values [ \"filepaths\" ] else : # make into dataframe even if only one column for clearer indexing files_df = pd . DataFrame ( pd . read_csv ( values [ \"filepaths\" ])) if \"filepath\" not in files_df . columns : raise ValueError ( f \" { values [ 'filepaths' ] } must contain a `filepath` column.\" ) else : files_df = files_df [[ \"filepath\" ]] # can only contain one row per filepath num_duplicates = len ( files_df ) - files_df . filepath . nunique () if num_duplicates > 0 : logger . warning ( f \"Found { num_duplicates } duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\" ) files_df = files_df [[ \"filepath\" ]] . drop_duplicates () values [ \"filepaths\" ] = check_files_exist_and_load ( df = files_df , data_dir = values [ \"data_dir\" ], skip_load_validation = values [ \"skip_load_validation\" ], ) return values Attributes \u00b6 batch_size : int = 2 class-attribute \u00b6 checkpoint : Optional [ FilePath ] = None class-attribute \u00b6 data_dir : DirectoryPath = '' class-attribute \u00b6 dry_run : bool = False class-attribute \u00b6 filepaths : Optional [ FilePath ] = None class-attribute \u00b6 gpus : int = GPUS_AVAILABLE class-attribute \u00b6 model_cache_dir : Optional [ Path ] = None class-attribute \u00b6 model_name : Optional [ ModelEnum ] = ModelEnum . time_distributed class-attribute \u00b6 num_workers : int = 3 class-attribute \u00b6 output_class_names : bool = False class-attribute \u00b6 overwrite : bool = False class-attribute \u00b6 proba_threshold : Optional [ float ] = None class-attribute \u00b6 save : bool = True class-attribute \u00b6 save_dir : Optional [ Path ] = None class-attribute \u00b6 skip_load_validation : bool = False class-attribute \u00b6 weight_download_region : RegionEnum = 'us' class-attribute \u00b6 Functions \u00b6 get_filepaths ( values ) \u00b6 If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column filepath contains files with valid suffixes. Source code in zamba/models/config.py 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 @root_validator ( pre = False , skip_on_failure = True ) def get_filepaths ( cls , values ): \"\"\"If no file list is passed, get all files in data directory. Warn if there are unsupported suffixes. Filepaths is set to a dataframe, where column `filepath` contains files with valid suffixes. \"\"\" if values [ \"filepaths\" ] is None : logger . info ( f \"Getting files in { values [ 'data_dir' ] } .\" ) files = [] new_suffixes = [] # iterate over all files in data directory for f in values [ \"data_dir\" ] . rglob ( \"*\" ): if f . is_file (): # keep just files with supported suffixes if f . suffix . lower () in VIDEO_SUFFIXES : files . append ( f . resolve ()) else : new_suffixes . append ( f . suffix . lower ()) if len ( new_suffixes ) > 0 : logger . warning ( f \"Ignoring { len ( new_suffixes ) } file(s) with suffixes { set ( new_suffixes ) } . To include, specify all video suffixes with a VIDEO_SUFFIXES environment variable.\" ) if len ( files ) == 0 : raise ValueError ( f \"No video files found in { values [ 'data_dir' ] } .\" ) logger . info ( f \"Found { len ( files ) } videos in { values [ 'data_dir' ] } .\" ) values [ \"filepaths\" ] = pd . DataFrame ( files , columns = [ \"filepath\" ]) return values validate_dry_run_and_save ( values ) \u00b6 Source code in zamba/models/config.py 742 743 744 745 746 747 748 749 750 751 752 753 @root_validator ( skip_on_failure = True ) def validate_dry_run_and_save ( cls , values ): if values [ \"dry_run\" ] and ( ( values [ \"save\" ] is not False ) or ( values [ \"save_dir\" ] is not None ) ): logger . warning ( \"Cannot save when predicting with dry_run=True. Setting save=False and save_dir=None.\" ) values [ \"save\" ] = False values [ \"save_dir\" ] = None return values validate_files ( values ) \u00b6 Source code in zamba/models/config.py 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 @root_validator ( skip_on_failure = True ) def validate_files ( cls , values ): # if globbing from data directory, already have valid dataframe if isinstance ( values [ \"filepaths\" ], pd . DataFrame ): files_df = values [ \"filepaths\" ] else : # make into dataframe even if only one column for clearer indexing files_df = pd . DataFrame ( pd . read_csv ( values [ \"filepaths\" ])) if \"filepath\" not in files_df . columns : raise ValueError ( f \" { values [ 'filepaths' ] } must contain a `filepath` column.\" ) else : files_df = files_df [[ \"filepath\" ]] # can only contain one row per filepath num_duplicates = len ( files_df ) - files_df . filepath . nunique () if num_duplicates > 0 : logger . warning ( f \"Found { num_duplicates } duplicate row(s) in filepaths csv. Dropping duplicates so predictions will have one row per video.\" ) files_df = files_df [[ \"filepath\" ]] . drop_duplicates () values [ \"filepaths\" ] = check_files_exist_and_load ( df = files_df , data_dir = values [ \"data_dir\" ], skip_load_validation = values [ \"skip_load_validation\" ], ) return values validate_proba_threshold ( values ) \u00b6 Source code in zamba/models/config.py 790 791 792 793 794 795 796 797 798 799 800 801 802 @root_validator ( skip_on_failure = True ) def validate_proba_threshold ( cls , values ): if values [ \"proba_threshold\" ] is not None : if ( values [ \"proba_threshold\" ] <= 0 ) or ( values [ \"proba_threshold\" ] >= 1 ): raise ValueError ( \"Setting proba_threshold outside of the range (0, 1) will cause all probabilities to be rounded to the same value.\" ) if values [ \"output_class_names\" ] is True : logger . warning ( \"`output_class_names` will be ignored because `proba_threshold` is specified.\" ) return values validate_save_dir ( values ) \u00b6 Source code in zamba/models/config.py 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 @root_validator ( skip_on_failure = True ) def validate_save_dir ( cls , values ): save_dir = values [ \"save_dir\" ] save = values [ \"save\" ] # if no save_dir but save is True, use current working directory if save_dir is None and save : save_dir = Path . cwd () if save_dir is not None : # check if files exist if ( ( save_dir / \"zamba_predictions.csv\" ) . exists () or ( save_dir / \"predict_configuration.yaml\" ) . exists () ) and not values [ \"overwrite\" ]: raise ValueError ( f \"zamba_predictions.csv and/or predict_configuration.yaml already exist in { save_dir } . If you would like to overwrite, set overwrite=True\" ) # make a directory if needed save_dir . mkdir ( parents = True , exist_ok = True ) # set save to True if save_dir is set if not save : save = True values [ \"save_dir\" ] = save_dir values [ \"save\" ] = save return values SchedulerConfig \u00b6 Bases: ZambaBaseModel Configuration containing parameters for a custom pytorch learning rate scheduler. See https://pytorch.org/docs/stable/optim.html for options. Parameters: Name Type Description Default scheduler str Name of learning rate scheduler to use. See https://pytorch.org/docs/stable/optim.html for options. required scheduler_params dict Parameters passed to learning rate scheduler upon initialization (eg. {\"milestones\": [1], \"gamma\": 0.5, \"verbose\": True}). Defaults to None. required Source code in zamba/models/config.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 class SchedulerConfig ( ZambaBaseModel ): \"\"\"Configuration containing parameters for a custom pytorch learning rate scheduler. See https://pytorch.org/docs/stable/optim.html for options. Args: scheduler (str): Name of learning rate scheduler to use. See https://pytorch.org/docs/stable/optim.html for options. scheduler_params (dict, optional): Parameters passed to learning rate scheduler upon initialization (eg. {\"milestones\": [1], \"gamma\": 0.5, \"verbose\": True}). Defaults to None. \"\"\" scheduler : Optional [ str ] scheduler_params : Optional [ dict ] = None @validator ( \"scheduler\" , always = True ) def validate_scheduler ( cls , scheduler ): if scheduler is None : return None elif scheduler not in torch . optim . lr_scheduler . __dict__ . keys (): raise ValueError ( \"Scheduler is not a `torch.optim.lr_scheduler`. \" \"See https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py \" \"for options.\" ) else : return scheduler Attributes \u00b6 scheduler : Optional [ str ] class-attribute \u00b6 scheduler_params : Optional [ dict ] = None class-attribute \u00b6 Functions \u00b6 validate_scheduler ( scheduler ) \u00b6 Source code in zamba/models/config.py 300 301 302 303 304 305 306 307 308 309 310 311 312 @validator ( \"scheduler\" , always = True ) def validate_scheduler ( cls , scheduler ): if scheduler is None : return None elif scheduler not in torch . optim . lr_scheduler . __dict__ . keys (): raise ValueError ( \"Scheduler is not a `torch.optim.lr_scheduler`. \" \"See https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py \" \"for options.\" ) else : return scheduler TrainConfig \u00b6 Bases: ZambaBaseModel Configuration for training a model. Parameters: Name Type Description Default labels FilePath or pandas DataFrame Path to a CSV or pandas DataFrame containing labels for training, with one row per label. There must be columns called 'filepath' (absolute or relative to the data_dir) and 'label', and optionally columns called 'split' (\"train\", \"val\", or \"holdout\") and 'site'. Labels must be specified to train a model. required data_dir DirectoryPath Path to a directory containing training videos. Defaults to the working directory. required model_name str Name of the model to use for training. Options are: time_distributed, slowfast, european, blank_nonblank. Defaults to time_distributed. required checkpoint FilePath Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to resume training. If None, defaults to a pretrained model. Defaults to None. required scheduler_config SchedulerConfig or str Config for setting up the learning rate scheduler on the model. If \"default\", uses scheduler that was used for training. If None, will not use a scheduler. Defaults to \"default\". required dry_run bool or int, Optional Run one training and validation batch for one epoch to detect any bugs prior to training the full model. Disables tuners, checkpoint callbacks, loggers, and logger callbacks. Defaults to False. required batch_size int Batch size to use for training. Defaults to 2. required auto_lr_find bool Use a learning rate finder algorithm when calling trainer.tune() to try to find an optimal initial learning rate. Defaults to False. The learning rate finder is not guaranteed to find a good learning rate; depending on the dataset, it can select a learning rate that leads to poor model training. Use with caution. required backbone_finetune_params BackboneFinetuneConfig Set parameters to finetune a backbone model to align with the current learning rate. Defaults to a BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5, backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False, train_bn=False, verbose=True). required gpus int Number of GPUs to train on applied per node. Defaults to all of the available GPUs found on the machine. required num_workers int Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. The maximum value is the number of CPUs in the system. Defaults to 3. required max_epochs int Stop training once this number of epochs is reached. Disabled by default (None), which stops training at 1000 epochs. required early_stopping_config EarlyStoppingConfig Configuration for early stopping, which monitors a metric during training and stops training when the metric stops improving. Defaults to EarlyStoppingConfig(monitor='val_macro_f1', patience=5, verbose=True, mode='max'). required weight_download_region str s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (European Union), or \"asia\" (Asia Pacific). Defaults to \"us\". required split_proportions dict Proportions used to divide data into training, validation, and holdout sets if a if a \"split\" column is not included in labels. Defaults to \"train\": 3, \"val\": 1, \"holdout\": 1. required save_dir Path Path to a directory where training files will be saved. Files include the best model checkpoint ( model_name .ckpt), training configuration (configuration.yaml), Tensorboard logs (events.out.tfevents...), test metrics (test_metrics.json), validation metrics (val_metrics.json), and model hyperparameters (hparams.yml). If None, a folder is created in the working directory. Defaults to None. required overwrite bool If True, will save outputs in save_dir overwriting if those exist. If False, will create auto-incremented version_n folder in save_dir with model outputs. Defaults to False. required from_scratch bool Instantiate the model with base weights. This means starting with ImageNet weights for image-based models (time_distributed, european, and blank_nonblank) and Kinetics weights for video-based models (slowfast). Defaults to False. required use_default_model_labels bool By default, output the full set of default model labels rather than just the species in the labels file. Only applies if the provided labels are a subset of the default model labels. If set to False, will replace the model head for finetuning and output only the species in the provided labels file. required model_cache_dir Path Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, uses your default cache directory. Defaults to None. required Source code in zamba/models/config.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class TrainConfig ( ZambaBaseModel ): \"\"\" Configuration for training a model. Args: labels (FilePath or pandas DataFrame): Path to a CSV or pandas DataFrame containing labels for training, with one row per label. There must be columns called 'filepath' (absolute or relative to the data_dir) and 'label', and optionally columns called 'split' (\"train\", \"val\", or \"holdout\") and 'site'. Labels must be specified to train a model. data_dir (DirectoryPath): Path to a directory containing training videos. Defaults to the working directory. model_name (str, optional): Name of the model to use for training. Options are: time_distributed, slowfast, european, blank_nonblank. Defaults to time_distributed. checkpoint (FilePath, optional): Path to a custom checkpoint file (.ckpt) generated by zamba that can be used to resume training. If None, defaults to a pretrained model. Defaults to None. scheduler_config (SchedulerConfig or str, optional): Config for setting up the learning rate scheduler on the model. If \"default\", uses scheduler that was used for training. If None, will not use a scheduler. Defaults to \"default\". dry_run (bool or int, Optional): Run one training and validation batch for one epoch to detect any bugs prior to training the full model. Disables tuners, checkpoint callbacks, loggers, and logger callbacks. Defaults to False. batch_size (int): Batch size to use for training. Defaults to 2. auto_lr_find (bool): Use a learning rate finder algorithm when calling trainer.tune() to try to find an optimal initial learning rate. Defaults to False. The learning rate finder is not guaranteed to find a good learning rate; depending on the dataset, it can select a learning rate that leads to poor model training. Use with caution. backbone_finetune_params (BackboneFinetuneConfig, optional): Set parameters to finetune a backbone model to align with the current learning rate. Defaults to a BackboneFinetuneConfig(unfreeze_backbone_at_epoch=5, backbone_initial_ratio_lr=0.01, multiplier=1, pre_train_bn=False, train_bn=False, verbose=True). gpus (int): Number of GPUs to train on applied per node. Defaults to all of the available GPUs found on the machine. num_workers (int): Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. The maximum value is the number of CPUs in the system. Defaults to 3. max_epochs (int, optional): Stop training once this number of epochs is reached. Disabled by default (None), which stops training at 1000 epochs. early_stopping_config (EarlyStoppingConfig, optional): Configuration for early stopping, which monitors a metric during training and stops training when the metric stops improving. Defaults to EarlyStoppingConfig(monitor='val_macro_f1', patience=5, verbose=True, mode='max'). weight_download_region (str): s3 region to download pretrained weights from. Options are \"us\" (United States), \"eu\" (European Union), or \"asia\" (Asia Pacific). Defaults to \"us\". split_proportions (dict): Proportions used to divide data into training, validation, and holdout sets if a if a \"split\" column is not included in labels. Defaults to \"train\": 3, \"val\": 1, \"holdout\": 1. save_dir (Path, optional): Path to a directory where training files will be saved. Files include the best model checkpoint (``model_name``.ckpt), training configuration (configuration.yaml), Tensorboard logs (events.out.tfevents...), test metrics (test_metrics.json), validation metrics (val_metrics.json), and model hyperparameters (hparams.yml). If None, a folder is created in the working directory. Defaults to None. overwrite (bool): If True, will save outputs in `save_dir` overwriting if those exist. If False, will create auto-incremented `version_n` folder in `save_dir` with model outputs. Defaults to False. skip_load_validation (bool). Skip ffprobe check, which verifies that all videos can be loaded and skips files that cannot be loaded. Defaults to False. from_scratch (bool): Instantiate the model with base weights. This means starting with ImageNet weights for image-based models (time_distributed, european, and blank_nonblank) and Kinetics weights for video-based models (slowfast). Defaults to False. use_default_model_labels (bool, optional): By default, output the full set of default model labels rather than just the species in the labels file. Only applies if the provided labels are a subset of the default model labels. If set to False, will replace the model head for finetuning and output only the species in the provided labels file. model_cache_dir (Path, optional): Cache directory where downloaded model weights will be saved. If None and the MODEL_CACHE_DIR environment variable is not set, uses your default cache directory. Defaults to None. \"\"\" labels : Union [ FilePath , pd . DataFrame ] data_dir : DirectoryPath = \"\" checkpoint : Optional [ FilePath ] = None scheduler_config : Optional [ Union [ str , SchedulerConfig ]] = \"default\" model_name : Optional [ ModelEnum ] = ModelEnum . time_distributed dry_run : Union [ bool , int ] = False batch_size : int = 2 auto_lr_find : bool = False backbone_finetune_config : Optional [ BackboneFinetuneConfig ] = BackboneFinetuneConfig () gpus : int = GPUS_AVAILABLE num_workers : int = 3 max_epochs : Optional [ int ] = None early_stopping_config : Optional [ EarlyStoppingConfig ] = EarlyStoppingConfig () weight_download_region : RegionEnum = \"us\" split_proportions : Optional [ Dict [ str , int ]] = { \"train\" : 3 , \"val\" : 1 , \"holdout\" : 1 } save_dir : Path = Path . cwd () overwrite : bool = False skip_load_validation : bool = False from_scratch : bool = False use_default_model_labels : Optional [ bool ] = None model_cache_dir : Optional [ Path ] = None class Config : arbitrary_types_allowed = True _validate_gpus = validator ( \"gpus\" , allow_reuse = True , pre = True )( validate_gpus ) _validate_model_cache_dir = validator ( \"model_cache_dir\" , allow_reuse = True , always = True )( validate_model_cache_dir ) @root_validator ( skip_on_failure = True ) def validate_from_scratch_and_checkpoint ( cls , values ): if values [ \"from_scratch\" ]: if values [ \"checkpoint\" ] is not None : raise ValueError ( \"If from_scratch=True, you cannot specify a checkpoint.\" ) if values [ \"model_name\" ] is None : raise ValueError ( \"If from_scratch=True, model_name cannot be None.\" ) return values _validate_model_name_and_checkpoint = root_validator ( allow_reuse = True , skip_on_failure = True )( validate_model_name_and_checkpoint ) @validator ( \"scheduler_config\" , always = True ) def validate_scheduler_config ( cls , scheduler_config ): if scheduler_config is None : return SchedulerConfig ( scheduler = None ) elif isinstance ( scheduler_config , str ) and scheduler_config != \"default\" : raise ValueError ( \"Scheduler can either be 'default', None, or a SchedulerConfig.\" ) else : return scheduler_config @root_validator ( skip_on_failure = True ) def turn_off_load_validation_if_dry_run ( cls , values ): if values [ \"dry_run\" ] and not values [ \"skip_load_validation\" ]: logger . info ( \"Turning off video loading check since dry_run=True.\" ) values [ \"skip_load_validation\" ] = True return values @root_validator ( skip_on_failure = True ) def validate_filepaths_and_labels ( cls , values ): logger . info ( \"Validating labels csv.\" ) labels = ( pd . read_csv ( values [ \"labels\" ]) if not isinstance ( values [ \"labels\" ], pd . DataFrame ) else values [ \"labels\" ] ) if not set ([ \"label\" , \"filepath\" ]) . issubset ( labels . columns ): raise ValueError ( f \" { values [ 'labels' ] } must contain `filepath` and `label` columns.\" ) # subset to required and optional cols_to_keep = [ c for c in labels . columns if c in [ \"filepath\" , \"label\" , \"site\" , \"split\" ]] labels = labels [ cols_to_keep ] # validate split column has no partial nulls or invalid values if \"split\" in labels . columns : # if split is entirely null, warn, drop column, and generate splits automatically if labels . split . isnull () . all (): logger . warning ( \"Split column is entirely null. Will generate splits automatically using `split_proportions`.\" ) labels = labels . drop ( \"split\" , axis = 1 ) # error if split column has null values elif labels . split . isnull () . any (): raise ValueError ( f \"Found { labels . split . isnull () . sum () } row(s) with null `split`. Fill in these rows with either `train`, `val`, or `holdout`. Alternatively, do not include a `split` column in your labels and we'll generate splits for you using `split_proportions`.\" ) # otherwise check that split values are valid elif not set ( labels . split ) . issubset ({ \"train\" , \"val\" , \"holdout\" }): raise ValueError ( f \"Found the following invalid values for `split`: { set ( labels . split ) . difference ({ 'train' , 'val' , 'holdout' }) } . `split` can only contain `train`, `val`, or `holdout.`\" ) elif values [ \"split_proportions\" ] is not None : logger . warning ( \"Labels contains split column yet split_proportions are also provided. Split column in labels takes precedence.\" ) # set to None for clarity in final configuration.yaml values [ \"split_proportions\" ] = None # error if labels are entirely null null_labels = labels . label . isnull () if sum ( null_labels ) == len ( labels ): raise ValueError ( \"Species cannot be null for all videos.\" ) # skip and warn about any videos without species label elif sum ( null_labels ) > 0 : logger . warning ( f \"Found { sum ( null_labels ) } filepath(s) with no label. Will skip.\" ) labels = labels [ ~ null_labels ] # check that all videos exist and can be loaded values [ \"labels\" ] = check_files_exist_and_load ( df = labels , data_dir = values [ \"data_dir\" ], skip_load_validation = values [ \"skip_load_validation\" ], ) return values @root_validator ( skip_on_failure = True ) def validate_provided_species_and_use_default_model_labels ( cls , values ): \"\"\"If the model species are the desired output, the labels file must contain a subset of the model species. \"\"\" provided_species = set ( values [ \"labels\" ] . label ) model_species = set ( get_model_species ( checkpoint = values [ \"checkpoint\" ], model_name = values [ \"model_name\" ]) ) if not provided_species . issubset ( model_species ): # if labels are not a subset, user cannot set use_default_model_labels to True if values [ \"use_default_model_labels\" ]: raise ValueError ( \"Conflicting information between `use_default_model_labels=True` and the \" \"species provided in labels file. \" \"If you want your model to predict all the zamba species, make sure your \" \"labels are a subset. The species in the labels file that are not \" f \"in the model species are { provided_species - model_species } . \" \"If you want your model to only predict the species in your labels file, \" \"set `use_default_model_labels` to False.\" ) else : values [ \"use_default_model_labels\" ] = False # if labels are a subset, default to True if no value provided elif values [ \"use_default_model_labels\" ] is None : values [ \"use_default_model_labels\" ] = True return values @root_validator ( skip_on_failure = True ) def preprocess_labels ( cls , values ): \"\"\"One hot encode, add splits, and check for binary case. Replaces values['labels'] with modified DataFrame. Args: values: dictionary containing 'labels' and other config info \"\"\" logger . info ( \"Preprocessing labels into one hot encoded labels with one row per video.\" ) labels = values [ \"labels\" ] # lowercase to facilitate subset checking labels [ \"label\" ] = labels . label . str . lower () model_species = get_model_species ( checkpoint = values [ \"checkpoint\" ], model_name = values [ \"model_name\" ] ) labels [ \"label\" ] = pd . Categorical ( labels . label , categories = model_species if values [ \"use_default_model_labels\" ] else None ) # one hot encode collapse to one row per video labels = ( pd . get_dummies ( labels . rename ( columns = { \"label\" : \"species\" }), columns = [ \"species\" ]) . groupby ( \"filepath\" ) . max () ) # if no \"split\" column, set up train, val, and holdout split if \"split\" not in labels . columns : make_split ( labels , values ) # if there are only two species columns and every video belongs to one of them, # keep only blank label if it exists to allow resuming of blank_nonblank model # otherwise drop the second species column so the problem is treated as a binary classification species_cols = labels . filter ( regex = \"species_\" ) . columns sums = labels [ species_cols ] . sum ( axis = 1 ) if len ( species_cols ) == 2 and ( sums == 1 ) . all (): col_to_keep = \"species_blank\" if \"species_blank\" in species_cols else species_cols [ 0 ] col_to_drop = [ c for c in species_cols if c != col_to_keep ] logger . warning ( f \"Binary case detected so only one species column will be kept. Output will be the binary case of { col_to_keep } .\" ) labels = labels . drop ( columns = col_to_drop ) # filepath becomes column instead of index values [ \"labels\" ] = labels . reset_index () return values Attributes \u00b6 auto_lr_find : bool = False class-attribute \u00b6 backbone_finetune_config : Optional [ BackboneFinetuneConfig ] = BackboneFinetuneConfig () class-attribute \u00b6 batch_size : int = 2 class-attribute \u00b6 checkpoint : Optional [ FilePath ] = None class-attribute \u00b6 data_dir : DirectoryPath = '' class-attribute \u00b6 dry_run : Union [ bool , int ] = False class-attribute \u00b6 early_stopping_config : Optional [ EarlyStoppingConfig ] = EarlyStoppingConfig () class-attribute \u00b6 from_scratch : bool = False class-attribute \u00b6 gpus : int = GPUS_AVAILABLE class-attribute \u00b6 labels : Union [ FilePath , pd . DataFrame ] class-attribute \u00b6 max_epochs : Optional [ int ] = None class-attribute \u00b6 model_cache_dir : Optional [ Path ] = None class-attribute \u00b6 model_name : Optional [ ModelEnum ] = ModelEnum . time_distributed class-attribute \u00b6 num_workers : int = 3 class-attribute \u00b6 overwrite : bool = False class-attribute \u00b6 save_dir : Path = Path . cwd () class-attribute \u00b6 scheduler_config : Optional [ Union [ str , SchedulerConfig ]] = 'default' class-attribute \u00b6 skip_load_validation : bool = False class-attribute \u00b6 split_proportions : Optional [ Dict [ str , int ]] = { 'train' : 3 , 'val' : 1 , 'holdout' : 1 } class-attribute \u00b6 use_default_model_labels : Optional [ bool ] = None class-attribute \u00b6 weight_download_region : RegionEnum = 'us' class-attribute \u00b6 Classes \u00b6 Config \u00b6 Source code in zamba/models/config.py 416 417 class Config : arbitrary_types_allowed = True Attributes \u00b6 arbitrary_types_allowed = True class-attribute \u00b6 Functions \u00b6 preprocess_labels ( values ) \u00b6 One hot encode, add splits, and check for binary case. Replaces values['labels'] with modified DataFrame. Parameters: Name Type Description Default values dictionary containing 'labels' and other config info required Source code in zamba/models/config.py 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 @root_validator ( skip_on_failure = True ) def preprocess_labels ( cls , values ): \"\"\"One hot encode, add splits, and check for binary case. Replaces values['labels'] with modified DataFrame. Args: values: dictionary containing 'labels' and other config info \"\"\" logger . info ( \"Preprocessing labels into one hot encoded labels with one row per video.\" ) labels = values [ \"labels\" ] # lowercase to facilitate subset checking labels [ \"label\" ] = labels . label . str . lower () model_species = get_model_species ( checkpoint = values [ \"checkpoint\" ], model_name = values [ \"model_name\" ] ) labels [ \"label\" ] = pd . Categorical ( labels . label , categories = model_species if values [ \"use_default_model_labels\" ] else None ) # one hot encode collapse to one row per video labels = ( pd . get_dummies ( labels . rename ( columns = { \"label\" : \"species\" }), columns = [ \"species\" ]) . groupby ( \"filepath\" ) . max () ) # if no \"split\" column, set up train, val, and holdout split if \"split\" not in labels . columns : make_split ( labels , values ) # if there are only two species columns and every video belongs to one of them, # keep only blank label if it exists to allow resuming of blank_nonblank model # otherwise drop the second species column so the problem is treated as a binary classification species_cols = labels . filter ( regex = \"species_\" ) . columns sums = labels [ species_cols ] . sum ( axis = 1 ) if len ( species_cols ) == 2 and ( sums == 1 ) . all (): col_to_keep = \"species_blank\" if \"species_blank\" in species_cols else species_cols [ 0 ] col_to_drop = [ c for c in species_cols if c != col_to_keep ] logger . warning ( f \"Binary case detected so only one species column will be kept. Output will be the binary case of { col_to_keep } .\" ) labels = labels . drop ( columns = col_to_drop ) # filepath becomes column instead of index values [ \"labels\" ] = labels . reset_index () return values turn_off_load_validation_if_dry_run ( values ) \u00b6 Source code in zamba/models/config.py 449 450 451 452 453 454 @root_validator ( skip_on_failure = True ) def turn_off_load_validation_if_dry_run ( cls , values ): if values [ \"dry_run\" ] and not values [ \"skip_load_validation\" ]: logger . info ( \"Turning off video loading check since dry_run=True.\" ) values [ \"skip_load_validation\" ] = True return values validate_filepaths_and_labels ( values ) \u00b6 Source code in zamba/models/config.py 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 @root_validator ( skip_on_failure = True ) def validate_filepaths_and_labels ( cls , values ): logger . info ( \"Validating labels csv.\" ) labels = ( pd . read_csv ( values [ \"labels\" ]) if not isinstance ( values [ \"labels\" ], pd . DataFrame ) else values [ \"labels\" ] ) if not set ([ \"label\" , \"filepath\" ]) . issubset ( labels . columns ): raise ValueError ( f \" { values [ 'labels' ] } must contain `filepath` and `label` columns.\" ) # subset to required and optional cols_to_keep = [ c for c in labels . columns if c in [ \"filepath\" , \"label\" , \"site\" , \"split\" ]] labels = labels [ cols_to_keep ] # validate split column has no partial nulls or invalid values if \"split\" in labels . columns : # if split is entirely null, warn, drop column, and generate splits automatically if labels . split . isnull () . all (): logger . warning ( \"Split column is entirely null. Will generate splits automatically using `split_proportions`.\" ) labels = labels . drop ( \"split\" , axis = 1 ) # error if split column has null values elif labels . split . isnull () . any (): raise ValueError ( f \"Found { labels . split . isnull () . sum () } row(s) with null `split`. Fill in these rows with either `train`, `val`, or `holdout`. Alternatively, do not include a `split` column in your labels and we'll generate splits for you using `split_proportions`.\" ) # otherwise check that split values are valid elif not set ( labels . split ) . issubset ({ \"train\" , \"val\" , \"holdout\" }): raise ValueError ( f \"Found the following invalid values for `split`: { set ( labels . split ) . difference ({ 'train' , 'val' , 'holdout' }) } . `split` can only contain `train`, `val`, or `holdout.`\" ) elif values [ \"split_proportions\" ] is not None : logger . warning ( \"Labels contains split column yet split_proportions are also provided. Split column in labels takes precedence.\" ) # set to None for clarity in final configuration.yaml values [ \"split_proportions\" ] = None # error if labels are entirely null null_labels = labels . label . isnull () if sum ( null_labels ) == len ( labels ): raise ValueError ( \"Species cannot be null for all videos.\" ) # skip and warn about any videos without species label elif sum ( null_labels ) > 0 : logger . warning ( f \"Found { sum ( null_labels ) } filepath(s) with no label. Will skip.\" ) labels = labels [ ~ null_labels ] # check that all videos exist and can be loaded values [ \"labels\" ] = check_files_exist_and_load ( df = labels , data_dir = values [ \"data_dir\" ], skip_load_validation = values [ \"skip_load_validation\" ], ) return values validate_from_scratch_and_checkpoint ( values ) \u00b6 Source code in zamba/models/config.py 425 426 427 428 429 430 431 432 433 434 @root_validator ( skip_on_failure = True ) def validate_from_scratch_and_checkpoint ( cls , values ): if values [ \"from_scratch\" ]: if values [ \"checkpoint\" ] is not None : raise ValueError ( \"If from_scratch=True, you cannot specify a checkpoint.\" ) if values [ \"model_name\" ] is None : raise ValueError ( \"If from_scratch=True, model_name cannot be None.\" ) return values validate_provided_species_and_use_default_model_labels ( values ) \u00b6 If the model species are the desired output, the labels file must contain a subset of the model species. Source code in zamba/models/config.py 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 @root_validator ( skip_on_failure = True ) def validate_provided_species_and_use_default_model_labels ( cls , values ): \"\"\"If the model species are the desired output, the labels file must contain a subset of the model species. \"\"\" provided_species = set ( values [ \"labels\" ] . label ) model_species = set ( get_model_species ( checkpoint = values [ \"checkpoint\" ], model_name = values [ \"model_name\" ]) ) if not provided_species . issubset ( model_species ): # if labels are not a subset, user cannot set use_default_model_labels to True if values [ \"use_default_model_labels\" ]: raise ValueError ( \"Conflicting information between `use_default_model_labels=True` and the \" \"species provided in labels file. \" \"If you want your model to predict all the zamba species, make sure your \" \"labels are a subset. The species in the labels file that are not \" f \"in the model species are { provided_species - model_species } . \" \"If you want your model to only predict the species in your labels file, \" \"set `use_default_model_labels` to False.\" ) else : values [ \"use_default_model_labels\" ] = False # if labels are a subset, default to True if no value provided elif values [ \"use_default_model_labels\" ] is None : values [ \"use_default_model_labels\" ] = True return values validate_scheduler_config ( scheduler_config ) \u00b6 Source code in zamba/models/config.py 440 441 442 443 444 445 446 447 @validator ( \"scheduler_config\" , always = True ) def validate_scheduler_config ( cls , scheduler_config ): if scheduler_config is None : return SchedulerConfig ( scheduler = None ) elif isinstance ( scheduler_config , str ) and scheduler_config != \"default\" : raise ValueError ( \"Scheduler can either be 'default', None, or a SchedulerConfig.\" ) else : return scheduler_config ZambaBaseModel \u00b6 Bases: BaseModel Set defaults for all models that inherit from the pydantic base model. Source code in zamba/models/config.py 217 218 219 220 221 222 223 class ZambaBaseModel ( BaseModel ): \"\"\"Set defaults for all models that inherit from the pydantic base model.\"\"\" class Config : extra = \"forbid\" use_enum_values = True validate_assignment = True Classes \u00b6 Config \u00b6 Source code in zamba/models/config.py 220 221 222 223 class Config : extra = \"forbid\" use_enum_values = True validate_assignment = True Attributes \u00b6 extra = 'forbid' class-attribute \u00b6 use_enum_values = True class-attribute \u00b6 validate_assignment = True class-attribute \u00b6 Functions \u00b6 check_files_exist_and_load ( df : pd . DataFrame , data_dir : DirectoryPath , skip_load_validation : bool ) \u00b6 Check whether files in file list exist and can be loaded with ffmpeg. Warn and skip files that don't exist or can't be loaded. Parameters: Name Type Description Default df pd . DataFrame DataFrame with a \"filepath\" column required data_dir Path Data folder to prepend if filepath is not an absolute path. required skip_load_validation bool Skip ffprobe check that verifies all videos can be loaded. required Returns: Type Description pd.DataFrame: DataFrame with valid and loadable videos. Source code in zamba/models/config.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def check_files_exist_and_load ( df : pd . DataFrame , data_dir : DirectoryPath , skip_load_validation : bool ): \"\"\"Check whether files in file list exist and can be loaded with ffmpeg. Warn and skip files that don't exist or can't be loaded. Args: df (pd.DataFrame): DataFrame with a \"filepath\" column data_dir (Path): Data folder to prepend if filepath is not an absolute path. skip_load_validation (bool): Skip ffprobe check that verifies all videos can be loaded. Returns: pd.DataFrame: DataFrame with valid and loadable videos. \"\"\" # update filepath column to prepend data_dir df [ \"filepath\" ] = str ( data_dir ) / df . filepath . path # we can have multiple rows per file with labels so limit just to one row per file for these checks files_df = df [[ \"filepath\" ]] . drop_duplicates () # check for missing files logger . info ( f \"Checking all { len ( files_df ) : , } filepaths exist. Trying fast file checking...\" ) # try to check files in parallel paths = files_df [ \"filepath\" ] . apply ( Path ) exists = pqdm ( paths , Path . exists , n_jobs = 16 ) exists = np . array ( exists ) # if fast checking fails, fall back to slow checking # if an I/O error is in `exists`, the array has dtype `object` if exists . dtype != bool : logger . info ( \"Fast file checking failed. Running slower check, which can take 30 seconds per thousand files.\" ) exists = files_df [ \"filepath\" ] . path . exists () # select the missing files invalid_files = files_df [ ~ exists ] # if no files exist if len ( invalid_files ) == len ( files_df ): raise ValueError ( f \"None of the video filepaths exist. Are you sure they're specified correctly? Here's an example invalid path: { invalid_files . filepath . values [ 0 ] } . Either specify absolute filepaths in the csv or provide filepaths relative to `data_dir`.\" ) # if at least some files exist elif len ( invalid_files ) > 0 : logger . debug ( f \"The following files could not be found: { '/n' . join ( invalid_files . filepath . values . tolist ()) } \" ) logger . warning ( f \"Skipping { len ( invalid_files ) } file(s) that could not be found. For example, { invalid_files . filepath . values [ 0 ] } .\" ) # remove invalid files to prep for ffprobe check on remaining files_df = files_df [ ~ files_df . filepath . isin ( invalid_files . filepath )] bad_load = [] if not skip_load_validation : logger . info ( \"Checking that all videos can be loaded. If you're very confident all your videos can be loaded, you can skip this with `skip_load_validation`, but it's not recommended.\" ) # ffprobe check for f in tqdm ( files_df . filepath ): try : ffmpeg . probe ( str ( f )) except ffmpeg . Error as exc : logger . debug ( ZambaFfmpegException ( exc . stderr )) bad_load . append ( f ) if len ( bad_load ) > 0 : logger . warning ( f \"Skipping { len ( bad_load ) } file(s) that could not be loaded with ffmpeg.\" ) df = df [ ( ~ df . filepath . isin ( bad_load )) & ( ~ df . filepath . isin ( invalid_files . filepath )) ] . reset_index ( drop = True ) return df make_split ( labels , values ) \u00b6 Add a split column to labels . Parameters: Name Type Description Default labels DataFrame with one row per video required values dictionary with config info required Source code in zamba/models/config.py 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 def make_split ( labels , values ): \"\"\"Add a split column to `labels`. Args: labels: DataFrame with one row per video values: dictionary with config info \"\"\" logger . info ( f \"Dividing videos into train, val, and holdout sets using the following split proportions: { values [ 'split_proportions' ] } .\" ) # use site info if we have it if \"site\" in labels . columns : logger . info ( \"Using provided 'site' column to do a site-specific split\" ) labels [ \"split\" ] = create_site_specific_splits ( labels [ \"site\" ], proportions = values [ \"split_proportions\" ] ) else : # otherwise randomly allocate logger . info ( \"No 'site' column found so videos for each species will be randomly allocated across splits using provided split proportions.\" ) expected_splits = [ k for k , v in values [ \"split_proportions\" ] . items () if v > 0 ] random . seed ( SPLIT_SEED ) # check we have at least as many videos per species as we have splits # labels are OHE at this point num_videos_per_species = labels . filter ( regex = \"species_\" ) . sum () . to_dict () too_few = { k . split ( \"species_\" , 1 )[ 1 ]: v for k , v in num_videos_per_species . items () if 0 < v < len ( expected_splits ) } if len ( too_few ) > 0 : raise ValueError ( f \"Not all species have enough videos to allocate into the following splits: { ', ' . join ( expected_splits ) } . A minimum of { len ( expected_splits ) } videos per label is required. Found the following counts: { too_few } . Either remove these labels or add more videos.\" ) for c in labels . filter ( regex = \"species_\" ) . columns : species_df = labels [ labels [ c ] > 0 ] if len ( species_df ): # within each species, seed splits by putting one video in each set and then allocate videos based on split proportions labels . loc [ species_df . index , \"split\" ] = expected_splits + random . choices ( list ( values [ \"split_proportions\" ] . keys ()), weights = list ( values [ \"split_proportions\" ] . values ()), k = len ( species_df ) - len ( expected_splits ), ) logger . info ( f \" { labels . split . value_counts () } \" ) # write splits.csv filename = values [ \"save_dir\" ] / \"splits.csv\" logger . info ( f \"Writing out split information to { filename } .\" ) # create the directory to save if we need to values [ \"save_dir\" ] . mkdir ( parents = True , exist_ok = True ) labels . reset_index ()[[ \"filepath\" , \"split\" ]] . drop_duplicates () . to_csv ( filename , index = False ) validate_gpus ( gpus : int ) \u00b6 Ensure the number of GPUs requested is equal to or less than the number of GPUs available on the machine. Source code in zamba/models/config.py 70 71 72 73 74 75 76 def validate_gpus ( gpus : int ): \"\"\"Ensure the number of GPUs requested is equal to or less than the number of GPUs available on the machine.\"\"\" if gpus > GPUS_AVAILABLE : raise ValueError ( f \"Found only { GPUS_AVAILABLE } GPU(s). Cannot use { gpus } .\" ) else : return gpus validate_model_cache_dir ( model_cache_dir : Optional [ Path ]) \u00b6 Set up cache directory for downloading model weight. Order of priority is: config argument, environment variable, or user's default cache dir. Source code in zamba/models/config.py 79 80 81 82 83 84 85 86 87 88 def validate_model_cache_dir ( model_cache_dir : Optional [ Path ]): \"\"\"Set up cache directory for downloading model weight. Order of priority is: config argument, environment variable, or user's default cache dir. \"\"\" if model_cache_dir is None : model_cache_dir = os . getenv ( \"MODEL_CACHE_DIR\" , Path ( appdirs . user_cache_dir ()) / \"zamba\" ) model_cache_dir = Path ( model_cache_dir ) model_cache_dir . mkdir ( parents = True , exist_ok = True ) return model_cache_dir validate_model_name_and_checkpoint ( cls , values ) \u00b6 Ensures a checkpoint file or model name is provided. If a model name is provided, looks up the corresponding public checkpoint file from the official configs. Download the checkpoint if it does not yet exist. Source code in zamba/models/config.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def validate_model_name_and_checkpoint ( cls , values ): \"\"\"Ensures a checkpoint file or model name is provided. If a model name is provided, looks up the corresponding public checkpoint file from the official configs. Download the checkpoint if it does not yet exist. \"\"\" checkpoint = values . get ( \"checkpoint\" ) model_name = values . get ( \"model_name\" ) # must specify either checkpoint or model name if checkpoint is None and model_name is None : raise ValueError ( \"Must provide either model_name or checkpoint path.\" ) # checkpoint supercedes model elif checkpoint is not None and model_name is not None : logger . info ( f \"Using checkpoint file: { checkpoint } .\" ) # get model name from checkpoint so it can be used for the video loader config hparams = get_checkpoint_hparams ( checkpoint ) values [ \"model_name\" ] = available_models [ hparams [ \"model_class\" ]] . _default_model_name elif checkpoint is None and model_name is not None : if not values . get ( \"from_scratch\" ): # get public weights file from official models config values [ \"checkpoint\" ] = get_model_checkpoint_filename ( model_name ) # if cached version exists, use that cached_path = Path ( values [ \"model_cache_dir\" ]) / values [ \"checkpoint\" ] if cached_path . exists (): values [ \"checkpoint\" ] = cached_path # download if checkpoint doesn't exist if not values [ \"checkpoint\" ] . exists (): logger . info ( f \"Downloading weights for model to { values [ 'model_cache_dir' ] } .\" ) values [ \"checkpoint\" ] = download_weights ( filename = str ( values [ \"checkpoint\" ]), weight_region = values [ \"weight_download_region\" ], destination_dir = values [ \"model_cache_dir\" ], ) return values","title":"zamba.models.config"},{"location":"api-reference/models-config/#zambamodelsconfig","text":"","title":"zamba.models.config"},{"location":"api-reference/models-config/#zamba.models.config-attributes","text":"","title":"Attributes"},{"location":"api-reference/models-config/#zamba.models.config-classes","text":"","title":"Classes"},{"location":"api-reference/models-config/#zamba.models.config-functions","text":"","title":"Functions"},{"location":"api-reference/models-efficientnet_models/","text":"zamba.models.efficientnet_models \u00b6 Classes \u00b6 TimeDistributedEfficientNet \u00b6 Bases: ZambaVideoClassificationLightningModule Source code in zamba/models/efficientnet_models.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @register_model class TimeDistributedEfficientNet ( ZambaVideoClassificationLightningModule ): _default_model_name = ( \"time_distributed\" # used to look up default configuration for checkpoints ) def __init__ ( self , num_frames = 16 , finetune_from : Optional [ Union [ os . PathLike , str ]] = None , ** kwargs ): super () . __init__ ( ** kwargs ) if finetune_from is None : efficientnet = timm . create_model ( \"efficientnetv2_rw_m\" , pretrained = True ) efficientnet . classifier = nn . Identity () else : efficientnet = self . load_from_checkpoint ( finetune_from ) . base . module # freeze base layers for param in efficientnet . parameters (): param . requires_grad = False num_backbone_final_features = efficientnet . num_features self . backbone = torch . nn . ModuleList ( [ efficientnet . get_submodule ( \"blocks.5\" ), efficientnet . conv_head , efficientnet . bn2 , efficientnet . global_pool , ] ) self . base = TimeDistributed ( efficientnet , tdim = 1 ) self . classifier = nn . Sequential ( nn . Linear ( num_backbone_final_features , 256 ), nn . Dropout ( 0.2 ), nn . ReLU (), nn . Linear ( 256 , 64 ), nn . Flatten (), nn . Linear ( 64 * num_frames , self . num_classes ), ) self . save_hyperparameters ( \"num_frames\" ) def forward ( self , x ): self . base . eval () x = self . base ( x ) return self . classifier ( x ) Attributes \u00b6 backbone = torch . nn . ModuleList ([ efficientnet . get_submodule ( 'blocks.5' ), efficientnet . conv_head , efficientnet . bn2 , efficientnet . global_pool ]) instance-attribute \u00b6 base = TimeDistributed ( efficientnet , tdim = 1 ) instance-attribute \u00b6 classifier = nn . Sequential ( nn . Linear ( num_backbone_final_features , 256 ), nn . Dropout ( 0.2 ), nn . ReLU (), nn . Linear ( 256 , 64 ), nn . Flatten (), nn . Linear ( 64 * num_frames , self . num_classes )) instance-attribute \u00b6 Functions \u00b6 __init__ ( num_frames = 16 , finetune_from : Optional [ Union [ os . PathLike , str ]] = None , ** kwargs ) \u00b6 Source code in zamba/models/efficientnet_models.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , num_frames = 16 , finetune_from : Optional [ Union [ os . PathLike , str ]] = None , ** kwargs ): super () . __init__ ( ** kwargs ) if finetune_from is None : efficientnet = timm . create_model ( \"efficientnetv2_rw_m\" , pretrained = True ) efficientnet . classifier = nn . Identity () else : efficientnet = self . load_from_checkpoint ( finetune_from ) . base . module # freeze base layers for param in efficientnet . parameters (): param . requires_grad = False num_backbone_final_features = efficientnet . num_features self . backbone = torch . nn . ModuleList ( [ efficientnet . get_submodule ( \"blocks.5\" ), efficientnet . conv_head , efficientnet . bn2 , efficientnet . global_pool , ] ) self . base = TimeDistributed ( efficientnet , tdim = 1 ) self . classifier = nn . Sequential ( nn . Linear ( num_backbone_final_features , 256 ), nn . Dropout ( 0.2 ), nn . ReLU (), nn . Linear ( 256 , 64 ), nn . Flatten (), nn . Linear ( 64 * num_frames , self . num_classes ), ) self . save_hyperparameters ( \"num_frames\" ) forward ( x ) \u00b6 Source code in zamba/models/efficientnet_models.py 58 59 60 61 def forward ( self , x ): self . base . eval () x = self . base ( x ) return self . classifier ( x ) Functions \u00b6","title":"zamba.models.efficientnet_models"},{"location":"api-reference/models-efficientnet_models/#zambamodelsefficientnet_models","text":"","title":"zamba.models.efficientnet_models"},{"location":"api-reference/models-efficientnet_models/#zamba.models.efficientnet_models-classes","text":"","title":"Classes"},{"location":"api-reference/models-efficientnet_models/#zamba.models.efficientnet_models-functions","text":"","title":"Functions"},{"location":"api-reference/models-model_manager/","text":"zamba.models.model_manager \u00b6 Attributes \u00b6 Classes \u00b6 ModelManager \u00b6 Bases: object Mediates loading, configuration, and logic of model calls. Parameters: Name Type Description Default config ModelConfig Instantiated ModelConfig. required Source code in zamba/models/model_manager.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 class ModelManager ( object ): \"\"\"Mediates loading, configuration, and logic of model calls. Args: config (ModelConfig): Instantiated ModelConfig. \"\"\" def __init__ ( self , config : ModelConfig ): self . config = config @classmethod def from_yaml ( cls , config ): if not isinstance ( config , ModelConfig ): config = ModelConfig . parse_file ( config ) return cls ( config ) def train ( self ): train_model ( train_config = self . config . train_config , video_loader_config = self . config . video_loader_config , ) def predict ( self ): predict_model ( predict_config = self . config . predict_config , video_loader_config = self . config . video_loader_config , ) Attributes \u00b6 config = config instance-attribute \u00b6 Functions \u00b6 __init__ ( config : ModelConfig ) \u00b6 Source code in zamba/models/model_manager.py 434 435 def __init__ ( self , config : ModelConfig ): self . config = config from_yaml ( config ) classmethod \u00b6 Source code in zamba/models/model_manager.py 437 438 439 440 441 @classmethod def from_yaml ( cls , config ): if not isinstance ( config , ModelConfig ): config = ModelConfig . parse_file ( config ) return cls ( config ) predict () \u00b6 Source code in zamba/models/model_manager.py 449 450 451 452 453 def predict ( self ): predict_model ( predict_config = self . config . predict_config , video_loader_config = self . config . video_loader_config , ) train () \u00b6 Source code in zamba/models/model_manager.py 443 444 445 446 447 def train ( self ): train_model ( train_config = self . config . train_config , video_loader_config = self . config . video_loader_config , ) Functions \u00b6 instantiate_model ( checkpoint : os . PathLike , labels : Optional [ pd . DataFrame ] = None , scheduler_config : Optional [ SchedulerConfig ] = None , from_scratch : Optional [ bool ] = None , model_name : Optional [ ModelEnum ] = None , use_default_model_labels : Optional [ bool ] = None ) -> ZambaVideoClassificationLightningModule \u00b6 Instantiates the model from a checkpoint and detects whether the model head should be replaced. The model head is replaced if labels contain species that are not on the model or use_default_model_labels=False. Supports model instantiation for the following cases: - train from scratch (from_scratch=True) - finetune with new species (from_scratch=False, labels contains different species than model) - finetune with a subset of zamba species and output only the species in the labels file (use_default_model_labels=False) - finetune with a subset of zamba species but output all zamba species (use_default_model_labels=True) - predict using pretrained model (labels=None) Parameters: Name Type Description Default checkpoint path Path to a checkpoint on disk. required labels pd . DataFrame Dataframe where filepath is the index and columns are one hot encoded species. None scheduler_config SchedulerConfig SchedulerConfig to use for training or finetuning. Only used if labels is not None. None from_scratch bool Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Only used if labels is not None. None model_name ModelEnum Model name used to look up default hparams used for that model. Only relevant if training from scratch. None use_default_model_labels(bool, optional Whether to output the full set of default model labels rather than just the species in the labels file. Only used if labels is not None. required Returns: Name Type Description ZambaVideoClassificationLightningModule ZambaVideoClassificationLightningModule Instantiated model Source code in zamba/models/model_manager.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def instantiate_model ( checkpoint : os . PathLike , labels : Optional [ pd . DataFrame ] = None , scheduler_config : Optional [ SchedulerConfig ] = None , from_scratch : Optional [ bool ] = None , model_name : Optional [ ModelEnum ] = None , use_default_model_labels : Optional [ bool ] = None , ) -> ZambaVideoClassificationLightningModule : \"\"\"Instantiates the model from a checkpoint and detects whether the model head should be replaced. The model head is replaced if labels contain species that are not on the model or use_default_model_labels=False. Supports model instantiation for the following cases: - train from scratch (from_scratch=True) - finetune with new species (from_scratch=False, labels contains different species than model) - finetune with a subset of zamba species and output only the species in the labels file (use_default_model_labels=False) - finetune with a subset of zamba species but output all zamba species (use_default_model_labels=True) - predict using pretrained model (labels=None) Args: checkpoint (path): Path to a checkpoint on disk. labels (pd.DataFrame, optional): Dataframe where filepath is the index and columns are one hot encoded species. scheduler_config (SchedulerConfig, optional): SchedulerConfig to use for training or finetuning. Only used if labels is not None. from_scratch (bool, optional): Whether to instantiate the model with base weights. This means starting from the imagenet weights for image based models and the Kinetics weights for video models. Only used if labels is not None. model_name (ModelEnum, optional): Model name used to look up default hparams used for that model. Only relevant if training from scratch. use_default_model_labels(bool, optional): Whether to output the full set of default model labels rather than just the species in the labels file. Only used if labels is not None. Returns: ZambaVideoClassificationLightningModule: Instantiated model \"\"\" if from_scratch : hparams = get_default_hparams ( model_name ) else : hparams = get_checkpoint_hparams ( checkpoint ) model_class = available_models [ hparams [ \"model_class\" ]] logger . info ( f \"Instantiating model: { model_class . __name__ } \" ) # predicting if labels is None : # predict; load from checkpoint uses associated hparams logger . info ( \"Loading from checkpoint.\" ) model = model_class . load_from_checkpoint ( checkpoint_path = checkpoint ) return model # get species from labels file species = labels . filter ( regex = r \"^species_\" ) . columns . tolist () species = [ s . split ( \"species_\" , 1 )[ 1 ] for s in species ] # train from scratch if from_scratch : logger . info ( \"Training from scratch.\" ) # default would use scheduler used for pretrained model if scheduler_config != \"default\" : hparams . update ( scheduler_config . dict ()) hparams . update ({ \"species\" : species }) model = model_class ( ** hparams ) log_schedulers ( model ) return model # determine if finetuning or resuming training # check if species in label file are a subset of pretrained model species is_subset = set ( species ) . issubset ( set ( hparams [ \"species\" ])) if is_subset : if use_default_model_labels : return resume_training ( scheduler_config = scheduler_config , hparams = hparams , species = species , model_class = model_class , checkpoint = checkpoint , labels = labels , ) else : logger . info ( \"Limiting only to species in labels file. Replacing model head and finetuning.\" ) return replace_head ( scheduler_config = scheduler_config , hparams = hparams , species = species , model_class = model_class , checkpoint = checkpoint , ) # without a subset, you will always get a new head # the config validation prohibits setting use_default_model_labels to True without a subset else : logger . info ( \"Provided species do not fully overlap with Zamba species. Replacing model head and finetuning.\" ) return replace_head ( scheduler_config = scheduler_config , hparams = hparams , species = species , model_class = model_class , checkpoint = checkpoint , ) log_schedulers ( model ) \u00b6 Source code in zamba/models/model_manager.py 173 174 175 def log_schedulers ( model ): logger . info ( f \"Using learning rate scheduler: { model . hparams [ 'scheduler' ] } \" ) logger . info ( f \"Using scheduler params: { model . hparams [ 'scheduler_params' ] } \" ) predict_model ( predict_config : PredictConfig , video_loader_config : VideoLoaderConfig = None ) \u00b6 Predicts from a model and writes out predictions to a csv. Parameters: Name Type Description Default predict_config PredictConfig Pydantic config for performing inference. required video_loader_config VideoLoaderConfig Pydantic config for preprocessing videos. If None, will use default for model specified in PredictConfig. None Source code in zamba/models/model_manager.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 def predict_model ( predict_config : PredictConfig , video_loader_config : VideoLoaderConfig = None , ): \"\"\"Predicts from a model and writes out predictions to a csv. Args: predict_config (PredictConfig): Pydantic config for performing inference. video_loader_config (VideoLoaderConfig, optional): Pydantic config for preprocessing videos. If None, will use default for model specified in PredictConfig. \"\"\" # get default VLC for model if not specified if video_loader_config is None : video_loader_config = ModelConfig ( predict_config = predict_config , video_loader_config = video_loader_config ) . video_loader_config # set up model model = instantiate_model ( checkpoint = predict_config . checkpoint , ) data_module = ZambaDataModule ( video_loader_config = video_loader_config , transform = MODEL_MAPPING [ model . __class__ . __name__ ][ \"transform\" ], predict_metadata = predict_config . filepaths , batch_size = predict_config . batch_size , num_workers = predict_config . num_workers , ) validate_species ( model , data_module ) if video_loader_config . cache_dir is None : logger . info ( \"No cache dir is specified. Videos will not be cached.\" ) else : logger . info ( f \"Videos will be cached to { video_loader_config . cache_dir } .\" ) trainer = pl . Trainer ( gpus = predict_config . gpus , logger = False , fast_dev_run = predict_config . dry_run ) configuration = { \"model_class\" : model . model_class , \"species\" : model . species , \"predict_config\" : json . loads ( predict_config . json ( exclude = { \"filepaths\" })), \"inference_start_time\" : datetime . utcnow () . isoformat (), \"video_loader_config\" : json . loads ( video_loader_config . json ()), } if predict_config . save is not False : config_path = predict_config . save_dir / \"predict_configuration.yaml\" logger . info ( f \"Writing out full configuration to { config_path } .\" ) with config_path . open ( \"w\" ) as fp : yaml . dump ( configuration , fp ) dataloader = data_module . predict_dataloader () logger . info ( \"Starting prediction...\" ) probas = trainer . predict ( model = model , dataloaders = dataloader ) df = pd . DataFrame ( np . vstack ( probas ), columns = model . species , index = dataloader . dataset . original_indices ) # change output format if specified if predict_config . proba_threshold is not None : df = ( df > predict_config . proba_threshold ) . astype ( int ) elif predict_config . output_class_names : df = df . idxmax ( axis = 1 ) else : # round to a useful number of places df = df . round ( 5 ) if predict_config . save is not False : preds_path = predict_config . save_dir / \"zamba_predictions.csv\" logger . info ( f \"Saving out predictions to { preds_path } .\" ) with preds_path . open ( \"w\" ) as fp : df . to_csv ( fp , index = True ) return df replace_head ( scheduler_config , hparams , species , model_class , checkpoint ) \u00b6 Source code in zamba/models/model_manager.py 141 142 143 144 145 146 147 148 149 def replace_head ( scheduler_config , hparams , species , model_class , checkpoint ): # update in case we want to finetune with different scheduler if scheduler_config != \"default\" : hparams . update ( scheduler_config . dict ()) hparams . update ({ \"species\" : species }) model = model_class ( finetune_from = checkpoint , ** hparams ) log_schedulers ( model ) return model resume_training ( scheduler_config , hparams , species , model_class , checkpoint , labels ) \u00b6 Source code in zamba/models/model_manager.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def resume_training ( scheduler_config , hparams , species , model_class , checkpoint , labels , ): # resume training; add additional species columns to labels file if needed logger . info ( \"Provided species fully overlap with Zamba species. Resuming training from latest checkpoint.\" ) # update in case we want to resume with different scheduler if scheduler_config != \"default\" : hparams . update ( scheduler_config . dict ()) model = model_class . load_from_checkpoint ( checkpoint_path = checkpoint , ** hparams ) log_schedulers ( model ) return model train_model ( train_config : TrainConfig , video_loader_config : Optional [ VideoLoaderConfig ] = None ) \u00b6 Trains a model. Parameters: Name Type Description Default train_config TrainConfig Pydantic config for training. required video_loader_config VideoLoaderConfig Pydantic config for preprocessing videos. If None, will use default for model specified in TrainConfig. None Source code in zamba/models/model_manager.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def train_model ( train_config : TrainConfig , video_loader_config : Optional [ VideoLoaderConfig ] = None , ): \"\"\"Trains a model. Args: train_config (TrainConfig): Pydantic config for training. video_loader_config (VideoLoaderConfig, optional): Pydantic config for preprocessing videos. If None, will use default for model specified in TrainConfig. \"\"\" # get default VLC for model if not specified if video_loader_config is None : video_loader_config = ModelConfig ( train_config = train_config , video_loader_config = video_loader_config ) . video_loader_config # set up model model = instantiate_model ( checkpoint = train_config . checkpoint , labels = train_config . labels , scheduler_config = train_config . scheduler_config , from_scratch = train_config . from_scratch , model_name = train_config . model_name , use_default_model_labels = train_config . use_default_model_labels , ) data_module = ZambaDataModule ( video_loader_config = video_loader_config , transform = MODEL_MAPPING [ model . __class__ . __name__ ][ \"transform\" ], train_metadata = train_config . labels , batch_size = train_config . batch_size , num_workers = train_config . num_workers , ) validate_species ( model , data_module ) train_config . save_dir . mkdir ( parents = True , exist_ok = True ) # add folder version_n that auto increments if we are not overwriting tensorboard_version = train_config . save_dir . name if train_config . overwrite else None tensorboard_save_dir = ( train_config . save_dir . parent if train_config . overwrite else train_config . save_dir ) tensorboard_logger = TensorBoardLogger ( save_dir = tensorboard_save_dir , name = None , version = tensorboard_version , default_hp_metric = False , ) logging_and_save_dir = ( tensorboard_logger . log_dir if not train_config . overwrite else train_config . save_dir ) model_checkpoint = ModelCheckpoint ( dirpath = logging_and_save_dir , filename = train_config . model_name , monitor = train_config . early_stopping_config . monitor if train_config . early_stopping_config is not None else None , mode = train_config . early_stopping_config . mode if train_config . early_stopping_config is not None else \"min\" , ) callbacks = [ model_checkpoint ] if train_config . early_stopping_config is not None : callbacks . append ( EarlyStopping ( ** train_config . early_stopping_config . dict ())) if train_config . backbone_finetune_config is not None : callbacks . append ( BackboneFinetuning ( ** train_config . backbone_finetune_config . dict ())) trainer = pl . Trainer ( gpus = train_config . gpus , max_epochs = train_config . max_epochs , auto_lr_find = train_config . auto_lr_find , logger = tensorboard_logger , callbacks = callbacks , fast_dev_run = train_config . dry_run , strategy = DDPStrategy ( find_unused_parameters = False ) if data_module . multiprocessing_context is not None else None , ) if video_loader_config . cache_dir is None : logger . info ( \"No cache dir is specified. Videos will not be cached.\" ) else : logger . info ( f \"Videos will be cached to { video_loader_config . cache_dir } .\" ) if train_config . auto_lr_find : logger . info ( \"Finding best learning rate.\" ) trainer . tune ( model , data_module ) try : git_hash = git . Repo ( search_parent_directories = True ) . head . object . hexsha except git . exc . InvalidGitRepositoryError : git_hash = None configuration = { \"git_hash\" : git_hash , \"model_class\" : model . model_class , \"species\" : model . species , \"starting_learning_rate\" : model . lr , \"train_config\" : json . loads ( train_config . json ( exclude = { \"labels\" })), \"training_start_time\" : datetime . utcnow () . isoformat (), \"video_loader_config\" : json . loads ( video_loader_config . json ()), } if not train_config . dry_run : config_path = Path ( logging_and_save_dir ) / \"train_configuration.yaml\" config_path . parent . mkdir ( exist_ok = True , parents = True ) logger . info ( f \"Writing out full configuration to { config_path } .\" ) with config_path . open ( \"w\" ) as fp : yaml . dump ( configuration , fp ) logger . info ( \"Starting training...\" ) trainer . fit ( model , data_module ) if not train_config . dry_run : if trainer . datamodule . test_dataloader () is not None : logger . info ( \"Calculating metrics on holdout set.\" ) test_metrics = trainer . test ( dataloaders = trainer . datamodule . test_dataloader (), ckpt_path = \"best\" )[ 0 ] with ( Path ( logging_and_save_dir ) / \"test_metrics.json\" ) . open ( \"w\" ) as fp : json . dump ( test_metrics , fp , indent = 2 ) if trainer . datamodule . val_dataloader () is not None : logger . info ( \"Calculating metrics on validation set.\" ) val_metrics = trainer . validate ( dataloaders = trainer . datamodule . val_dataloader (), ckpt_path = \"best\" )[ 0 ] with ( Path ( logging_and_save_dir ) / \"val_metrics.json\" ) . open ( \"w\" ) as fp : json . dump ( val_metrics , fp , indent = 2 ) return trainer validate_species ( model : ZambaVideoClassificationLightningModule , data_module : ZambaDataModule ) \u00b6 Source code in zamba/models/model_manager.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def validate_species ( model : ZambaVideoClassificationLightningModule , data_module : ZambaDataModule ): conflicts = [] for dataloader_name , dataloader in zip ( ( \"Train\" , \"Val\" , \"Test\" ), ( data_module . train_dataloader (), data_module . val_dataloader (), data_module . test_dataloader (), ), ): if ( dataloader is not None ) and ( dataloader . dataset . species != model . species ): conflicts . append ( f \"\"\" { dataloader_name } dataset includes: \\n { \", \" . join ( dataloader . dataset . species ) } \\n \"\"\" ) if len ( conflicts ) > 0 : conflicts . append ( f \"\"\"Model predicts: \\n { \", \" . join ( model . species ) } \"\"\" ) conflict_msg = \" \\n\\n \" . join ( conflicts ) raise ValueError ( f \"\"\"Dataloader species and model species do not match. \\n\\n { conflict_msg } \"\"\" )","title":"zamba.models.model_manager"},{"location":"api-reference/models-model_manager/#zambamodelsmodel_manager","text":"","title":"zamba.models.model_manager"},{"location":"api-reference/models-model_manager/#zamba.models.model_manager-attributes","text":"","title":"Attributes"},{"location":"api-reference/models-model_manager/#zamba.models.model_manager-classes","text":"","title":"Classes"},{"location":"api-reference/models-model_manager/#zamba.models.model_manager-functions","text":"","title":"Functions"},{"location":"api-reference/models-slowfast_models/","text":"zamba.models.slowfast_models \u00b6 Classes \u00b6 SlowFast \u00b6 Bases: ZambaVideoClassificationLightningModule Pretrained SlowFast model for fine-tuning with the following architecture: Input -> SlowFast Base (including trainable Backbone) -> Res Basic Head -> Output Attributes: Name Type Description backbone torch . nn . Module When scheduling the backbone to train with the BackboneFinetune callback, this indicates the trainable part of the base. base torch . nn . Module The entire model prior to the head. head torch . nn . Module The trainable head. _backbone_output_dim int Dimensionality of the backbone output (and head input). Source code in zamba/models/slowfast_models.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 @register_model class SlowFast ( ZambaVideoClassificationLightningModule ): \"\"\"Pretrained SlowFast model for fine-tuning with the following architecture: Input -> SlowFast Base (including trainable Backbone) -> Res Basic Head -> Output Attributes: backbone (torch.nn.Module): When scheduling the backbone to train with the `BackboneFinetune` callback, this indicates the trainable part of the base. base (torch.nn.Module): The entire model prior to the head. head (torch.nn.Module): The trainable head. _backbone_output_dim (int): Dimensionality of the backbone output (and head input). \"\"\" _default_model_name = \"slowfast\" # used to look up default configuration for checkpoints def __init__ ( self , backbone_mode : str = \"train\" , post_backbone_dropout : Optional [ float ] = None , output_with_global_average : bool = True , head_dropout_rate : Optional [ float ] = None , head_hidden_layer_sizes : Optional [ Tuple [ int ]] = None , finetune_from : Optional [ Union [ os . PathLike , str ]] = None , ** kwargs , ): \"\"\"Initializes the SlowFast model. Args: backbone_mode (str): If \"eval\", treat the backbone as a feature extractor and set to evaluation mode in all forward passes. post_backbone_dropout (float, optional): Dropout that operates on the output of the backbone + pool (before the fully-connected layer in the head). output_with_global_average (bool): If True, apply an adaptive average pooling operation after the fully-connected layer in the head. head_dropout_rate (float, optional): Optional dropout rate applied after backbone and between projection layers in the head. head_hidden_layer_sizes (tuple of int): If not None, the size of hidden layers in the head multilayer perceptron. finetune_from (pathlike or str, optional): If not None, load an existing model from the path and resume training from an existing model. \"\"\" super () . __init__ ( ** kwargs ) if finetune_from is None : self . initialize_from_torchub () else : model = self . load_from_checkpoint ( finetune_from ) self . _backbone_output_dim = model . head . proj . in_features self . backbone = model . backbone self . base = model . base for param in self . base . parameters (): param . requires_grad = False head = ResNetBasicHead ( proj = build_multilayer_perceptron ( self . _backbone_output_dim , head_hidden_layer_sizes , self . num_classes , activation = torch . nn . ReLU , dropout = head_dropout_rate , output_activation = None , ), activation = None , pool = None , dropout = None if post_backbone_dropout is None else torch . nn . Dropout ( post_backbone_dropout ), output_pool = torch . nn . AdaptiveAvgPool3d ( 1 ), ) self . backbone_mode = backbone_mode self . head = head self . save_hyperparameters ( \"backbone_mode\" , \"head_dropout_rate\" , \"head_hidden_layer_sizes\" , \"output_with_global_average\" , \"post_backbone_dropout\" , ) def initialize_from_torchub ( self ): \"\"\"Loads SlowFast model from torchhub and prepares ZambaVideoClassificationLightningModule by removing the head and setting the backbone and base.\"\"\" # workaround for pytorch bug torch . hub . _validate_not_a_forked_repo = lambda a , b , c : True base = torch . hub . load ( \"facebookresearch/pytorchvideo:0.1.3\" , model = \"slowfast_r50\" , pretrained = True ) self . _backbone_output_dim = base . blocks [ - 1 ] . proj . in_features base . blocks = base . blocks [: - 1 ] # Remove the pre-trained head # self.backbone attribute lets `BackboneFinetune` freeze and unfreeze that module self . backbone = base . blocks [ - 2 :] self . base = base def forward ( self , x , * args , ** kwargs ): if self . backbone_mode == \"eval\" : self . base . eval () x = self . base ( x ) return self . head ( x ) Attributes \u00b6 backbone = model . backbone instance-attribute \u00b6 backbone_mode = backbone_mode instance-attribute \u00b6 base = model . base instance-attribute \u00b6 head = head instance-attribute \u00b6 Functions \u00b6 __init__ ( backbone_mode : str = 'train' , post_backbone_dropout : Optional [ float ] = None , output_with_global_average : bool = True , head_dropout_rate : Optional [ float ] = None , head_hidden_layer_sizes : Optional [ Tuple [ int ]] = None , finetune_from : Optional [ Union [ os . PathLike , str ]] = None , ** kwargs ) \u00b6 Initializes the SlowFast model. Parameters: Name Type Description Default backbone_mode str If \"eval\", treat the backbone as a feature extractor and set to evaluation mode in all forward passes. 'train' post_backbone_dropout float Dropout that operates on the output of the backbone + pool (before the fully-connected layer in the head). None output_with_global_average bool If True, apply an adaptive average pooling operation after the fully-connected layer in the head. True head_dropout_rate float Optional dropout rate applied after backbone and between projection layers in the head. None head_hidden_layer_sizes tuple of int If not None, the size of hidden layers in the head multilayer perceptron. None finetune_from pathlike or str If not None, load an existing model from the path and resume training from an existing model. None Source code in zamba/models/slowfast_models.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , backbone_mode : str = \"train\" , post_backbone_dropout : Optional [ float ] = None , output_with_global_average : bool = True , head_dropout_rate : Optional [ float ] = None , head_hidden_layer_sizes : Optional [ Tuple [ int ]] = None , finetune_from : Optional [ Union [ os . PathLike , str ]] = None , ** kwargs , ): \"\"\"Initializes the SlowFast model. Args: backbone_mode (str): If \"eval\", treat the backbone as a feature extractor and set to evaluation mode in all forward passes. post_backbone_dropout (float, optional): Dropout that operates on the output of the backbone + pool (before the fully-connected layer in the head). output_with_global_average (bool): If True, apply an adaptive average pooling operation after the fully-connected layer in the head. head_dropout_rate (float, optional): Optional dropout rate applied after backbone and between projection layers in the head. head_hidden_layer_sizes (tuple of int): If not None, the size of hidden layers in the head multilayer perceptron. finetune_from (pathlike or str, optional): If not None, load an existing model from the path and resume training from an existing model. \"\"\" super () . __init__ ( ** kwargs ) if finetune_from is None : self . initialize_from_torchub () else : model = self . load_from_checkpoint ( finetune_from ) self . _backbone_output_dim = model . head . proj . in_features self . backbone = model . backbone self . base = model . base for param in self . base . parameters (): param . requires_grad = False head = ResNetBasicHead ( proj = build_multilayer_perceptron ( self . _backbone_output_dim , head_hidden_layer_sizes , self . num_classes , activation = torch . nn . ReLU , dropout = head_dropout_rate , output_activation = None , ), activation = None , pool = None , dropout = None if post_backbone_dropout is None else torch . nn . Dropout ( post_backbone_dropout ), output_pool = torch . nn . AdaptiveAvgPool3d ( 1 ), ) self . backbone_mode = backbone_mode self . head = head self . save_hyperparameters ( \"backbone_mode\" , \"head_dropout_rate\" , \"head_hidden_layer_sizes\" , \"output_with_global_average\" , \"post_backbone_dropout\" , ) forward ( x , * args , ** kwargs ) \u00b6 Source code in zamba/models/slowfast_models.py 111 112 113 114 115 116 def forward ( self , x , * args , ** kwargs ): if self . backbone_mode == \"eval\" : self . base . eval () x = self . base ( x ) return self . head ( x ) initialize_from_torchub () \u00b6 Loads SlowFast model from torchhub and prepares ZambaVideoClassificationLightningModule by removing the head and setting the backbone and base. Source code in zamba/models/slowfast_models.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def initialize_from_torchub ( self ): \"\"\"Loads SlowFast model from torchhub and prepares ZambaVideoClassificationLightningModule by removing the head and setting the backbone and base.\"\"\" # workaround for pytorch bug torch . hub . _validate_not_a_forked_repo = lambda a , b , c : True base = torch . hub . load ( \"facebookresearch/pytorchvideo:0.1.3\" , model = \"slowfast_r50\" , pretrained = True ) self . _backbone_output_dim = base . blocks [ - 1 ] . proj . in_features base . blocks = base . blocks [: - 1 ] # Remove the pre-trained head # self.backbone attribute lets `BackboneFinetune` freeze and unfreeze that module self . backbone = base . blocks [ - 2 :] self . base = base Functions \u00b6","title":"zamba.models.slowfast_models"},{"location":"api-reference/models-slowfast_models/#zambamodelsslowfast_models","text":"","title":"zamba.models.slowfast_models"},{"location":"api-reference/models-slowfast_models/#zamba.models.slowfast_models-classes","text":"","title":"Classes"},{"location":"api-reference/models-slowfast_models/#zamba.models.slowfast_models-functions","text":"","title":"Functions"},{"location":"api-reference/models-utils/","text":"zamba.models.utils \u00b6 Attributes \u00b6 S3_BUCKET = 's3://drivendata-public-assets' module-attribute \u00b6 Classes \u00b6 RegionEnum \u00b6 Bases: str , Enum Source code in zamba/models/utils.py 17 18 19 20 class RegionEnum ( str , Enum ): us = \"us\" eu = \"eu\" asia = \"asia\" Attributes \u00b6 asia = 'asia' class-attribute \u00b6 eu = 'eu' class-attribute \u00b6 us = 'us' class-attribute \u00b6 Functions \u00b6 download_weights ( filename : str , destination_dir : Union [ os . PathLike , str ], weight_region : RegionEnum = RegionEnum ( 'us' )) -> Path \u00b6 Source code in zamba/models/utils.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def download_weights ( filename : str , destination_dir : Union [ os . PathLike , str ], weight_region : RegionEnum = RegionEnum ( \"us\" ), ) -> Path : # get s3 bucket based on region if weight_region != \"us\" : region_bucket = f \" { S3_BUCKET } - { weight_region } \" else : region_bucket = S3_BUCKET s3p = S3Path ( f \" { region_bucket } /zamba_official_models/ { filename } \" , client = S3Client ( local_cache_dir = destination_dir , no_sign_request = True ), ) s3p . download_to ( destination_dir ) return str ( Path ( destination_dir ) / s3p . name ) get_checkpoint_hparams ( checkpoint ) \u00b6 Source code in zamba/models/utils.py 62 63 def get_checkpoint_hparams ( checkpoint ): return copy . deepcopy ( _cached_hparams ( checkpoint )) get_default_hparams ( model ) \u00b6 Source code in zamba/models/utils.py 53 54 55 56 57 58 59 def get_default_hparams ( model ): if isinstance ( model , Enum ): model = model . value hparams_file = MODELS_DIRECTORY / model / \"hparams.yaml\" with hparams_file . open () as f : return yaml . safe_load ( f ) get_model_checkpoint_filename ( model_name ) \u00b6 Source code in zamba/models/utils.py 43 44 45 46 47 48 49 50 def get_model_checkpoint_filename ( model_name ): if isinstance ( model_name , Enum ): model_name = model_name . value config_file = MODELS_DIRECTORY / model_name / \"config.yaml\" with config_file . open () as f : config_dict = yaml . safe_load ( f ) return Path ( config_dict [ \"public_checkpoint\" ]) get_model_species ( checkpoint , model_name ) \u00b6 Source code in zamba/models/utils.py 71 72 73 74 75 76 77 def get_model_species ( checkpoint , model_name ): # hparams on checkpoint supersede base model if checkpoint is not None : model_species = get_checkpoint_hparams ( checkpoint )[ \"species\" ] else : model_species = get_default_hparams ( model_name )[ \"species\" ] return model_species","title":"zamba.models.utils"},{"location":"api-reference/models-utils/#zambamodelsutils","text":"","title":"zamba.models.utils"},{"location":"api-reference/models-utils/#zamba.models.utils-attributes","text":"","title":"Attributes"},{"location":"api-reference/models-utils/#zamba.models.utils-classes","text":"","title":"Classes"},{"location":"api-reference/models-utils/#zamba.models.utils-functions","text":"","title":"Functions"},{"location":"api-reference/object-detection-megadetector_lite_yolox/","text":"zamba.object_detection.yolox.megadetector_lite_yolox \u00b6 Attributes \u00b6 LOCAL_MD_LITE_MODEL = Path ( __file__ ) . parent / 'assets' / 'yolox_tiny_640_20220528.pth' module-attribute \u00b6 LOCAL_MD_LITE_MODEL_KWARGS = Path ( __file__ ) . parent / 'assets' / 'yolox_tiny_640_20220528_model_kwargs.json' module-attribute \u00b6 Classes \u00b6 FillModeEnum \u00b6 Bases: str , Enum Enum for frame filtering fill modes Attributes: Name Type Description repeat Randomly resample qualifying frames to get to n_frames score_sorted Take up to n_frames in sort order (even if some have zero probability) weighted_euclidean Sample the remaining frames weighted by their euclidean distance in time to the frames over the threshold weighted_prob Sample the remaining frames weighted by their predicted probability Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class FillModeEnum ( str , Enum ): \"\"\"Enum for frame filtering fill modes Attributes: repeat: Randomly resample qualifying frames to get to n_frames score_sorted: Take up to n_frames in sort order (even if some have zero probability) weighted_euclidean: Sample the remaining frames weighted by their euclidean distance in time to the frames over the threshold weighted_prob: Sample the remaining frames weighted by their predicted probability \"\"\" repeat = \"repeat\" score_sorted = \"score_sorted\" weighted_euclidean = \"weighted_euclidean\" weighted_prob = \"weighted_prob\" Attributes \u00b6 repeat = 'repeat' class-attribute \u00b6 score_sorted = 'score_sorted' class-attribute \u00b6 weighted_euclidean = 'weighted_euclidean' class-attribute \u00b6 weighted_prob = 'weighted_prob' class-attribute \u00b6 MegadetectorLiteYoloX \u00b6 Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 class MegadetectorLiteYoloX : def __init__ ( self , path : os . PathLike = LOCAL_MD_LITE_MODEL , kwargs : os . PathLike = LOCAL_MD_LITE_MODEL_KWARGS , config : Optional [ Union [ MegadetectorLiteYoloXConfig , dict ]] = None , ): \"\"\"MegadetectorLite based on YOLOX. Args: path (pathlike): Path to trained YoloX model checkpoint (.pth extension) config (MegadetectorLiteYoloXConfig): YoloX configuration \"\"\" if config is None : config = MegadetectorLiteYoloXConfig () elif isinstance ( config , dict ): config = MegadetectorLiteYoloXConfig . parse_obj ( config ) yolox = YoloXModel . load ( checkpoint = path , model_kwargs_path = kwargs , ) ckpt = torch . load ( yolox . args . ckpt , map_location = config . device ) model = yolox . exp . get_model () model . load_state_dict ( ckpt [ \"model\" ]) model = model . eval () . to ( config . device ) self . model = model self . yolox = yolox self . config = config self . num_classes = yolox . exp . num_classes @staticmethod def scale_and_pad_array ( image_array : np . ndarray , output_width : int , output_height : int ) -> np . ndarray : return np . array ( ImageOps . pad ( Image . fromarray ( image_array ), ( output_width , output_height ), method = Image . BICUBIC , color = None , centering = ( 0 , 0 ), ) ) def _preprocess ( self , frame : np . ndarray ) -> np . ndarray : \"\"\"Process an image for the model, including scaling/padding the image, transposing from (height, width, channel) to (channel, height, width) and casting to float. \"\"\" arr = np . ascontiguousarray ( self . scale_and_pad_array ( frame , self . config . image_width , self . config . image_height ), dtype = np . float32 , ) return np . moveaxis ( arr , 2 , 0 ) def _preprocess_video ( self , video : np . ndarray ) -> np . ndarray : \"\"\"Process a video for the model, including resizing the frames in the video, transposing from (batch, height, width, channel) to (batch, channel, height, width) and casting to float. \"\"\" resized_video = np . zeros ( ( video . shape [ 0 ], video . shape [ 3 ], self . config . image_height , self . config . image_width ), dtype = np . float32 , ) for frame_idx in range ( video . shape [ 0 ]): resized_video [ frame_idx ] = self . _preprocess ( video [ frame_idx ]) return resized_video def detect_video ( self , video_arr : np . ndarray , pbar : bool = False ): \"\"\"Runs object detection on an video. Args: video_arr (np.ndarray): An video array with dimensions (frames, height, width, channels). pbar (int): Whether to show progress bar. Defaults to False. Returns: list: A list containing detections and score for each frame. Each tuple contains two arrays: the first is an array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). The second is an array of object detection confidence scores of length (object) where object is the number of objects detected. \"\"\" pbar = tqdm if pbar else lambda x : x # batch of frames batch_size = self . config . frame_batch_size video_outputs = [] with torch . no_grad (): for i in range ( 0 , len ( video_arr ), batch_size ): a = video_arr [ i : i + batch_size ] outputs = self . model ( torch . from_numpy ( self . _preprocess_video ( a )) . to ( self . config . device ) ) outputs = postprocess ( outputs , self . num_classes , self . config . confidence , self . config . nms_threshold ) video_outputs . extend ( outputs ) detections = [] for o in pbar ( video_outputs ): detections . append ( self . _process_frame_output ( o , original_height = video_arr . shape [ 1 ], original_width = video_arr . shape [ 2 ] ) ) return detections def detect_image ( self , img_arr : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Runs object detection on an image. Args: img_arr (np.ndarray): An image array with dimensions (height, width, channels). Returns: np.ndarray: An array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). np.ndarray: An array of object detection confidence scores of length (object) where object is the number of objects detected. \"\"\" with torch . no_grad (): outputs = self . model ( torch . from_numpy ( self . _preprocess ( img_arr )) . unsqueeze ( 0 ) . to ( self . config . device ) ) output = postprocess ( outputs , self . num_classes , self . config . confidence , self . config . nms_threshold ) return self . _process_frame_output ( output [ 0 ], img_arr . shape [ 0 ], img_arr . shape [ 1 ]) def _process_frame_output ( self , output , original_height , original_width ): if output is None : return np . array ([]), np . array ([]) else : detections = pd . DataFrame ( output . cpu () . numpy (), columns = [ \"x1\" , \"y1\" , \"x2\" , \"y2\" , \"score1\" , \"score2\" , \"class_num\" ], ) . assign ( score = lambda row : row . score1 * row . score2 ) # Transform bounding box to be in terms of the original image dimensions ratio = min ( self . config . image_width / original_width , self . config . image_height / original_height , ) detections [[ \"x1\" , \"y1\" , \"x2\" , \"y2\" ]] /= ratio # Express bounding boxes in terms of proportions of original image dimensions detections [[ \"x1\" , \"x2\" ]] /= original_width detections [[ \"y1\" , \"y2\" ]] /= original_height return detections [[ \"x1\" , \"y1\" , \"x2\" , \"y2\" ]] . values , detections . score . values def filter_frames ( self , frames : np . ndarray , detections : List [ Tuple [ float , float , float , float ]] ) -> np . ndarray : \"\"\"Filter video frames using megadetector lite. Which frames are returned depends on the fill_mode and how many frames are above the confidence threshold. If more than n_frames are above the threshold, the top n_frames are returned. Otherwise add to those over threshold based on fill_mode. If none of these conditions are met, returns all frames above the threshold. Args: frames (np.ndarray): Array of video frames to filter with dimensions (frames, height, width, channels) detections (list of tuples): List of detection results for each frame. Each element is a tuple of the list of bounding boxes [array(x1, y1, x2, y2)] and the detection probabilities, both as float Returns: np.ndarray: An array of video frames of length n_frames or shorter \"\"\" frame_scores = pd . Series ( [( np . max ( score ) if ( len ( score ) > 0 ) else 0 ) for _ , score in detections ] ) . sort_values ( ascending = False ) # reduce to one score per frame selected_indices = frame_scores . loc [ frame_scores > self . config . confidence ] . index if self . config . n_frames is None : # no minimum n_frames provided, just select all the frames with scores > threshold pass elif len ( selected_indices ) >= self . config . n_frames : # num. frames with scores > threshold is greater than the requested number of frames selected_indices = ( frame_scores [ selected_indices ] . sort_values ( ascending = False ) . iloc [: self . config . n_frames ] . index ) elif len ( selected_indices ) < self . config . n_frames : # num. frames with scores > threshold is less than the requested number of frames # repeat frames that are above threshold to get to n_frames rng = np . random . RandomState ( self . config . seed ) if self . config . fill_mode == \"repeat\" : repeated_indices = rng . choice ( selected_indices , self . config . n_frames - len ( selected_indices ), replace = True , ) selected_indices = np . concatenate (( selected_indices , repeated_indices )) # take frames in sorted order up to n_frames, even if score is zero elif self . config . fill_mode == \"score_sorted\" : selected_indices = ( frame_scores . sort_values ( ascending = False ) . iloc [: self . config . n_frames ] . index ) # sample up to n_frames, prefer points closer to frames with detection elif self . config . fill_mode == \"weighted_euclidean\" : sample_from = frame_scores . loc [ ~ frame_scores . index . isin ( selected_indices )] . index # take one over euclidean distance to all points with detection weights = [ 1 / np . linalg . norm ( selected_indices - sample ) for sample in sample_from ] # normalize weights weights /= np . sum ( weights ) sampled = rng . choice ( sample_from , self . config . n_frames - len ( selected_indices ), replace = False , p = weights , ) selected_indices = np . concatenate (( selected_indices , sampled )) # sample up to n_frames, weight by predicted probability - only if some frames have nonzero prob elif ( self . config . fill_mode == \"weighted_prob\" ) and ( len ( selected_indices ) > 0 ): sample_from = frame_scores . loc [ ~ frame_scores . index . isin ( selected_indices )] . index weights = frame_scores [ sample_from ] / np . sum ( frame_scores [ sample_from ]) sampled = rng . choice ( sample_from , self . config . n_frames - len ( selected_indices ), replace = False , p = weights , ) selected_indices = np . concatenate (( selected_indices , sampled )) # sort the selected images back into their original order if self . config . sort_by_time : selected_indices = sorted ( selected_indices ) return frames [ selected_indices ] Attributes \u00b6 config = config instance-attribute \u00b6 model = model instance-attribute \u00b6 num_classes = yolox . exp . num_classes instance-attribute \u00b6 yolox = yolox instance-attribute \u00b6 Functions \u00b6 __init__ ( path : os . PathLike = LOCAL_MD_LITE_MODEL , kwargs : os . PathLike = LOCAL_MD_LITE_MODEL_KWARGS , config : Optional [ Union [ MegadetectorLiteYoloXConfig , dict ]] = None ) \u00b6 MegadetectorLite based on YOLOX. Parameters: Name Type Description Default path pathlike Path to trained YoloX model checkpoint (.pth extension) LOCAL_MD_LITE_MODEL config MegadetectorLiteYoloXConfig YoloX configuration None Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , path : os . PathLike = LOCAL_MD_LITE_MODEL , kwargs : os . PathLike = LOCAL_MD_LITE_MODEL_KWARGS , config : Optional [ Union [ MegadetectorLiteYoloXConfig , dict ]] = None , ): \"\"\"MegadetectorLite based on YOLOX. Args: path (pathlike): Path to trained YoloX model checkpoint (.pth extension) config (MegadetectorLiteYoloXConfig): YoloX configuration \"\"\" if config is None : config = MegadetectorLiteYoloXConfig () elif isinstance ( config , dict ): config = MegadetectorLiteYoloXConfig . parse_obj ( config ) yolox = YoloXModel . load ( checkpoint = path , model_kwargs_path = kwargs , ) ckpt = torch . load ( yolox . args . ckpt , map_location = config . device ) model = yolox . exp . get_model () model . load_state_dict ( ckpt [ \"model\" ]) model = model . eval () . to ( config . device ) self . model = model self . yolox = yolox self . config = config self . num_classes = yolox . exp . num_classes detect_image ( img_arr : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ] \u00b6 Runs object detection on an image. Parameters: Name Type Description Default img_arr np . ndarray An image array with dimensions (height, width, channels). required Returns: Type Description np . ndarray np.ndarray: An array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). np . ndarray np.ndarray: An array of object detection confidence scores of length (object) where object is the number of objects detected. Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def detect_image ( self , img_arr : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Runs object detection on an image. Args: img_arr (np.ndarray): An image array with dimensions (height, width, channels). Returns: np.ndarray: An array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). np.ndarray: An array of object detection confidence scores of length (object) where object is the number of objects detected. \"\"\" with torch . no_grad (): outputs = self . model ( torch . from_numpy ( self . _preprocess ( img_arr )) . unsqueeze ( 0 ) . to ( self . config . device ) ) output = postprocess ( outputs , self . num_classes , self . config . confidence , self . config . nms_threshold ) return self . _process_frame_output ( output [ 0 ], img_arr . shape [ 0 ], img_arr . shape [ 1 ]) detect_video ( video_arr : np . ndarray , pbar : bool = False ) \u00b6 Runs object detection on an video. Parameters: Name Type Description Default video_arr np . ndarray An video array with dimensions (frames, height, width, channels). required pbar int Whether to show progress bar. Defaults to False. False Returns: Name Type Description list A list containing detections and score for each frame. Each tuple contains two arrays: the first is an array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). The second is an array of object detection confidence scores of length (object) where object is the number of objects detected. Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def detect_video ( self , video_arr : np . ndarray , pbar : bool = False ): \"\"\"Runs object detection on an video. Args: video_arr (np.ndarray): An video array with dimensions (frames, height, width, channels). pbar (int): Whether to show progress bar. Defaults to False. Returns: list: A list containing detections and score for each frame. Each tuple contains two arrays: the first is an array of bounding box detections with dimensions (object, 4) where object is the number of objects detected and the other 4 dimension are (x1, y1, x2, y1). The second is an array of object detection confidence scores of length (object) where object is the number of objects detected. \"\"\" pbar = tqdm if pbar else lambda x : x # batch of frames batch_size = self . config . frame_batch_size video_outputs = [] with torch . no_grad (): for i in range ( 0 , len ( video_arr ), batch_size ): a = video_arr [ i : i + batch_size ] outputs = self . model ( torch . from_numpy ( self . _preprocess_video ( a )) . to ( self . config . device ) ) outputs = postprocess ( outputs , self . num_classes , self . config . confidence , self . config . nms_threshold ) video_outputs . extend ( outputs ) detections = [] for o in pbar ( video_outputs ): detections . append ( self . _process_frame_output ( o , original_height = video_arr . shape [ 1 ], original_width = video_arr . shape [ 2 ] ) ) return detections filter_frames ( frames : np . ndarray , detections : List [ Tuple [ float , float , float , float ]]) -> np . ndarray \u00b6 Filter video frames using megadetector lite. Which frames are returned depends on the fill_mode and how many frames are above the confidence threshold. If more than n_frames are above the threshold, the top n_frames are returned. Otherwise add to those over threshold based on fill_mode. If none of these conditions are met, returns all frames above the threshold. Parameters: Name Type Description Default frames np . ndarray Array of video frames to filter with dimensions (frames, height, width, channels) required detections list of tuples List of detection results for each frame. Each element is a tuple of the list of bounding boxes [array(x1, y1, x2, y2)] and the detection probabilities, both as float required Returns: Type Description np . ndarray np.ndarray: An array of video frames of length n_frames or shorter Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def filter_frames ( self , frames : np . ndarray , detections : List [ Tuple [ float , float , float , float ]] ) -> np . ndarray : \"\"\"Filter video frames using megadetector lite. Which frames are returned depends on the fill_mode and how many frames are above the confidence threshold. If more than n_frames are above the threshold, the top n_frames are returned. Otherwise add to those over threshold based on fill_mode. If none of these conditions are met, returns all frames above the threshold. Args: frames (np.ndarray): Array of video frames to filter with dimensions (frames, height, width, channels) detections (list of tuples): List of detection results for each frame. Each element is a tuple of the list of bounding boxes [array(x1, y1, x2, y2)] and the detection probabilities, both as float Returns: np.ndarray: An array of video frames of length n_frames or shorter \"\"\" frame_scores = pd . Series ( [( np . max ( score ) if ( len ( score ) > 0 ) else 0 ) for _ , score in detections ] ) . sort_values ( ascending = False ) # reduce to one score per frame selected_indices = frame_scores . loc [ frame_scores > self . config . confidence ] . index if self . config . n_frames is None : # no minimum n_frames provided, just select all the frames with scores > threshold pass elif len ( selected_indices ) >= self . config . n_frames : # num. frames with scores > threshold is greater than the requested number of frames selected_indices = ( frame_scores [ selected_indices ] . sort_values ( ascending = False ) . iloc [: self . config . n_frames ] . index ) elif len ( selected_indices ) < self . config . n_frames : # num. frames with scores > threshold is less than the requested number of frames # repeat frames that are above threshold to get to n_frames rng = np . random . RandomState ( self . config . seed ) if self . config . fill_mode == \"repeat\" : repeated_indices = rng . choice ( selected_indices , self . config . n_frames - len ( selected_indices ), replace = True , ) selected_indices = np . concatenate (( selected_indices , repeated_indices )) # take frames in sorted order up to n_frames, even if score is zero elif self . config . fill_mode == \"score_sorted\" : selected_indices = ( frame_scores . sort_values ( ascending = False ) . iloc [: self . config . n_frames ] . index ) # sample up to n_frames, prefer points closer to frames with detection elif self . config . fill_mode == \"weighted_euclidean\" : sample_from = frame_scores . loc [ ~ frame_scores . index . isin ( selected_indices )] . index # take one over euclidean distance to all points with detection weights = [ 1 / np . linalg . norm ( selected_indices - sample ) for sample in sample_from ] # normalize weights weights /= np . sum ( weights ) sampled = rng . choice ( sample_from , self . config . n_frames - len ( selected_indices ), replace = False , p = weights , ) selected_indices = np . concatenate (( selected_indices , sampled )) # sample up to n_frames, weight by predicted probability - only if some frames have nonzero prob elif ( self . config . fill_mode == \"weighted_prob\" ) and ( len ( selected_indices ) > 0 ): sample_from = frame_scores . loc [ ~ frame_scores . index . isin ( selected_indices )] . index weights = frame_scores [ sample_from ] / np . sum ( frame_scores [ sample_from ]) sampled = rng . choice ( sample_from , self . config . n_frames - len ( selected_indices ), replace = False , p = weights , ) selected_indices = np . concatenate (( selected_indices , sampled )) # sort the selected images back into their original order if self . config . sort_by_time : selected_indices = sorted ( selected_indices ) return frames [ selected_indices ] scale_and_pad_array ( image_array : np . ndarray , output_width : int , output_height : int ) -> np . ndarray staticmethod \u00b6 Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 109 110 111 112 113 114 115 116 117 118 119 120 121 @staticmethod def scale_and_pad_array ( image_array : np . ndarray , output_width : int , output_height : int ) -> np . ndarray : return np . array ( ImageOps . pad ( Image . fromarray ( image_array ), ( output_width , output_height ), method = Image . BICUBIC , color = None , centering = ( 0 , 0 ), ) ) MegadetectorLiteYoloXConfig \u00b6 Bases: BaseModel Configuration for a MegadetectorLiteYoloX frame selection model Attributes: Name Type Description confidence float Only consider object detections with this confidence or greater nms_threshold float Non-maximum suppression is a method for filtering many bounding boxes around the same object to a single bounding box. This is a constant that determines how much to suppress similar bounding boxes. image_width int Scale image to this width before sending to object detection model. image_height int Scale image to this height before sending to object detection model. device str Where to run the object detection model, \"cpu\" or \"cuda\". frame_batch_size int Number of frames to predict on at once. n_frames int Max number of frames to return. If None returns all frames above the threshold. Defaults to None. fill_mode str Mode for upsampling if the number of frames above the threshold is less than n_frames. Defaults to \"repeat\". sort_by_time bool Whether to sort the selected frames by time (original order) before returning. If False, returns frames sorted by score (descending). Defaults to True. seed int Random state for random number generator. Defaults to 55. Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class MegadetectorLiteYoloXConfig ( BaseModel ): \"\"\"Configuration for a MegadetectorLiteYoloX frame selection model Attributes: confidence (float): Only consider object detections with this confidence or greater nms_threshold (float): Non-maximum suppression is a method for filtering many bounding boxes around the same object to a single bounding box. This is a constant that determines how much to suppress similar bounding boxes. image_width (int): Scale image to this width before sending to object detection model. image_height (int): Scale image to this height before sending to object detection model. device (str): Where to run the object detection model, \"cpu\" or \"cuda\". frame_batch_size (int): Number of frames to predict on at once. n_frames (int, optional): Max number of frames to return. If None returns all frames above the threshold. Defaults to None. fill_mode (str, optional): Mode for upsampling if the number of frames above the threshold is less than n_frames. Defaults to \"repeat\". sort_by_time (bool, optional): Whether to sort the selected frames by time (original order) before returning. If False, returns frames sorted by score (descending). Defaults to True. seed (int, optional): Random state for random number generator. Defaults to 55. \"\"\" confidence : float = 0.25 nms_threshold : float = 0.45 image_width : int = 640 image_height : int = 640 device : str = \"cuda\" if torch . cuda . is_available () else \"cpu\" frame_batch_size : int = 24 n_frames : Optional [ int ] = None fill_mode : Optional [ FillModeEnum ] = FillModeEnum . score_sorted sort_by_time : bool = True seed : Optional [ int ] = 55 class Config : extra = \"forbid\" Attributes \u00b6 confidence : float = 0.25 class-attribute \u00b6 device : str = 'cuda' if torch . cuda . is_available () else 'cpu' class-attribute \u00b6 fill_mode : Optional [ FillModeEnum ] = FillModeEnum . score_sorted class-attribute \u00b6 frame_batch_size : int = 24 class-attribute \u00b6 image_height : int = 640 class-attribute \u00b6 image_width : int = 640 class-attribute \u00b6 n_frames : Optional [ int ] = None class-attribute \u00b6 nms_threshold : float = 0.45 class-attribute \u00b6 seed : Optional [ int ] = 55 class-attribute \u00b6 sort_by_time : bool = True class-attribute \u00b6 Classes \u00b6 Config \u00b6 Source code in zamba/object_detection/yolox/megadetector_lite_yolox.py 72 73 class Config : extra = \"forbid\" Attributes \u00b6 extra = 'forbid' class-attribute \u00b6","title":"zamba.object_detection.yolox.megadetector_lite_yolox"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zambaobject_detectionyoloxmegadetector_lite_yolox","text":"","title":"zamba.object_detection.yolox.megadetector_lite_yolox"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox-attributes","text":"","title":"Attributes"},{"location":"api-reference/object-detection-megadetector_lite_yolox/#zamba.object_detection.yolox.megadetector_lite_yolox-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch-dataloaders/","text":"zamba.pytorch.dataloaders \u00b6 Classes \u00b6 FfmpegZambaVideoDataset \u00b6 Bases: VisionDataset Source code in zamba/pytorch/dataloaders.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 class FfmpegZambaVideoDataset ( VisionDataset ): def __init__ ( self , annotations : pd . DataFrame , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ VideoLoaderConfig ] = None , ): self . original_indices = annotations . index self . video_paths = annotations . index . tolist () self . species = [ s . split ( \"species_\" , 1 )[ 1 ] for s in annotations . columns ] self . targets = annotations self . transform = transform # get environment variable for cache if it exists if video_loader_config is None : video_loader_config = VideoLoaderConfig () self . video_loader_config = video_loader_config super () . __init__ ( root = None , transform = transform ) def __len__ ( self ): return len ( self . video_paths ) def __getitem__ ( self , index : int ): try : cached_load_video_frames = npy_cache ( cache_path = self . video_loader_config . cache_dir , cleanup = self . video_loader_config . cleanup_cache , )( load_video_frames ) video = cached_load_video_frames ( filepath = self . video_paths [ index ], config = self . video_loader_config ) except Exception as e : if isinstance ( e , IndexError ): raise # show ffmpeg error logger . debug ( e ) logger . warning ( f \"Video { self . video_paths [ index ] } could not be loaded. Using an array of all zeros instead.\" ) video = np . zeros ( ( self . video_loader_config . total_frames , self . video_loader_config . model_input_height if self . video_loader_config . model_input_height is not None else self . video_loader_config . frame_selection_height , self . video_loader_config . model_input_width if self . video_loader_config . model_input_width is not None else self . video_loader_config . frame_selection_width , 3 , ), dtype = \"int\" , ) # ignore pytorch warning about non-writeable tensors with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , UserWarning ) video = torch . from_numpy ( video ) if self . transform is not None : video = self . transform ( video ) target = self . targets . iloc [ index ] target = torch . tensor ( target ) . float () return video , target Attributes \u00b6 original_indices = annotations . index instance-attribute \u00b6 species = [ s . split ( 'species_' , 1 )[ 1 ] for s in annotations . columns ] instance-attribute \u00b6 targets = annotations instance-attribute \u00b6 transform = transform instance-attribute \u00b6 video_loader_config = video_loader_config instance-attribute \u00b6 video_paths = annotations . index . tolist () instance-attribute \u00b6 Functions \u00b6 __init__ ( annotations : pd . DataFrame , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ VideoLoaderConfig ] = None ) \u00b6 Source code in zamba/pytorch/dataloaders.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , annotations : pd . DataFrame , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ VideoLoaderConfig ] = None , ): self . original_indices = annotations . index self . video_paths = annotations . index . tolist () self . species = [ s . split ( \"species_\" , 1 )[ 1 ] for s in annotations . columns ] self . targets = annotations self . transform = transform # get environment variable for cache if it exists if video_loader_config is None : video_loader_config = VideoLoaderConfig () self . video_loader_config = video_loader_config super () . __init__ ( root = None , transform = transform ) Functions \u00b6 get_datasets ( train_metadata : Optional [ pd . DataFrame ] = None , predict_metadata : Optional [ pd . DataFrame ] = None , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ VideoLoaderConfig ] = None ) -> Tuple [ Optional [ FfmpegZambaVideoDataset ], Optional [ FfmpegZambaVideoDataset ], Optional [ FfmpegZambaVideoDataset ], Optional [ FfmpegZambaVideoDataset ]] \u00b6 Gets training and/or prediction datasets. Parameters: Name Type Description Default train_metadata pathlike Path to a CSV or DataFrame with columns: - filepath: path to a video, relative to video_dir - label:, label of the species that appears in the video - split (optional): If provided, \"train\", \"val\", or \"holdout\" indicating which dataset split the video will be included in. If not provided, and a \"site\" column exists, generate a site-specific split. Otherwise, generate a random split using split_proportions . - site (optional): If no \"split\" column, generate a site-specific split using the values in this column. None predict_metadata pathlike Path to a CSV or DataFrame with a \"filepath\" column. None Returns: Type Description Optional [ FfmpegZambaVideoDataset ] A tuple of (train_dataset, val_dataset, test_dataset, predict_dataset) where each dataset Optional [ FfmpegZambaVideoDataset ] can be None if not specified. Source code in zamba/pytorch/dataloaders.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_datasets ( train_metadata : Optional [ pd . DataFrame ] = None , predict_metadata : Optional [ pd . DataFrame ] = None , transform : Optional [ torchvision . transforms . transforms . Compose ] = None , video_loader_config : Optional [ VideoLoaderConfig ] = None , ) -> Tuple [ Optional [ \"FfmpegZambaVideoDataset\" ], Optional [ \"FfmpegZambaVideoDataset\" ], Optional [ \"FfmpegZambaVideoDataset\" ], Optional [ \"FfmpegZambaVideoDataset\" ], ]: \"\"\"Gets training and/or prediction datasets. Args: train_metadata (pathlike, optional): Path to a CSV or DataFrame with columns: - filepath: path to a video, relative to `video_dir` - label:, label of the species that appears in the video - split (optional): If provided, \"train\", \"val\", or \"holdout\" indicating which dataset split the video will be included in. If not provided, and a \"site\" column exists, generate a site-specific split. Otherwise, generate a random split using `split_proportions`. - site (optional): If no \"split\" column, generate a site-specific split using the values in this column. predict_metadata (pathlike, optional): Path to a CSV or DataFrame with a \"filepath\" column. transform (torchvision.transforms.transforms.Compose, optional) video_loader_config (VideoLoaderConfig, optional) Returns: A tuple of (train_dataset, val_dataset, test_dataset, predict_dataset) where each dataset can be None if not specified. \"\"\" if predict_metadata is not None : # enable filtering the same way on all datasets predict_metadata [ \"species_\" ] = 0 def subset_metadata_or_none ( metadata : Optional [ pd . DataFrame ] = None , subset : Optional [ str ] = None ) -> Optional [ pd . DataFrame ]: if metadata is None : return None else : metadata_subset = metadata . loc [ metadata . split == subset ] if subset else metadata if len ( metadata_subset ) > 0 : return FfmpegZambaVideoDataset ( annotations = metadata_subset . set_index ( \"filepath\" ) . filter ( regex = \"species\" ), transform = transform , video_loader_config = video_loader_config , ) else : return None train_dataset = subset_metadata_or_none ( train_metadata , \"train\" ) val_dataset = subset_metadata_or_none ( train_metadata , \"val\" ) test_dataset = subset_metadata_or_none ( train_metadata , \"holdout\" ) predict_dataset = subset_metadata_or_none ( predict_metadata ) return train_dataset , val_dataset , test_dataset , predict_dataset","title":"zamba.pytorch.dataloaders"},{"location":"api-reference/pytorch-dataloaders/#zambapytorchdataloaders","text":"","title":"zamba.pytorch.dataloaders"},{"location":"api-reference/pytorch-dataloaders/#zamba.pytorch.dataloaders-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch-dataloaders/#zamba.pytorch.dataloaders-functions","text":"","title":"Functions"},{"location":"api-reference/pytorch-finetuning/","text":"zamba.pytorch.finetuning \u00b6 Classes \u00b6 BackboneFinetuning \u00b6 Bases: pl . callbacks . finetuning . BackboneFinetuning Derived from PTL's built-in BackboneFinetuning , but during the backbone freeze phase, choose whether to freeze batch norm layers, even if train_bn is True (i.e., even if we train them during the backbone unfreeze phase). Finetune a backbone model based on a learning rate user-defined scheduling. When the backbone learning rate reaches the current model learning rate and should_align is set to True, it will align with it for the rest of the training. Parameters: Name Type Description Default unfreeze_backbone_at_epoch Epoch at which the backbone will be unfreezed. required lambda_func Scheduling function for increasing backbone learning rate. required backbone_initial_ratio_lr Used to scale down the backbone learning rate compared to rest of model required backbone_initial_lr Optional, Inital learning rate for the backbone. By default, we will use current_learning / backbone_initial_ratio_lr required should_align Wheter to align with current learning rate when backbone learning reaches it. required initial_denom_lr When unfreezing the backbone, the intial learning rate will current_learning_rate / initial_denom_lr. required train_bn Wheter to make Batch Normalization trainable. required verbose Display current learning rate for model and backbone required round Precision for displaying learning rate required Example:: >>> from pytorch_lightning import Trainer >>> from pytorch_lightning.callbacks import BackboneFinetuning >>> multiplicative = lambda epoch: 1.5 >>> backbone_finetuning = BackboneFinetuning(200, multiplicative) >>> trainer = Trainer(callbacks=[backbone_finetuning]) Source code in zamba/pytorch/finetuning.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class BackboneFinetuning ( pl . callbacks . finetuning . BackboneFinetuning ): r \"\"\" Derived from PTL's built-in ``BackboneFinetuning``, but during the backbone freeze phase, choose whether to freeze batch norm layers, even if ``train_bn`` is True (i.e., even if we train them during the backbone unfreeze phase). Finetune a backbone model based on a learning rate user-defined scheduling. When the backbone learning rate reaches the current model learning rate and ``should_align`` is set to True, it will align with it for the rest of the training. Args: unfreeze_backbone_at_epoch: Epoch at which the backbone will be unfreezed. lambda_func: Scheduling function for increasing backbone learning rate. backbone_initial_ratio_lr: Used to scale down the backbone learning rate compared to rest of model backbone_initial_lr: Optional, Inital learning rate for the backbone. By default, we will use current_learning / backbone_initial_ratio_lr should_align: Wheter to align with current learning rate when backbone learning reaches it. initial_denom_lr: When unfreezing the backbone, the intial learning rate will current_learning_rate / initial_denom_lr. train_bn: Wheter to make Batch Normalization trainable. verbose: Display current learning rate for model and backbone round: Precision for displaying learning rate Example:: >>> from pytorch_lightning import Trainer >>> from pytorch_lightning.callbacks import BackboneFinetuning >>> multiplicative = lambda epoch: 1.5 >>> backbone_finetuning = BackboneFinetuning(200, multiplicative) >>> trainer = Trainer(callbacks=[backbone_finetuning]) \"\"\" def __init__ ( self , * args , multiplier : Optional [ float ] = 1 , pre_train_bn : bool = False , ** kwargs ): if multiplier is not None : kwargs [ \"lambda_func\" ] = multiplier_factory ( multiplier ) super () . __init__ ( * args , ** kwargs ) # choose whether to train batch norm layers prior to finetuning phase self . pre_train_bn = pre_train_bn def freeze_before_training ( self , pl_module : \"pl.LightningModule\" ): self . freeze ( pl_module . backbone , train_bn = self . pre_train_bn ) Attributes \u00b6 pre_train_bn = pre_train_bn instance-attribute \u00b6 Functions \u00b6 __init__ ( * args , multiplier : Optional [ float ] = 1 , pre_train_bn : bool = False , ** kwargs ) \u00b6 Source code in zamba/pytorch/finetuning.py 64 65 66 67 68 69 70 71 def __init__ ( self , * args , multiplier : Optional [ float ] = 1 , pre_train_bn : bool = False , ** kwargs ): if multiplier is not None : kwargs [ \"lambda_func\" ] = multiplier_factory ( multiplier ) super () . __init__ ( * args , ** kwargs ) # choose whether to train batch norm layers prior to finetuning phase self . pre_train_bn = pre_train_bn freeze_before_training ( pl_module : pl . LightningModule ) \u00b6 Source code in zamba/pytorch/finetuning.py 73 74 def freeze_before_training ( self , pl_module : \"pl.LightningModule\" ): self . freeze ( pl_module . backbone , train_bn = self . pre_train_bn ) Functions \u00b6 multiplier_factory ( rate : float ) \u00b6 Returns a function that returns a constant value for use in computing a constant learning rate multiplier. Parameters: Name Type Description Default rate float Constant multiplier. required Source code in zamba/pytorch/finetuning.py 5 6 7 8 9 10 11 12 13 14 15 16 def multiplier_factory ( rate : float ): \"\"\"Returns a function that returns a constant value for use in computing a constant learning rate multiplier. Args: rate (float): Constant multiplier. \"\"\" def multiplier ( * args , ** kwargs ): return rate return multiplier","title":"zamba.pytorch.finetuning"},{"location":"api-reference/pytorch-finetuning/#zambapytorchfinetuning","text":"","title":"zamba.pytorch.finetuning"},{"location":"api-reference/pytorch-finetuning/#zamba.pytorch.finetuning-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch-finetuning/#zamba.pytorch.finetuning-functions","text":"","title":"Functions"},{"location":"api-reference/pytorch-layers/","text":"zamba.pytorch.layers \u00b6 Classes \u00b6 TimeDistributed \u00b6 Bases: torch . nn . Module Applies module over tdim identically for each step, use low_mem to compute one at a time. NOTE: vendored (with minor adaptations) from fastai: https://github.com/fastai/fastai/blob/4b0785254fdece1a44859956b6e54eedb167a97e/fastai/layers.py#L510-L544 Updates super. init () in init assign attributes in init inherit from torch.nn.Module rather than fastai.Module Source code in zamba/pytorch/layers.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class TimeDistributed ( torch . nn . Module ): \"\"\"Applies `module` over `tdim` identically for each step, use `low_mem` to compute one at a time. NOTE: vendored (with minor adaptations) from fastai: https://github.com/fastai/fastai/blob/4b0785254fdece1a44859956b6e54eedb167a97e/fastai/layers.py#L510-L544 Updates: - super.__init__() in init - assign attributes in init - inherit from torch.nn.Module rather than fastai.Module \"\"\" def __init__ ( self , module , low_mem = False , tdim = 1 ): super () . __init__ () self . low_mem = low_mem self . tdim = tdim self . module = module def forward ( self , * tensors , ** kwargs ): \"input x with shape:(bs,seq_len,channels,width,height)\" if self . low_mem or self . tdim != 1 : return self . low_mem_forward ( * tensors , ** kwargs ) else : # only support tdim=1 inp_shape = tensors [ 0 ] . shape bs , seq_len = inp_shape [ 0 ], inp_shape [ 1 ] out = self . module ( * [ x . view ( bs * seq_len , * x . shape [ 2 :]) for x in tensors ], ** kwargs ) return self . format_output ( out , bs , seq_len ) def low_mem_forward ( self , * tensors , ** kwargs ): \"input x with shape:(bs,seq_len,channels,width,height)\" seq_len = tensors [ 0 ] . shape [ self . tdim ] args_split = [ torch . unbind ( x , dim = self . tdim ) for x in tensors ] out = [] for i in range ( seq_len ): out . append ( self . module ( * [ args [ i ] for args in args_split ]), ** kwargs ) if isinstance ( out [ 0 ], tuple ): return _stack_tups ( out , stack_dim = self . tdim ) return torch . stack ( out , dim = self . tdim ) def format_output ( self , out , bs , seq_len ): \"unstack from batchsize outputs\" if isinstance ( out , tuple ): return tuple ( out_i . view ( bs , seq_len , * out_i . shape [ 1 :]) for out_i in out ) return out . view ( bs , seq_len , * out . shape [ 1 :]) def __repr__ ( self ): return f \"TimeDistributed( { self . module } )\" Attributes \u00b6 low_mem = low_mem instance-attribute \u00b6 module = module instance-attribute \u00b6 tdim = tdim instance-attribute \u00b6 Functions \u00b6 __init__ ( module , low_mem = False , tdim = 1 ) \u00b6 Source code in zamba/pytorch/layers.py 28 29 30 31 32 def __init__ ( self , module , low_mem = False , tdim = 1 ): super () . __init__ () self . low_mem = low_mem self . tdim = tdim self . module = module format_output ( out , bs , seq_len ) \u00b6 unstack from batchsize outputs Source code in zamba/pytorch/layers.py 56 57 58 59 60 def format_output ( self , out , bs , seq_len ): \"unstack from batchsize outputs\" if isinstance ( out , tuple ): return tuple ( out_i . view ( bs , seq_len , * out_i . shape [ 1 :]) for out_i in out ) return out . view ( bs , seq_len , * out . shape [ 1 :]) forward ( * tensors , ** kwargs ) \u00b6 input x with shape:(bs,seq_len,channels,width,height) Source code in zamba/pytorch/layers.py 34 35 36 37 38 39 40 41 42 43 def forward ( self , * tensors , ** kwargs ): \"input x with shape:(bs,seq_len,channels,width,height)\" if self . low_mem or self . tdim != 1 : return self . low_mem_forward ( * tensors , ** kwargs ) else : # only support tdim=1 inp_shape = tensors [ 0 ] . shape bs , seq_len = inp_shape [ 0 ], inp_shape [ 1 ] out = self . module ( * [ x . view ( bs * seq_len , * x . shape [ 2 :]) for x in tensors ], ** kwargs ) return self . format_output ( out , bs , seq_len ) low_mem_forward ( * tensors , ** kwargs ) \u00b6 input x with shape:(bs,seq_len,channels,width,height) Source code in zamba/pytorch/layers.py 45 46 47 48 49 50 51 52 53 54 def low_mem_forward ( self , * tensors , ** kwargs ): \"input x with shape:(bs,seq_len,channels,width,height)\" seq_len = tensors [ 0 ] . shape [ self . tdim ] args_split = [ torch . unbind ( x , dim = self . tdim ) for x in tensors ] out = [] for i in range ( seq_len ): out . append ( self . module ( * [ args [ i ] for args in args_split ]), ** kwargs ) if isinstance ( out [ 0 ], tuple ): return _stack_tups ( out , stack_dim = self . tdim ) return torch . stack ( out , dim = self . tdim )","title":"zamba.pytorch.layers"},{"location":"api-reference/pytorch-layers/#zambapytorchlayers","text":"","title":"zamba.pytorch.layers"},{"location":"api-reference/pytorch-layers/#zamba.pytorch.layers-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch-transforms/","text":"zamba.pytorch.transforms \u00b6 Attributes \u00b6 imagenet_normalization_values = dict ( mean = [ 0.485 , 0.456 , 0.406 ], std = [ 0.229 , 0.224 , 0.225 ]) module-attribute \u00b6 Classes \u00b6 ConvertTCHWtoCTHW \u00b6 Bases: torch . nn . Module Convert tensor from (T, C, H, W) to (C, T, H, W) Source code in zamba/pytorch/transforms.py 23 24 25 26 27 class ConvertTCHWtoCTHW ( torch . nn . Module ): \"\"\"Convert tensor from (T, C, H, W) to (C, T, H, W)\"\"\" def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 1 , 0 , 2 , 3 ) Functions \u00b6 forward ( vid : torch . Tensor ) -> torch . Tensor \u00b6 Source code in zamba/pytorch/transforms.py 26 27 def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 1 , 0 , 2 , 3 ) ConvertTHWCtoCTHW \u00b6 Bases: torch . nn . Module Convert tensor from (0:T, 1:H, 2:W, 3:C) to (3:C, 0:T, 1:H, 2:W) Source code in zamba/pytorch/transforms.py 9 10 11 12 13 class ConvertTHWCtoCTHW ( torch . nn . Module ): \"\"\"Convert tensor from (0:T, 1:H, 2:W, 3:C) to (3:C, 0:T, 1:H, 2:W)\"\"\" def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 3 , 0 , 1 , 2 ) Functions \u00b6 forward ( vid : torch . Tensor ) -> torch . Tensor \u00b6 Source code in zamba/pytorch/transforms.py 12 13 def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 3 , 0 , 1 , 2 ) ConvertTHWCtoTCHW \u00b6 Bases: torch . nn . Module Convert tensor from (T, H, W, C) to (T, C, H, W) Source code in zamba/pytorch/transforms.py 16 17 18 19 20 class ConvertTHWCtoTCHW ( torch . nn . Module ): \"\"\"Convert tensor from (T, H, W, C) to (T, C, H, W)\"\"\" def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 0 , 3 , 1 , 2 ) Functions \u00b6 forward ( vid : torch . Tensor ) -> torch . Tensor \u00b6 Source code in zamba/pytorch/transforms.py 19 20 def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . permute ( 0 , 3 , 1 , 2 ) PackSlowFastPathways \u00b6 Bases: torch . nn . Module Creates the slow and fast pathway inputs for the slowfast model. Source code in zamba/pytorch/transforms.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 class PackSlowFastPathways ( torch . nn . Module ): \"\"\"Creates the slow and fast pathway inputs for the slowfast model.\"\"\" def __init__ ( self , alpha : int = 4 ): super () . __init__ () self . alpha = alpha def forward ( self , frames : torch . Tensor ): fast_pathway = frames # Perform temporal sampling from the fast pathway. slow_pathway = torch . index_select ( frames , 1 , torch . linspace ( 0 , frames . shape [ 1 ] - 1 , frames . shape [ 1 ] // self . alpha ) . long (), ) frame_list = [ slow_pathway , fast_pathway ] return frame_list Attributes \u00b6 alpha = alpha instance-attribute \u00b6 Functions \u00b6 __init__ ( alpha : int = 4 ) \u00b6 Source code in zamba/pytorch/transforms.py 84 85 86 def __init__ ( self , alpha : int = 4 ): super () . __init__ () self . alpha = alpha forward ( frames : torch . Tensor ) \u00b6 Source code in zamba/pytorch/transforms.py 88 89 90 91 92 93 94 95 96 97 def forward ( self , frames : torch . Tensor ): fast_pathway = frames # Perform temporal sampling from the fast pathway. slow_pathway = torch . index_select ( frames , 1 , torch . linspace ( 0 , frames . shape [ 1 ] - 1 , frames . shape [ 1 ] // self . alpha ) . long (), ) frame_list = [ slow_pathway , fast_pathway ] return frame_list PadDimensions \u00b6 Bases: torch . nn . Module Pads a tensor to ensure a fixed output dimension for a give axis. Attributes: Name Type Description dimension_sizes A tuple of int or None the same length as the number of dimensions in the input tensor. If int, pad that dimension to at least that size. If None, do not pad. Source code in zamba/pytorch/transforms.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class PadDimensions ( torch . nn . Module ): \"\"\"Pads a tensor to ensure a fixed output dimension for a give axis. Attributes: dimension_sizes: A tuple of int or None the same length as the number of dimensions in the input tensor. If int, pad that dimension to at least that size. If None, do not pad. \"\"\" def __init__ ( self , dimension_sizes : Tuple [ Optional [ int ]]): super () . __init__ () self . dimension_sizes = dimension_sizes @staticmethod def compute_left_and_right_pad ( original_size : int , padded_size : int ) -> Tuple [ int , int ]: \"\"\"Computes left and right pad size. Args: original_size (list, int): The original tensor size padded_size (list, int): The desired tensor size Returns: Tuple[int]: Pad size for right and left. For odd padding size, the right = left + 1 \"\"\" if original_size >= padded_size : return 0 , 0 pad = padded_size - original_size quotient , remainder = divmod ( pad , 2 ) return quotient , quotient + remainder def forward ( self , vid : torch . Tensor ) -> torch . Tensor : padding = tuple ( itertools . chain . from_iterable ( ( 0 , 0 ) if padded_size is None else self . compute_left_and_right_pad ( original_size , padded_size ) for original_size , padded_size in zip ( vid . shape , self . dimension_sizes ) ) ) return torch . nn . functional . pad ( vid , padding [:: - 1 ]) Attributes \u00b6 dimension_sizes = dimension_sizes instance-attribute \u00b6 Functions \u00b6 __init__ ( dimension_sizes : Tuple [ Optional [ int ]]) \u00b6 Source code in zamba/pytorch/transforms.py 48 49 50 def __init__ ( self , dimension_sizes : Tuple [ Optional [ int ]]): super () . __init__ () self . dimension_sizes = dimension_sizes compute_left_and_right_pad ( original_size : int , padded_size : int ) -> Tuple [ int , int ] staticmethod \u00b6 Computes left and right pad size. Parameters: Name Type Description Default original_size list , int The original tensor size required padded_size list , int The desired tensor size required Returns: Type Description Tuple [ int , int ] Tuple[int]: Pad size for right and left. For odd padding size, the right = left + 1 Source code in zamba/pytorch/transforms.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @staticmethod def compute_left_and_right_pad ( original_size : int , padded_size : int ) -> Tuple [ int , int ]: \"\"\"Computes left and right pad size. Args: original_size (list, int): The original tensor size padded_size (list, int): The desired tensor size Returns: Tuple[int]: Pad size for right and left. For odd padding size, the right = left + 1 \"\"\" if original_size >= padded_size : return 0 , 0 pad = padded_size - original_size quotient , remainder = divmod ( pad , 2 ) return quotient , quotient + remainder forward ( vid : torch . Tensor ) -> torch . Tensor \u00b6 Source code in zamba/pytorch/transforms.py 69 70 71 72 73 74 75 76 77 78 def forward ( self , vid : torch . Tensor ) -> torch . Tensor : padding = tuple ( itertools . chain . from_iterable ( ( 0 , 0 ) if padded_size is None else self . compute_left_and_right_pad ( original_size , padded_size ) for original_size , padded_size in zip ( vid . shape , self . dimension_sizes ) ) ) return torch . nn . functional . pad ( vid , padding [:: - 1 ]) Uint8ToFloat \u00b6 Bases: torch . nn . Module Source code in zamba/pytorch/transforms.py 30 31 32 class Uint8ToFloat ( torch . nn . Module ): def forward ( self , tensor : torch . Tensor ) -> torch . Tensor : return tensor / 255.0 Functions \u00b6 forward ( tensor : torch . Tensor ) -> torch . Tensor \u00b6 Source code in zamba/pytorch/transforms.py 31 32 def forward ( self , tensor : torch . Tensor ) -> torch . Tensor : return tensor / 255.0 VideotoImg \u00b6 Bases: torch . nn . Module Source code in zamba/pytorch/transforms.py 35 36 37 class VideotoImg ( torch . nn . Module ): def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . squeeze ( 0 ) Functions \u00b6 forward ( vid : torch . Tensor ) -> torch . Tensor \u00b6 Source code in zamba/pytorch/transforms.py 36 37 def forward ( self , vid : torch . Tensor ) -> torch . Tensor : return vid . squeeze ( 0 ) Functions \u00b6 slowfast_transforms () \u00b6 Source code in zamba/pytorch/transforms.py 121 122 123 124 125 126 127 128 129 130 131 def slowfast_transforms (): return transforms . Compose ( [ ConvertTHWCtoTCHW (), Uint8ToFloat (), Normalize ( mean = [ 0.45 , 0.45 , 0.45 ], std = [ 0.225 , 0.225 , 0.225 ]), ConvertTCHWtoCTHW (), PadDimensions (( None , 32 , None , None )), PackSlowFastPathways (), ] ) zamba_image_model_transforms ( single_frame = False , normalization_values = imagenet_normalization_values , channels_first = False ) \u00b6 Source code in zamba/pytorch/transforms.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def zamba_image_model_transforms ( single_frame = False , normalization_values = imagenet_normalization_values , channels_first = False ): img_transforms = [ ConvertTHWCtoTCHW (), Uint8ToFloat (), transforms . Normalize ( ** imagenet_normalization_values ), ] if single_frame : img_transforms += [ VideotoImg ()] # squeeze dim if channels_first : img_transforms += [ ConvertTCHWtoCTHW ()] return transforms . Compose ( img_transforms )","title":"zamba.pytorch.transforms"},{"location":"api-reference/pytorch-transforms/#zambapytorchtransforms","text":"","title":"zamba.pytorch.transforms"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms-attributes","text":"","title":"Attributes"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch-transforms/#zamba.pytorch.transforms-functions","text":"","title":"Functions"},{"location":"api-reference/pytorch-utils/","text":"zamba.pytorch.utils \u00b6 Functions \u00b6 build_multilayer_perceptron ( input_size : int , hidden_layer_sizes : Optional [ Tuple [ int ]], output_size : int , activation : Optional [ torch . nn . Module ] = torch . nn . ReLU , dropout : Optional [ float ] = None , output_dropout : Optional [ float ] = None , output_activation : Optional [ torch . nn . Module ] = None ) -> torch . nn . Sequential \u00b6 Builds a multilayer perceptron. Parameters: Name Type Description Default input_size int Size of first input layer. required hidden_layer_sizes tuple of int If provided, size of hidden layers. required output_size int Size of the last output layer. required activation torch . nn . Module Activation layer between each pair of layers. torch.nn.ReLU dropout float If provided, insert dropout layers with the following dropout rate in between each pair of layers. None output_dropout float If provided, insert a dropout layer with the following dropout rate before the output. None output_activation torch . nn . Module Activation layer after the final layer. None Returns: Type Description torch . nn . Sequential torch.nn.Sequential Source code in zamba/pytorch/utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def build_multilayer_perceptron ( input_size : int , hidden_layer_sizes : Optional [ Tuple [ int ]], output_size : int , activation : Optional [ torch . nn . Module ] = torch . nn . ReLU , dropout : Optional [ float ] = None , output_dropout : Optional [ float ] = None , output_activation : Optional [ torch . nn . Module ] = None , ) -> torch . nn . Sequential : \"\"\"Builds a multilayer perceptron. Args: input_size (int): Size of first input layer. hidden_layer_sizes (tuple of int, optional): If provided, size of hidden layers. output_size (int): Size of the last output layer. activation (torch.nn.Module, optional): Activation layer between each pair of layers. dropout (float, optional): If provided, insert dropout layers with the following dropout rate in between each pair of layers. output_dropout (float, optional): If provided, insert a dropout layer with the following dropout rate before the output. output_activation (torch.nn.Module, optional): Activation layer after the final layer. Returns: torch.nn.Sequential \"\"\" if ( hidden_layer_sizes is None ) or len ( hidden_layer_sizes ) == 0 : return torch . nn . Linear ( input_size , output_size ) layers = [ torch . nn . Linear ( input_size , hidden_layer_sizes [ 0 ])] if activation is not None : layers . append ( activation ()) if ( dropout is not None ) and ( dropout > 0 ): layers . append ( torch . nn . Dropout ( dropout )) for in_size , out_size in zip ( hidden_layer_sizes [: - 1 ], hidden_layer_sizes [ 1 :]): layers . append ( torch . nn . Linear ( in_size , out_size )) if activation is not None : layers . append ( activation ()) if ( dropout is not None ) and ( dropout > 0 ): layers . append ( torch . nn . Dropout ( dropout )) layers . append ( torch . nn . Linear ( hidden_layer_sizes [ - 1 ], output_size )) if ( output_dropout is not None ) and ( output_dropout > 0 ): layers . append ( torch . nn . Dropout ( dropout )) if output_activation is not None : layers . append ( output_activation ()) return torch . nn . Sequential ( * layers )","title":"zamba.pytorch.utils"},{"location":"api-reference/pytorch-utils/#zambapytorchutils","text":"","title":"zamba.pytorch.utils"},{"location":"api-reference/pytorch-utils/#zamba.pytorch.utils-functions","text":"","title":"Functions"},{"location":"api-reference/pytorch_lightning-utils/","text":"zamba.pytorch_lightning.utils \u00b6 Attributes \u00b6 DEFAULT_TOP_K = ( 1 , 3 , 5 , 10 ) module-attribute \u00b6 default_transform = transforms . Compose ([ ConvertTHWCtoCTHW (), transforms . ConvertImageDtype ( torch . float32 )]) module-attribute \u00b6 Classes \u00b6 ZambaDataModule \u00b6 Bases: LightningDataModule Source code in zamba/pytorch_lightning/utils.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class ZambaDataModule ( LightningDataModule ): def __init__ ( self , batch_size : int = 1 , num_workers : int = max ( cpu_count () - 1 , 1 ), transform : transforms . Compose = default_transform , video_loader_config : Optional [ VideoLoaderConfig ] = None , prefetch_factor : int = 2 , train_metadata : Optional [ pd . DataFrame ] = None , predict_metadata : Optional [ pd . DataFrame ] = None , multiprocessing_context : Optional [ str ] = \"forkserver\" , * args , ** kwargs , ): self . batch_size = batch_size self . num_workers = num_workers # Number of parallel processes fetching data self . prefetch_factor = prefetch_factor self . video_loader_config = ( None if video_loader_config is None else video_loader_config . dict () ) self . train_metadata = train_metadata self . predict_metadata = predict_metadata ( self . train_dataset , self . val_dataset , self . test_dataset , self . predict_dataset , ) = get_datasets ( train_metadata = train_metadata , predict_metadata = predict_metadata , transform = transform , video_loader_config = video_loader_config , ) self . multiprocessing_context : BaseContext = ( None if ( multiprocessing_context is None ) or ( num_workers == 0 ) else multiprocessing_context ) super () . __init__ ( * args , ** kwargs ) def train_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . train_dataset : return torch . utils . data . DataLoader ( self . train_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = True , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) def val_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . val_dataset : return torch . utils . data . DataLoader ( self . val_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) def test_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . test_dataset : return torch . utils . data . DataLoader ( self . test_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) def predict_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . predict_dataset : return torch . utils . data . DataLoader ( self . predict_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = True , ) Attributes \u00b6 batch_size = batch_size instance-attribute \u00b6 multiprocessing_context : BaseContext = None if multiprocessing_context is None or num_workers == 0 else multiprocessing_context instance-attribute \u00b6 num_workers = num_workers instance-attribute \u00b6 predict_metadata = predict_metadata instance-attribute \u00b6 prefetch_factor = prefetch_factor instance-attribute \u00b6 train_metadata = train_metadata instance-attribute \u00b6 video_loader_config = None if video_loader_config is None else video_loader_config . dict () instance-attribute \u00b6 Functions \u00b6 __init__ ( batch_size : int = 1 , num_workers : int = max ( cpu_count () - 1 , 1 ), transform : transforms . Compose = default_transform , video_loader_config : Optional [ VideoLoaderConfig ] = None , prefetch_factor : int = 2 , train_metadata : Optional [ pd . DataFrame ] = None , predict_metadata : Optional [ pd . DataFrame ] = None , multiprocessing_context : Optional [ str ] = 'forkserver' , * args , ** kwargs ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , batch_size : int = 1 , num_workers : int = max ( cpu_count () - 1 , 1 ), transform : transforms . Compose = default_transform , video_loader_config : Optional [ VideoLoaderConfig ] = None , prefetch_factor : int = 2 , train_metadata : Optional [ pd . DataFrame ] = None , predict_metadata : Optional [ pd . DataFrame ] = None , multiprocessing_context : Optional [ str ] = \"forkserver\" , * args , ** kwargs , ): self . batch_size = batch_size self . num_workers = num_workers # Number of parallel processes fetching data self . prefetch_factor = prefetch_factor self . video_loader_config = ( None if video_loader_config is None else video_loader_config . dict () ) self . train_metadata = train_metadata self . predict_metadata = predict_metadata ( self . train_dataset , self . val_dataset , self . test_dataset , self . predict_dataset , ) = get_datasets ( train_metadata = train_metadata , predict_metadata = predict_metadata , transform = transform , video_loader_config = video_loader_config , ) self . multiprocessing_context : BaseContext = ( None if ( multiprocessing_context is None ) or ( num_workers == 0 ) else multiprocessing_context ) super () . __init__ ( * args , ** kwargs ) predict_dataloader () -> Optional [ torch . utils . data . DataLoader ] \u00b6 Source code in zamba/pytorch_lightning/utils.py 111 112 113 114 115 116 117 118 119 120 121 def predict_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . predict_dataset : return torch . utils . data . DataLoader ( self . predict_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = True , ) test_dataloader () -> Optional [ torch . utils . data . DataLoader ] \u00b6 Source code in zamba/pytorch_lightning/utils.py 99 100 101 102 103 104 105 106 107 108 109 def test_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . test_dataset : return torch . utils . data . DataLoader ( self . test_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) train_dataloader () -> Optional [ torch . utils . data . DataLoader ] \u00b6 Source code in zamba/pytorch_lightning/utils.py 75 76 77 78 79 80 81 82 83 84 85 def train_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . train_dataset : return torch . utils . data . DataLoader ( self . train_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = True , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) val_dataloader () -> Optional [ torch . utils . data . DataLoader ] \u00b6 Source code in zamba/pytorch_lightning/utils.py 87 88 89 90 91 92 93 94 95 96 97 def val_dataloader ( self ) -> Optional [ torch . utils . data . DataLoader ]: if self . val_dataset : return torch . utils . data . DataLoader ( self . val_dataset , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , multiprocessing_context = self . multiprocessing_context , prefetch_factor = self . prefetch_factor , persistent_workers = self . num_workers > 0 , ) ZambaVideoClassificationLightningModule \u00b6 Bases: LightningModule Source code in zamba/pytorch_lightning/utils.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 class ZambaVideoClassificationLightningModule ( LightningModule ): def __init__ ( self , species : List [ str ], lr : float = 1e-3 , scheduler : Optional [ str ] = None , scheduler_params : Optional [ dict ] = None , ** kwargs , ): super () . __init__ () if ( scheduler is None ) and ( scheduler_params is not None ): warnings . warn ( \"scheduler_params provided without scheduler. scheduler_params will have no effect.\" ) self . lr = lr self . species = species self . num_classes = len ( species ) if scheduler is not None : self . scheduler = torch . optim . lr_scheduler . __dict__ [ scheduler ] else : self . scheduler = scheduler self . scheduler_params = scheduler_params self . model_class = type ( self ) . __name__ self . save_hyperparameters ( \"lr\" , \"scheduler\" , \"scheduler_params\" , \"species\" ) self . hparams [ \"model_class\" ] = self . model_class def forward ( self , x ): return self . model ( x ) def on_train_start ( self ): metrics = { \"val_macro_f1\" : {}} if self . num_classes > 2 : metrics . update ( { f \"val_top_ { k } _accuracy\" : {} for k in DEFAULT_TOP_K if k < self . num_classes } ) else : metrics . update ({ \"val_accuracy\" : {}}) # write hparams to hparams.yaml file, log metrics to tb hparams tab self . logger . log_hyperparams ( self . hparams , metrics ) def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"train_loss\" , loss . detach ()) return loss def validation_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"val_loss\" , loss . detach ()) y_proba = torch . sigmoid ( y_hat . cpu ()) . numpy () return { \"y_true\" : y . cpu () . numpy () . astype ( int ), \"y_pred\" : y_proba . round () . astype ( int ), \"y_proba\" : y_proba , } @staticmethod def aggregate_step_outputs ( outputs : Dict [ str , np . ndarray ] ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: y_true = np . vstack ([ output [ \"y_true\" ] for output in outputs ]) y_pred = np . vstack ([ output [ \"y_pred\" ] for output in outputs ]) y_proba = np . vstack ([ output [ \"y_proba\" ] for output in outputs ]) return y_true , y_pred , y_proba def compute_and_log_metrics ( self , y_true : np . ndarray , y_pred : np . ndarray , y_proba : np . ndarray , subset : str ): self . log ( f \" { subset } _macro_f1\" , f1_score ( y_true , y_pred , average = \"macro\" , zero_division = 0 ), ) # if only two classes, skip top_k accuracy since not enough classes if self . num_classes > 2 : for k in DEFAULT_TOP_K : if k < self . num_classes : self . log ( f \" { subset } _top_ { k } _accuracy\" , top_k_accuracy_score ( y_true . argmax ( axis = 1 ), # top k accuracy only supports single label case y_proba , labels = np . arange ( y_proba . shape [ 1 ]), k = k , ), ) else : self . log ( f \" { subset } _accuracy\" , accuracy_score ( y_true , y_pred )) for metric_name , label , metric in compute_species_specific_metrics ( y_true , y_pred , self . species ): self . log ( f \"species/ { subset } _ { metric_name } / { label } \" , metric ) def validation_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Args: outputs (List[dict]): list of output dictionaries from each validation step containing y_pred and y_true. \"\"\" y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"val\" ) def test_step ( self , batch , batch_idx ): return self . validation_step ( batch , batch_idx ) def test_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"test\" ) def predict_step ( self , batch , batch_idx , dataloader_idx : Optional [ int ] = None ): x , y = batch y_hat = self ( x ) pred = torch . sigmoid ( y_hat ) . cpu () . numpy () return pred def _get_optimizer ( self ): return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . lr ) def configure_optimizers ( self ): \"\"\" Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. \"\"\" optim = self . _get_optimizer () if self . scheduler is None : return optim else : return { \"optimizer\" : optim , \"lr_scheduler\" : self . scheduler ( optim , ** ({} if self . scheduler_params is None else self . scheduler_params ) ), } def to_disk ( self , path : os . PathLike ): checkpoint = { \"state_dict\" : self . state_dict (), \"hyper_parameters\" : self . hparams , } torch . save ( checkpoint , path ) @classmethod def from_disk ( cls , path : os . PathLike ): return cls . load_from_checkpoint ( path ) Attributes \u00b6 lr = lr instance-attribute \u00b6 model_class = type ( self ) . __name__ instance-attribute \u00b6 num_classes = len ( species ) instance-attribute \u00b6 scheduler = torch . optim . lr_scheduler . __dict__ [ scheduler ] instance-attribute \u00b6 scheduler_params = scheduler_params instance-attribute \u00b6 species = species instance-attribute \u00b6 Functions \u00b6 __init__ ( species : List [ str ], lr : float = 0.001 , scheduler : Optional [ str ] = None , scheduler_params : Optional [ dict ] = None , ** kwargs ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __init__ ( self , species : List [ str ], lr : float = 1e-3 , scheduler : Optional [ str ] = None , scheduler_params : Optional [ dict ] = None , ** kwargs , ): super () . __init__ () if ( scheduler is None ) and ( scheduler_params is not None ): warnings . warn ( \"scheduler_params provided without scheduler. scheduler_params will have no effect.\" ) self . lr = lr self . species = species self . num_classes = len ( species ) if scheduler is not None : self . scheduler = torch . optim . lr_scheduler . __dict__ [ scheduler ] else : self . scheduler = scheduler self . scheduler_params = scheduler_params self . model_class = type ( self ) . __name__ self . save_hyperparameters ( \"lr\" , \"scheduler\" , \"scheduler_params\" , \"species\" ) self . hparams [ \"model_class\" ] = self . model_class aggregate_step_outputs ( outputs : Dict [ str , np . ndarray ]) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ] staticmethod \u00b6 Source code in zamba/pytorch_lightning/utils.py 190 191 192 193 194 195 196 197 198 @staticmethod def aggregate_step_outputs ( outputs : Dict [ str , np . ndarray ] ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: y_true = np . vstack ([ output [ \"y_true\" ] for output in outputs ]) y_pred = np . vstack ([ output [ \"y_pred\" ] for output in outputs ]) y_proba = np . vstack ([ output [ \"y_proba\" ] for output in outputs ]) return y_true , y_pred , y_proba compute_and_log_metrics ( y_true : np . ndarray , y_pred : np . ndarray , y_proba : np . ndarray , subset : str ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def compute_and_log_metrics ( self , y_true : np . ndarray , y_pred : np . ndarray , y_proba : np . ndarray , subset : str ): self . log ( f \" { subset } _macro_f1\" , f1_score ( y_true , y_pred , average = \"macro\" , zero_division = 0 ), ) # if only two classes, skip top_k accuracy since not enough classes if self . num_classes > 2 : for k in DEFAULT_TOP_K : if k < self . num_classes : self . log ( f \" { subset } _top_ { k } _accuracy\" , top_k_accuracy_score ( y_true . argmax ( axis = 1 ), # top k accuracy only supports single label case y_proba , labels = np . arange ( y_proba . shape [ 1 ]), k = k , ), ) else : self . log ( f \" { subset } _accuracy\" , accuracy_score ( y_true , y_pred )) for metric_name , label , metric in compute_species_specific_metrics ( y_true , y_pred , self . species ): self . log ( f \"species/ { subset } _ { metric_name } / { label } \" , metric ) configure_optimizers () \u00b6 Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. Source code in zamba/pytorch_lightning/utils.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def configure_optimizers ( self ): \"\"\" Setup the Adam optimizer. Note, that this function also can return a lr scheduler, which is usually useful for training video models. \"\"\" optim = self . _get_optimizer () if self . scheduler is None : return optim else : return { \"optimizer\" : optim , \"lr_scheduler\" : self . scheduler ( optim , ** ({} if self . scheduler_params is None else self . scheduler_params ) ), } forward ( x ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 154 155 def forward ( self , x ): return self . model ( x ) from_disk ( path : os . PathLike ) classmethod \u00b6 Source code in zamba/pytorch_lightning/utils.py 282 283 284 @classmethod def from_disk ( cls , path : os . PathLike ): return cls . load_from_checkpoint ( path ) on_train_start () \u00b6 Source code in zamba/pytorch_lightning/utils.py 157 158 159 160 161 162 163 164 165 166 167 168 def on_train_start ( self ): metrics = { \"val_macro_f1\" : {}} if self . num_classes > 2 : metrics . update ( { f \"val_top_ { k } _accuracy\" : {} for k in DEFAULT_TOP_K if k < self . num_classes } ) else : metrics . update ({ \"val_accuracy\" : {}}) # write hparams to hparams.yaml file, log metrics to tb hparams tab self . logger . log_hyperparams ( self . hparams , metrics ) predict_step ( batch , batch_idx , dataloader_idx : Optional [ int ] = None ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 249 250 251 252 253 def predict_step ( self , batch , batch_idx , dataloader_idx : Optional [ int ] = None ): x , y = batch y_hat = self ( x ) pred = torch . sigmoid ( y_hat ) . cpu () . numpy () return pred test_epoch_end ( outputs : List [ Dict [ str , np . ndarray ]]) \u00b6 Source code in zamba/pytorch_lightning/utils.py 245 246 247 def test_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"test\" ) test_step ( batch , batch_idx ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 242 243 def test_step ( self , batch , batch_idx ): return self . validation_step ( batch , batch_idx ) to_disk ( path : os . PathLike ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 275 276 277 278 279 280 def to_disk ( self , path : os . PathLike ): checkpoint = { \"state_dict\" : self . state_dict (), \"hyper_parameters\" : self . hparams , } torch . save ( checkpoint , path ) training_step ( batch , batch_idx ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 170 171 172 173 174 175 def training_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"train_loss\" , loss . detach ()) return loss validation_epoch_end ( outputs : List [ Dict [ str , np . ndarray ]]) \u00b6 Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Parameters: Name Type Description Default outputs List [ dict ] list of output dictionaries from each validation step containing y_pred and y_true. required Source code in zamba/pytorch_lightning/utils.py 231 232 233 234 235 236 237 238 239 240 def validation_epoch_end ( self , outputs : List [ Dict [ str , np . ndarray ]]): \"\"\"Aggregates validation_step outputs to compute and log the validation macro F1 and top K metrics. Args: outputs (List[dict]): list of output dictionaries from each validation step containing y_pred and y_true. \"\"\" y_true , y_pred , y_proba = self . aggregate_step_outputs ( outputs ) self . compute_and_log_metrics ( y_true , y_pred , y_proba , subset = \"val\" ) validation_step ( batch , batch_idx ) \u00b6 Source code in zamba/pytorch_lightning/utils.py 177 178 179 180 181 182 183 184 185 186 187 188 def validation_step ( self , batch , batch_idx ): x , y = batch y_hat = self ( x ) loss = F . binary_cross_entropy_with_logits ( y_hat , y ) self . log ( \"val_loss\" , loss . detach ()) y_proba = torch . sigmoid ( y_hat . cpu ()) . numpy () return { \"y_true\" : y . cpu () . numpy () . astype ( int ), \"y_pred\" : y_proba . round () . astype ( int ), \"y_proba\" : y_proba , } Functions \u00b6","title":"zamba.pytorch_lightning.utils"},{"location":"api-reference/pytorch_lightning-utils/#zambapytorch_lightningutils","text":"","title":"zamba.pytorch_lightning.utils"},{"location":"api-reference/pytorch_lightning-utils/#zamba.pytorch_lightning.utils-attributes","text":"","title":"Attributes"},{"location":"api-reference/pytorch_lightning-utils/#zamba.pytorch_lightning.utils-classes","text":"","title":"Classes"},{"location":"api-reference/pytorch_lightning-utils/#zamba.pytorch_lightning.utils-functions","text":"","title":"Functions"},{"location":"api-reference/settings/","text":"zamba.settings \u00b6 Attributes \u00b6 SPLIT_SEED = os . environ . get ( 'SPLIT_SEED' , 4007 ) module-attribute \u00b6 VIDEO_SUFFIXES = os . environ . get ( 'VIDEO_SUFFIXES' ) module-attribute \u00b6","title":"zamba.settings"},{"location":"api-reference/settings/#zambasettings","text":"","title":"zamba.settings"},{"location":"api-reference/settings/#zamba.settings-attributes","text":"","title":"Attributes"},{"location":"changelog/","text":"zamba changelog \u00b6 v2.2.4 (2022-11-10) \u00b6 Do not cache videos if the VIDEO_CACHE_DIR environment variable is an empty string or zero ( PR #245 ) v2.2.3 (2022-11-01) \u00b6 Fixes Lightning deprecation of DDPPlugin ( PR #244 ) v2.2.2 (2022-10-04) \u00b6 Adds a page to the docs summarizing the performance of the African species classification model on a holdout set ( PR #235 ) v2.2.1 (2022-09-27) \u00b6 Turn off showing local variables in Typer's exception and error handling ( PR #237 ) Fixes bug where the column order was incorrect for training models when the provided labels are a subset of the model's default labels ( PR #236 ) v2.2.0 (2022-09-26) \u00b6 Model releases and new features \u00b6 The default time_distributed model (African species classification) has been retrained on over 250,000 videos. This 16x increase in training data significantly improves accuracy. This new version replaces the previous one. ( PR #226 , PR #232 ) A new default model option is added: blank_nonblank . This model only does blank detection. This binary model can be trained and finetuned in the same way as the species classification models. This model was trained on both African and European data, totaling over 263,000 training videos. ( PR #228 ) Detect if a user is training in a binary model and preprocess the labels accordingly ( PR #215 ) Bug fixes and improvements \u00b6 Add a validator to ensure that using a model\u2019s default labels is only possible when the species in the provided labels file are a subset of those ( PR #229 ) Refactor the logic in instantiate_model for clarity ( PR #229 ) Use pqdm to check for missing files in parallel ( PR #224 ) Set model_name based on the provided checkpoint so that user-trained models use the appropriate video loader config ( PR #221 ) Leave data_dir as a relative path ( PR #219 ) Ensure hparams yaml files get included in the source distribution ( PR #210 ) Hold back setuptools so mkdocstrings works ( PR #207 ) Factor out get_cached_array_path ( PR #202 ) v2.1.0 (2022-07-15) \u00b6 Retrains the time distributed species classification model using the updated MegadetectorLite frame selection ( PR #199 ) Replaces the MegadetectorLite frame selection model with an improved model trained on significantly more data ( PR #195 ) v2.0.4 (2022-06-17) \u00b6 Pins thop to an earlier version ( PR #191 ) Fixes caching so a previously downloaded checkpoint file actually gets used ( PR #190 , PR #194 ) Removes a lightning deprecation warning for DDP ( PR #187 ) Ignores extra columns in the user-provided labels or filepaths csv ( PR #186 ) v2.0.3 (2022-05-06) \u00b6 Releasing to pick up #179. PR #179 removes the DensePose extra from the default dev requirements and tests. Docs are updated to clarify how to install and run tests for DensePose. v2.0.2 (2021-12-21) \u00b6 Releasing to pick up #172. PR #172 fixes bug where video loading that uses the YoloX model (all of the built in models) resulted in videos not being able to load. v2.0.1 (2021-12-15) \u00b6 Releasing to pick up #167 and #169. PR #169 fixes error in splitting data into train/test/val when only a few videos. PR #167 refactors yolox into an object_detection module Other documentation fixes also included. v2.0.0 (2021-10-22) \u00b6 Previous model: Machine learning competition \u00b6 The algorithms used by zamba v1 were based on the winning solution from the Pri-matrix Factorization machine learning competition, hosted by DrivenData . Data for the competition was provided by the Chimp&See project and manually labeled by volunteers. The competition had over 300 participants and over 450 submissions throughout the three month challenge. The v1 algorithm was adapted from the winning competition submission, with some aspects changed during development to improve performance. The core algorithm in zamba v1 was a stacked ensemble which consisted of a first layer of models that were then combined into a final prediction in a second layer. The first level of the stack consisted of 5 keras deep learning models, whose individual predictions were combined in the second level of the stack to form the final prediction. In v2, the stacked ensemble algorithm from v1 is replaced with three more powerful single-model options : time_distributed , slowfast , and european . The new models utilize state-of-the-art image and video classification architectures, and are able to outperform the much more computationally intensive stacked ensemble model. New geographies and species \u00b6 zamba v2 incorporates data from Western Europe (Germany). The new data is packaged in the pretrained european model, which can predict 11 common European species not present in zamba v1. zamba v2 also incorporates new training data from 15 countries in central and west Africa, and adds 12 additional species to the pretrained African models. Retraining flexibility \u00b6 Model training is made available zamba v2, so users can finetune a pretrained model using their own data to improve performance for a specific ecology or set of sites. zamba v2 also allows users to retrain a model on completely new species labels.","title":"`zamba` changelog"},{"location":"changelog/#zamba-changelog","text":"","title":"zamba changelog"},{"location":"changelog/#v224-2022-11-10","text":"Do not cache videos if the VIDEO_CACHE_DIR environment variable is an empty string or zero ( PR #245 )","title":"v2.2.4 (2022-11-10)"},{"location":"changelog/#v223-2022-11-01","text":"Fixes Lightning deprecation of DDPPlugin ( PR #244 )","title":"v2.2.3 (2022-11-01)"},{"location":"changelog/#v222-2022-10-04","text":"Adds a page to the docs summarizing the performance of the African species classification model on a holdout set ( PR #235 )","title":"v2.2.2 (2022-10-04)"},{"location":"changelog/#v221-2022-09-27","text":"Turn off showing local variables in Typer's exception and error handling ( PR #237 ) Fixes bug where the column order was incorrect for training models when the provided labels are a subset of the model's default labels ( PR #236 )","title":"v2.2.1 (2022-09-27)"},{"location":"changelog/#v220-2022-09-26","text":"","title":"v2.2.0 (2022-09-26)"},{"location":"changelog/#v210-2022-07-15","text":"Retrains the time distributed species classification model using the updated MegadetectorLite frame selection ( PR #199 ) Replaces the MegadetectorLite frame selection model with an improved model trained on significantly more data ( PR #195 )","title":"v2.1.0 (2022-07-15)"},{"location":"changelog/#v204-2022-06-17","text":"Pins thop to an earlier version ( PR #191 ) Fixes caching so a previously downloaded checkpoint file actually gets used ( PR #190 , PR #194 ) Removes a lightning deprecation warning for DDP ( PR #187 ) Ignores extra columns in the user-provided labels or filepaths csv ( PR #186 )","title":"v2.0.4 (2022-06-17)"},{"location":"changelog/#v203-2022-05-06","text":"Releasing to pick up #179. PR #179 removes the DensePose extra from the default dev requirements and tests. Docs are updated to clarify how to install and run tests for DensePose.","title":"v2.0.3 (2022-05-06)"},{"location":"changelog/#v202-2021-12-21","text":"Releasing to pick up #172. PR #172 fixes bug where video loading that uses the YoloX model (all of the built in models) resulted in videos not being able to load.","title":"v2.0.2 (2021-12-21)"},{"location":"changelog/#v201-2021-12-15","text":"Releasing to pick up #167 and #169. PR #169 fixes error in splitting data into train/test/val when only a few videos. PR #167 refactors yolox into an object_detection module Other documentation fixes also included.","title":"v2.0.1 (2021-12-15)"},{"location":"changelog/#v200-2021-10-22","text":"","title":"v2.0.0 (2021-10-22)"},{"location":"contribute/","text":"Help make zamba better \u00b6 zamba is an open source project, which means you can help make it better! Develop the GitHub repository \u00b6 To get involved, check out the GitHub code repository . There you can find open issues with comments and links to help you along. zamba uses continuous integration and test-driven development to ensure that we always have a working project. So what are you waiting for? git going! Installation for development \u00b6 To install zamba for development, you need to clone the git repository and then install the cloned version of the library for local development. To install for development: $ git clone https://github.com/drivendataorg/zamba.git $ cd zamba $ pip install -r requirements-dev.txt If your contribution is to the DensePose model, you will need to install the additional dependencies with: $ pip install -e . [ densepose ] Running the zamba test suite \u00b6 The included Makefile contains code that uses pytest to run all tests in zamba/tests . The command is (from the project root): $ make tests For DensePose related tests, the command is: $ make densepose-tests Submit additional training videos \u00b6 If you have additional labeled videos that may be useful for improving the basic models that ship with zamba , we'd love to hear from you! You can get in touch at info@drivendata.org","title":"Help make `zamba` better"},{"location":"contribute/#help-make-zamba-better","text":"zamba is an open source project, which means you can help make it better!","title":"Help make zamba better"},{"location":"contribute/#develop-the-github-repository","text":"To get involved, check out the GitHub code repository . There you can find open issues with comments and links to help you along. zamba uses continuous integration and test-driven development to ensure that we always have a working project. So what are you waiting for? git going!","title":"Develop the GitHub repository"},{"location":"contribute/#installation-for-development","text":"To install zamba for development, you need to clone the git repository and then install the cloned version of the library for local development. To install for development: $ git clone https://github.com/drivendataorg/zamba.git $ cd zamba $ pip install -r requirements-dev.txt If your contribution is to the DensePose model, you will need to install the additional dependencies with: $ pip install -e . [ densepose ]","title":"Installation for development"},{"location":"contribute/#running-the-zamba-test-suite","text":"The included Makefile contains code that uses pytest to run all tests in zamba/tests . The command is (from the project root): $ make tests For DensePose related tests, the command is: $ make densepose-tests","title":"Running the zamba test suite"},{"location":"contribute/#submit-additional-training-videos","text":"If you have additional labeled videos that may be useful for improving the basic models that ship with zamba , we'd love to hear from you! You can get in touch at info@drivendata.org","title":"Submit additional training videos"},{"location":"models/densepose/","text":"DensePose \u00b6 Background \u00b6 DensePose ( Neverova et al, 2021 ) is a model published by Facebook AI Research that can be used to get segmentations for animals that appear in videos. The model was trained on the following animals, but often works for other species as well: bear, cat, cow, dog, elephant, giraffe, horse, sheep, zebra. Here's an example of the segmentation output for a frame: Additionally, the model provides mapping of the segmentation output to specific anatomy for chimpanzees. This can be helpful for determining the orientation of chimpanzees in videos and for understanding their behaviors. Here is an example of what that output looks like: For more information on the algorithms and outputs of the DensePose model, see the Facebook DensePose Github Repository . Outputs \u00b6 The Zamba package supports running DensePose on videos to generate three types of outputs: A .json file with details of segmentations per video frame. A .mp4 file where the original video has the segmentation rendered on top of animal so that the output can be visually inspected. A .csv that contains the height and width of the bounding box around each chimpanzee, the frame number and timestamp of the observation, and the percentage of pixels in the bounding box that correspond with each anatomical part. This is specified by adding --output-type chimp_anatomy . Running the DensePose model is fairly computationally intensive. It is recommended to run the model at a relatively low framerate (e.g., 1 frame per second) to generate outputs for a video. JSON output files can also be quite large because they contain the full embedding. These are not written out by default. Installation \u00b6 In order to use the DensePose model, you must have PyTorch already installed on your system. Then you must install the densepose extra: pip install torch pip install \"https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz#egg=zamba[densepose]\" Running DensePose \u00b6 Once that is done, here's how to run the DensePose model: CLI # create a segmentation output video for each input video in PATH_TO_VIDEOS zamba densepose --data-dir PATH_TO_VIDEOS --render-output Python from zamba.models.densepose import DensePoseConfig densepose_conf = DensePoseConfig ( data_dir = \"PATH_TO_VIDEOS\" , render_output = True ) densepose_conf . run_model () Getting help \u00b6 To see all of the available options, run zamba densepose --help . $ zamba densepose --help Usage: zamba densepose [OPTIONS] Run densepose algorithm on videos. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to video or image file or folder containing images/videos. --filepaths PATH Path to csv containing `filepath` column with videos. --save-dir PATH An optional directory for saving the output. Defaults to the current working directory. --config PATH Specify options using yaml configuration file instead of through command line options. --fps FLOAT Number of frames per second to process. Defaults to 1.0 (1 frame per second). [default: 1.0] --output-type [segmentation|chimp_anatomy] If 'chimp_anatomy' will apply anatomy model from densepose to the rendering and create a CSV with the anatomy visible in each frame. If 'segmentation', will just output the segmented area where an animal is identified, which works for more species than chimpanzees. [default: chimp_anatomy] --render-output / --no-render-output If True, generate an output image or video with either the segmentation or anatomy rendered depending on the `output_type` that is chosen. [default: no-render-output] --weight-download-region [us|eu|asia] Server region for downloading weights. --cache-dir PATH Path to directory for model weights. Alternatively, specify with environment variable `ZAMBA_CACHE_DIR`. If not specified, user's cache directory is used. -y, --yes Skip confirmation of configuration and proceed right to prediction. --help Show this message and exit.","title":"DensePose"},{"location":"models/densepose/#densepose","text":"","title":"DensePose"},{"location":"models/densepose/#background","text":"DensePose ( Neverova et al, 2021 ) is a model published by Facebook AI Research that can be used to get segmentations for animals that appear in videos. The model was trained on the following animals, but often works for other species as well: bear, cat, cow, dog, elephant, giraffe, horse, sheep, zebra. Here's an example of the segmentation output for a frame: Additionally, the model provides mapping of the segmentation output to specific anatomy for chimpanzees. This can be helpful for determining the orientation of chimpanzees in videos and for understanding their behaviors. Here is an example of what that output looks like: For more information on the algorithms and outputs of the DensePose model, see the Facebook DensePose Github Repository .","title":"Background"},{"location":"models/densepose/#outputs","text":"The Zamba package supports running DensePose on videos to generate three types of outputs: A .json file with details of segmentations per video frame. A .mp4 file where the original video has the segmentation rendered on top of animal so that the output can be visually inspected. A .csv that contains the height and width of the bounding box around each chimpanzee, the frame number and timestamp of the observation, and the percentage of pixels in the bounding box that correspond with each anatomical part. This is specified by adding --output-type chimp_anatomy . Running the DensePose model is fairly computationally intensive. It is recommended to run the model at a relatively low framerate (e.g., 1 frame per second) to generate outputs for a video. JSON output files can also be quite large because they contain the full embedding. These are not written out by default.","title":"Outputs"},{"location":"models/densepose/#installation","text":"In order to use the DensePose model, you must have PyTorch already installed on your system. Then you must install the densepose extra: pip install torch pip install \"https://github.com/drivendataorg/zamba/releases/latest/download/zamba.tar.gz#egg=zamba[densepose]\"","title":"Installation"},{"location":"models/densepose/#running-densepose","text":"Once that is done, here's how to run the DensePose model: CLI # create a segmentation output video for each input video in PATH_TO_VIDEOS zamba densepose --data-dir PATH_TO_VIDEOS --render-output Python from zamba.models.densepose import DensePoseConfig densepose_conf = DensePoseConfig ( data_dir = \"PATH_TO_VIDEOS\" , render_output = True ) densepose_conf . run_model ()","title":"Running DensePose"},{"location":"models/densepose/#getting-help","text":"To see all of the available options, run zamba densepose --help . $ zamba densepose --help Usage: zamba densepose [OPTIONS] Run densepose algorithm on videos. If an argument is specified in both the command line and in a yaml file, the command line input will take precedence. Options: --data-dir PATH Path to video or image file or folder containing images/videos. --filepaths PATH Path to csv containing `filepath` column with videos. --save-dir PATH An optional directory for saving the output. Defaults to the current working directory. --config PATH Specify options using yaml configuration file instead of through command line options. --fps FLOAT Number of frames per second to process. Defaults to 1.0 (1 frame per second). [default: 1.0] --output-type [segmentation|chimp_anatomy] If 'chimp_anatomy' will apply anatomy model from densepose to the rendering and create a CSV with the anatomy visible in each frame. If 'segmentation', will just output the segmented area where an animal is identified, which works for more species than chimpanzees. [default: chimp_anatomy] --render-output / --no-render-output If True, generate an output image or video with either the segmentation or anatomy rendered depending on the `output_type` that is chosen. [default: no-render-output] --weight-download-region [us|eu|asia] Server region for downloading weights. --cache-dir PATH Path to directory for model weights. Alternatively, specify with environment variable `ZAMBA_CACHE_DIR`. If not specified, user's cache directory is used. -y, --yes Skip confirmation of configuration and proceed right to prediction. --help Show this message and exit.","title":"Getting help"},{"location":"models/species-detection/","text":"Available models \u00b6 The algorithms in zamba are designed to identify species of animals that appear in camera trap videos. The pretrained models that ship with the zamba package are: blank_nonblank , time_distributed , slowfast , and european . For more details of each, read on! Model summary \u00b6 Model Geography Relative strengths Architecture Number of training videos blank_nonblank Central Africa, West Africa, and Western Europe Just blank detection, without species classification Image-based TimeDistributedEfficientNet ~263,000 time_distributed Central and West Africa Recommended species classification model for jungle ecologies Image-based TimeDistributedEfficientNet ~250,000 slowfast Central and West Africa Potentially better than time_distributed at small species detection Video-native SlowFast ~15,000 european Western Europe Trained on non-jungle ecologies Finetuned time_distributed model ~13,000 The models trained on the largest datasets took a couple weeks to train on a single GPU machine. Some models will be updated in the future, and you can always check the changelog to see if there have been updates. All models support training, fine-tuning, and inference. For fine-tuning, we recommend using the time_distributed model as the starting point. What species can zamba detect? \u00b6 The blank_nonblank model is trained to do blank detection, without the species classification. It only outputs the probability that the video is blank , meaning that it does not contain an animal. The time_distributed and slowfast models are both trained to identify 32 common species from Central and West Africa. The output labels in these models are: aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal The european model is trained to identify 11 common species in Western Europe. The possible class labels are: bird blank domestic_cat european_badger european_beaver european_hare european_roe_deer north_american_raccoon red_fox weasel wild_boar blank_nonblank model \u00b6 Architecture \u00b6 The blank_nonblank uses the same architecture as time_distributed model, but there is only one output class as this is a binary classification problem. Default configuration \u00b6 The full default configuration is available on Github . The blank_nonblank model uses the same default configuration as the time_distributed model. For the frame selection, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then the classification model is run on only the 16 frames with the highest predicted probability of detection. Training data \u00b6 The blank_nonblank model was trained on all the data used for the the time_distributed and european models. time_distributed model \u00b6 Architecture \u00b6 The time_distributed model was built by re-training a well-known image classification architecture called EfficientNetV2 (Tan, M., & Le, Q., 2019) to identify the species in our camera trap videos. EfficientNetV2 models are convolutional neural networks designed to jointly optimize model size and training speed. EfficientNetV2 is image native, meaning it classifies each frame separately when generating predictions. The model is wrapped in a TimeDistributed layer which enables a single prediction per video. Training data \u00b6 The time_distributed model was trained using data collected and annotated by trained ecologists from Cameroon, Central African Republic, Democratic Republic of the Congo, Gabon, Guinea, Liberia, Mozambique, Nigeria, Republic of the Congo, Senegal, Tanzania, and Uganda, as well as citizen scientists on the Chimp&See platform. The data included camera trap videos from: Country Location Cameroon Campo Ma'an National Park Korup National Park Central African Republic Dzanga-Sangha Protected Area C\u00f4te d'Ivoire Como\u00e9 National Park Guiroutou Ta\u00ef National Park Democratic Republic of the Congo Bili-Uele Protect Area Salonga National Park Gabon Loango National Park Lop\u00e9 National Park Guinea Bakoun Classified Forest Moyen-Bafing National Park Liberia East Nimba Nature Reserve Grebo-Krahn National Park Sapo National Park Mozambique Gorongosa National Park Nigeria Gashaka-Gumti National Park Republic of the Congo Conkouati-Douli National Park Nouabale-Ndoki National Park Senegal Kayan Tanzania Grumeti Game Reserve Ugalla River National Park Uganda Budongo Forest Reserve Bwindi Forest National Park Ngogo and Kibale National Park Default configuration \u00b6 The full default configuration is available on Github . By default, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then time_distributed is run on only the 16 frames with the highest predicted probability of detection. By default, videos are resized to 240x426 pixels following frame selection. The default video loading configuration for time_distributed is: video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 4 total_frames : 16 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 16 frame_batch_size : 24 image_height : 640 image_width : 640 You can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file . The only requirement for the time_distributed model is that the video loader must return 16 frames. slowfast model \u00b6 Architecture \u00b6 The slowfast model was built by re-training a video classification backbone called SlowFast (Feichtenhofer, C., Fan, H., Malik, J., & He, K., 2019). SlowFast refers to the two model pathways involved: one that operates at a low frame rate to capture spatial semantics, and one that operates at a high frame rate to capture motion over time. Source: Feichtenhofer, C., Fan, H., Malik, J., & He, K. (2019). Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 6202-6211). Unlike time_distributed , slowfast is video native. This means it takes into account the relationship between frames in a video, rather than running independently on each frame. Training data \u00b6 The slowfast model was trained on a subset of the data used for the time_distributed model. Default configuration \u00b6 The full default configuration is available on Github . By default, an efficient object detection model called MegadetectorLite is run on all frames to determine which are the most likely to contain an animal. Then slowfast is run on only the 32 frames with the highest predicted probability of detection. By default, videos are resized to 240x426 pixels. The full default video loading configuration is: video_loader_config : model_input_height : 240 model_input_width : 426 crop_bottom_pixels : 50 fps : 8 total_frames : 32 ensure_total_frames : true megadetector_lite_config : confidence : 0.25 fill_mode : score_sorted n_frames : 32 image_height : 416 image_width : 416 You can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file . The two requirements for the slowfast model are that: the video loader must return 32 frames videos inputted into the model must be at least 200 x 200 pixels european model \u00b6 Architecture \u00b6 The european model starts from the a previous version of the time_distributed model, and then replaces and trains the final output layer to predict European species. Training data \u00b6 The european model is finetuned with data collected and annotated by partners at the German Centre for Integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig and The Max Planck Institute for Evolutionary Anthropology . The finetuning data included camera trap videos from Hintenteiche bei Biesenbrow, Germany. Default configuration \u00b6 The full default configuration is available on Github . The european model uses the same default configuration as the time_distributed model. As with all models, you can choose different frame selection methods and vary the size of the images that are used by passing in a custom YAML configuration file . The only requirement for the european model is that the video loader must return 16 frames. MegadetectorLite \u00b6 Frame selection for video models is critical as it would be infeasible to train neural networks on all the frames in a video. For all the species detection models that ship with zamba , the default frame selection method is an efficient object detection model called MegadetectorLite that determines the likelihood that each frame contains an animal. Then, only the frames with the highest probability of detection are passed to the model. MegadetectorLite combines two open-source models: Megadetector is a pretrained image model designed to detect animals, people, and vehicles in camera trap videos. YOLOX is a high-performance, lightweight object detection model that is much less computationally intensive than Megadetector. While highly accurate, Megadetector is too computationally intensive to run on every frame. MegadetectorLite was created by training a YOLOX model using the predictions of the Megadetector as ground truth - this method is called student-teacher training . MegadetectorLite can be imported into Python code and used directly since it has convenient methods for detect_image and detect_video . See the API documentation for more details . User contributed models \u00b6 We encourage people to share their custom models trained with Zamba. If you train a model and want to make it available, please add it to the Model Zoo Wiki for others to be able to use! To use one of these models, download the weights file and the configuration file from the Model Zoo Wiki. You'll need to create a configuration yaml to use that at least contains the same video_loader_config from the configuration yaml you downloaded. Then you can run the model with: $ zamba predict --checkpoint downloaded_weights.ckpt --config predict_config.yaml","title":"Species detection"},{"location":"models/species-detection/#available-models","text":"The algorithms in zamba are designed to identify species of animals that appear in camera trap videos. The pretrained models that ship with the zamba package are: blank_nonblank , time_distributed , slowfast , and european . For more details of each, read on!","title":"Available models"},{"location":"models/species-detection/#model-summary","text":"Model Geography Relative strengths Architecture Number of training videos blank_nonblank Central Africa, West Africa, and Western Europe Just blank detection, without species classification Image-based TimeDistributedEfficientNet ~263,000 time_distributed Central and West Africa Recommended species classification model for jungle ecologies Image-based TimeDistributedEfficientNet ~250,000 slowfast Central and West Africa Potentially better than time_distributed at small species detection Video-native SlowFast ~15,000 european Western Europe Trained on non-jungle ecologies Finetuned time_distributed model ~13,000 The models trained on the largest datasets took a couple weeks to train on a single GPU machine. Some models will be updated in the future, and you can always check the changelog to see if there have been updates. All models support training, fine-tuning, and inference. For fine-tuning, we recommend using the time_distributed model as the starting point.","title":"Model summary"},{"location":"models/species-detection/#what-species-can-zamba-detect","text":"The blank_nonblank model is trained to do blank detection, without the species classification. It only outputs the probability that the video is blank , meaning that it does not contain an animal. The time_distributed and slowfast models are both trained to identify 32 common species from Central and West Africa. The output labels in these models are: aardvark antelope_duiker badger bat bird blank cattle cheetah chimpanzee_bonobo civet_genet elephant equid forest_buffalo fox giraffe gorilla hare_rabbit hippopotamus hog human hyena large_flightless_bird leopard lion mongoose monkey_prosimian pangolin porcupine reptile rodent small_cat wild_dog_jackal The european model is trained to identify 11 common species in Western Europe. The possible class labels are: bird blank domestic_cat european_badger european_beaver european_hare european_roe_deer north_american_raccoon red_fox weasel wild_boar","title":"What species can zamba detect?"},{"location":"models/species-detection/#blank_nonblank-model","text":"","title":"blank_nonblank model"},{"location":"models/species-detection/#time_distributed-model","text":"","title":"time_distributed model"},{"location":"models/species-detection/#slowfast-model","text":"","title":"slowfast model"},{"location":"models/species-detection/#european-model","text":"","title":"european model"},{"location":"models/species-detection/#megadetectorlite","text":"Frame selection for video models is critical as it would be infeasible to train neural networks on all the frames in a video. For all the species detection models that ship with zamba , the default frame selection method is an efficient object detection model called MegadetectorLite that determines the likelihood that each frame contains an animal. Then, only the frames with the highest probability of detection are passed to the model. MegadetectorLite combines two open-source models: Megadetector is a pretrained image model designed to detect animals, people, and vehicles in camera trap videos. YOLOX is a high-performance, lightweight object detection model that is much less computationally intensive than Megadetector. While highly accurate, Megadetector is too computationally intensive to run on every frame. MegadetectorLite was created by training a YOLOX model using the predictions of the Megadetector as ground truth - this method is called student-teacher training . MegadetectorLite can be imported into Python code and used directly since it has convenient methods for detect_image and detect_video . See the API documentation for more details .","title":"MegadetectorLite"},{"location":"models/species-detection/#user-contributed-models","text":"We encourage people to share their custom models trained with Zamba. If you train a model and want to make it available, please add it to the Model Zoo Wiki for others to be able to use! To use one of these models, download the weights file and the configuration file from the Model Zoo Wiki. You'll need to create a configuration yaml to use that at least contains the same video_loader_config from the configuration yaml you downloaded. Then you can run the model with: $ zamba predict --checkpoint downloaded_weights.ckpt --config predict_config.yaml","title":"User contributed models"},{"location":"models/td-full-metrics/","text":"Time-distributed model performance \u00b6 African forest model \u00b6 The African species time-distributed model was trained using almost 250,000 videos from 14 countries in West, Central, and East Africa. These videos include examples of 30 animal species, plus some blank videos and some showing humans. To evaluate the performance of the model, we held out 30,324 videos from 101 randomly-chosen sites. Removing blank videos \u00b6 One use of this model is to identify blank videos so they can be discarded or ignored. In this dataset, 42% of the videos are blank, so removing them can save substantial amounts of viewing time and storage space. The model assigns a probability that each video is blank, so one strategy is to remove videos if their probability exceeds a given threshold. Of course, the model is not perfect, so there is a chance we will wrongly remove a video that actually contains an animal. To assess this tradeoff, we can use the holdout set to simulate this strategy with a range of thresholds. For each threshold, we compute the fraction of blank videos correctly discarded and the fraction of non-blank videos incorrectly discarded. The following figure shows the results. The markers indicate three levels of tolerance for losing non-blank videos. For example, if it's acceptable to lose 5% of non-blank videos, we can choose a threshold that removes 63% of the blank videos. If we can tolerate a loss of 10%, we can remove 80% of the blanks. And if we can tolerate a loss of 15%, we can remove 90% of the blanks. Above that, the percentage of lost videos increases steeply. Accuracy \u00b6 In addition to identifying blank videos, the model also computes a probability that each of 30 animal species appears in each video (plus human and blank). We can use these probabilities to quantify the accuracy of the model for species classification. Specifically, we computed: Top-1 accuracy, which is the fraction of videos where the species with the highest predicted probability is, in fact, present. Top-3 accuracy, which is the fraction of videos where one of the three species the model considered most likely is present. Over all videos in the holdout set, the top-1 accuracy is 82%; the top-3 accuracy is 94% . As an example, if you choose a video at random and the species with the highest predicted probability is elephant, the probability is 82% that the video contains an elephant, according to the human-generated labels. If the three most likely species were elephant, hippopotamus, and cattle, the probability is 94% that the video contains at least one of those species. These results depend in part on the species represented in a particular dataset. For example, in the small number of videos from Equatorial Guinea, only three species appear. For these videos, the top-1 accuracy is 97%, much higher than the overall accuracy. In the videos from Ivory Coast, 21 species are represented, so the problem is harder. For these videos, top-1 accuracy is 80%, a little lower than the overall accuracy. Recall and precision by species \u00b6 One of the goals of classification is to help with retrieval, that is, efficiently finding videos containing a particular species. To evaluate the performance of the model for retrieval, we can use Recall, which is the fraction of videos containing a particular species that are correctly classified, and Precision, which is the fraction of videos the model labels with a particular species that actually contain that species. The following figure shows recall and precision for the species in the holdout set, excluding 11 species where there are too few examples in the holdout set to compute meaningful estimates of these metrics. It's clear that we are able to retrieve some species more efficiently than others. For example, elephants are relatively easy to find. Of the videos that contain elephants, 84% are correctly classified; and of the videos that the model labels \"elephant\", 94% contain elephants. So a researcher using the model to find elephant videos could find a large majority of them while viewing only a small number of non-elephant videos. Not surprisingly, smaller animals are harder to find. For example, the recall for rodent videos is only 22%. However, it is still possible to search for rodents by selecting videos that assign a relatively high probability to \"rodent\", even if it assigns a higher probability to another species or \"blank\". Description of the holdout set \u00b6 The videos in the holdout set are a random sample from the complete set of labeled videos, but they are selected on a transect-by-transect basis; that is, videos from each transect are assigned entirely to the training set or entirely to the holdout set. So the performance of the model on the holdout set should reflect its performance on videos from a transect the model has never seen. All 14 countries are represented in the holdout set; the following table shows the number of videos from each country. These proportions are roughly consistent with the proportions in the complete set. Country Number of videos Ivory Coast 10,987 Guinea 4,300 DR Congo 2,750 Uganda 2,497 Tanzania 1,794 Mozambique 1,168 Senegal 1,131 Gabon 1,116 Cameroon 1,114 Liberia 1,065 Central African Republic 997 Nigeria 889 Congo Republic 678 Equatorial Guinea 38 The following figure shows the number of videos containing each of 30 animal species, plus some videos showing humans and a substantial number of blank videos. One of the challenges of this kind of classification is that some species are much more common than others. For species that appear in a small number of videos, we expect the model to be less accurate because it has fewer examples to learn from. Also, for these species it is hard to assess performance precisely because there are few examples in the holdout set. If you would like to add more examples of the species you work with, see how to build on the model .","title":"African species performance"},{"location":"models/td-full-metrics/#time-distributed-model-performance","text":"","title":"Time-distributed model performance"},{"location":"models/td-full-metrics/#african-forest-model","text":"The African species time-distributed model was trained using almost 250,000 videos from 14 countries in West, Central, and East Africa. These videos include examples of 30 animal species, plus some blank videos and some showing humans. To evaluate the performance of the model, we held out 30,324 videos from 101 randomly-chosen sites.","title":"African forest model"}]}